import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('Ð—Ð°Ð¿ÑƒÑÐº Ð³Ð¸Ð±Ñ€Ð¸Ð´Ð½Ð¾Ð³Ð¾ RAG-Ð¿Ð¾Ð¸ÑÐºÐ°...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = 'Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð° Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = 'Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`ÐŸÐ¾Ð¸ÑÐº Ð² Ð¿Ñ€Ð¾Ñ„Ð¸Ð»Ðµ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ¾Ð´Ð°.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ð³Ñ€Ð°Ñ„ (Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ ${initialIds.length} ÑÑ‚Ð°Ñ€Ñ‚Ð¾Ð²Ñ‹Ñ… ÑƒÐ·Ð»Ð¾Ð²)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð´Ð° Ð´Ð»Ñ ${allIds.length} ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = 'Ð¡Ð±Ð¾Ñ€ÐºÐ° RAG-ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚Ð°...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-ÑÐ½Ð°Ð¿ÑˆÐ¾Ñ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

export async function viewIndex(options) {
  const spinner = ora('Connecting to database...').start();
  const knex = getKnex();

  try {
    await initDb();

    spinner.text = 'Fetching code chunks from database...';

    // Build query with optional filters
    let query = knex('code_chunks')
      .select('id', 'file_path', 'chunk_type', 'chunk_name', 'profile')
      .orderBy('id', 'asc');

    // Apply file filter if specified
    if (options.file) {
      query = query.where('file_path', 'like', `%${options.file}%`);
      spinner.info(`Filtering by file path: ${options.file}`);
    }

    // Apply pagination
    if (options.limit) {
      query = query.limit(options.limit);
    }
    if (options.offset) {
      query = query.offset(options.offset);
    }

    const chunks = await query;

    if (chunks.length === 0) {
      spinner.warn('No code chunks found in the database.');
      return;
    }

    spinner.succeed(`Found ${chunks.length} code chunks`);

    // Display results in a formatted table
    console.log('\nðŸ“Š Code Chunks Index:');
    console.log('â•'.repeat(100));
    console.table(chunks.map(chunk => ({
      ID: chunk.id,
      'File Path': chunk.file_path.replace(process.cwd(), '.'),
      Type: chunk.chunk_type,
      Name: chunk.chunk_name,
      Profile: chunk.profile || 'default'
    })));

    // Show summary
    const totalCount = await knex('code_chunks').count('* as count').first();
    console.log(`\nShowing ${chunks.length} of ${totalCount.count} total chunks`);

    if (options.limit && chunks.length === options.limit) {
      console.log(`\nðŸ’¡ Use --offset ${(options.offset || 0) + options.limit} to view the next page`);
    }

  } catch (error) {
    spinner.fail(`Failed to view index: ${error.message}`);
  } finally {
    await destroyDb();
  }
}