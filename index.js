#!/usr/bin/env node



import { Command } from 'commander';
import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import isBinaryPath from 'is-binary-path';
import { fileURLToPath } from 'url';
import ignore from 'ignore';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
// NEW: Import inquirer for user prompts
import inquirer from 'inquirer';

const __dirname = path.dirname(fileURLToPath(import.meta.url));
const gzip = promisify(zlib.gzip);
const gunzip = promisify(zlib.gunzip);

const DEFAULT_CONFIG = {
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc', '.class', '.o', '.so', '.dylib'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/', '.next/', '.nuxt/', 'target/', 'bin/', 'obj/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};



/**
 * Generates the detailed, universal Markdown header for the snapshot file.
 * This header contains the full operational instructions for the AI model.
 * @param {object} stats - The statistics object from the snapshot process.
 * @param {string} repoName - The name of the repository.
 * @param {boolean} includeAiInstructions - Whether to include AI instructions.
 * @param {object} config - The configuration object containing environment definitions.
 * @returns {string} A formatted Markdown string with comprehensive AI instructions.
 */
function generateSnapshotHeader(stats, repoName, includeAiInstructions = true, config = null) {
  const timestamp = new Date().toISOString();
  
  if (!includeAiInstructions) {
    return `# Repository Snapshot

**Repository:** ${repoName}  
**Generated:** ${timestamp}  
**Tool:** eck-snapshot  
**Files Included:** ${stats.includedFiles} of ${stats.totalFiles}

---

`;
  }

  
  return `# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **${repoName}** software repository, generated by the \`eck-snapshot\` tool on **${timestamp}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by \`--- File: /path/to/file ---\` headers.

**Snapshot Stats:**
- **Files Included:** ${stats.includedFiles}
- **Total Files in Repo:** ${stats.totalFiles}

---

## 2. Environment Self-Detection Protocol

**CRITICAL:** Before proceeding with any operations, you MUST determine your execution environment by running the following detection commands and comparing the results to the environment patterns defined in this project's configuration.

### Detection Commands to Run:
\`\`\`bash
whoami
echo $NODE_ENV
echo $USER
echo $HOSTNAME
echo $CI
echo $GITHUB_ACTIONS
echo $DISPLAY
\`\`\`

### Environment Definitions:
${config && config.environments ? Object.entries(config.environments).map(([key, env]) => `
**${env.name} (${key})**
- Description: ${env.description}
- GUI Available: ${env.hasGUI ? 'Yes' : 'No'}
- Detection Patterns: ${JSON.stringify(env.detectionPatterns || {}, null, 2)}
- Allowed Commands: ${env.allowedCommands?.join(', ') || 'Standard operations'}
- Prohibited Commands: ${env.prohibitedCommands?.join(', ') || 'None'}
`).join('\n') : 'No environment definitions found in config'}

### Self-Detection Steps:
1. **Run the detection commands above**
2. **Compare output** to the detection patterns for each environment
3. **Identify your environment** based on matching patterns
4. **Follow the constraints** for your identified environment throughout the session
5. **If no environment matches**, assume a restrictive server environment and avoid GUI operations

**‚ö†Ô∏è CRITICAL WARNING:** If you detect a headless/server environment (no DISPLAY variable, production NODE_ENV, CI environment), do NOT attempt to run GUI applications, browser automation, or interactive development tools. Focus on server-appropriate commands only.

---

## 3. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for a code-execution AI agent.

### PROJECT OVERVIEW
- **Project:** ${repoName}
- **Description:** A CLI tool to create and restore single-file text snapshots of a Git repository, optimized for providing context to Large Language Models (LLMs).

### CORE WORKFLOW: The Interactive Command Cycle
1.  **Analyze User Request:** Understand the user's goal in their native language.
2.  **Formulate a Plan:** Create a high-level technical plan to solve the user's request.
3.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
4.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
    -   **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
    -   **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
5.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
6.  **Iterate:** Continue the cycle based on user feedback.

### COMMUNICATION PROTOCOL
-   **User Interaction:** ALWAYS communicate with the user in the language they use.
-   **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.

### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for the agent must be presented in a special block with a "Copy" button. Use this enhanced format for maximum clarity and execution accuracy:

\`\`\`json
{
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes"
  }
}
\`\`\`

### PROJECT CONTEXT (\`eck-snapshot\`)
-   **Type:** Node.js CLI Application, executed directly.
-   **Module System:** ES Modules (\`"type": "module"\` in package.json).
-   **Main File:** \`index.js\` contains all primary logic (837 lines).
-   **Configuration:** \`.ecksnapshot.config.js\` is used for custom filtering and settings.
-   **Key Dependencies:** \`commander\`, \`execa\`, \`inquirer\`, \`ignore\`, \`p-limit\`, \`cli-progress\`.

### ARCHITECTURE DETAILS FOR CLAUDE CODE
**Core Functions Location:**
- \`createRepoSnapshot()\` - Line 333: Main snapshot creation
- \`restoreSnapshot()\` - Line 579: Snapshot restoration  
- \`processFile()\` - Line 265: Individual file processing
- \`generateDirectoryTree()\` - Line 224: Tree generation
- \`generateSnapshotHeader()\` - Line 42: AI instruction header
- CLI setup - Lines 800-837: Commander.js configuration

**Common Modification Patterns:**
- CLI options: Modify commander setup (lines 808-822, 824-835)
- Configuration: Update DEFAULT_CONFIG object (lines 23-31)
- File processing: Enhance processFile() function
- Output formats: Modify generateSnapshotHeader() or output logic
- Dependencies: Update package.json and import statements

**Testing Status:**
- No test framework currently configured
- package.json test script returns error
- Manual testing via \`node index.js\` commands
- Consider adding vitest or jest for future testing

**Development Workflow:**
- Direct execution: \`node index.js [command] [options]\`
- Package creation: \`npm pack\`
- Local testing: \`node index.js --help\`
- Configuration testing: modify \`.ecksnapshot.config.js\`

**Critical Implementation Notes:**
- All file paths normalized to forward slashes
- ES module imports only (no CommonJS)
- Error handling with detailed user messages
- Progress tracking for long operations
- Security: Path validation prevents directory traversal
- Cross-platform compatibility maintained

---
`;
}





function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

function matchesPattern(filePath, patterns) {
    const fileName = path.basename(filePath);
    return patterns.some(pattern => {
        const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
        try {
            const regex = new RegExp(regexPattern);
            return regex.test(fileName);
        } catch (e) {
            console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
            return false;
        }
    });
}


async function loadConfig(configPath) {
  let config = { ...DEFAULT_CONFIG };
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è  Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}


async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath) {
  const files = [];
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      // Skip if matches ignore patterns
      if (config.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      // Skip hidden files unless explicitly included
      if (!config.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, config, relativeTo);
        files.push(...subFiles);
      } else {
        // Skip ignored files and extensions
        if (config.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, config.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è  Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è  No .gitignore file found or could not be read');
    return ignore();
  }
}

async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è  Warning: Could not read directory: ${dir}`);
    return '';
  }
}

async function processFile(filePath, config, gitignore, stats) {
  const fileName = path.basename(filePath);
  const fileExt = path.extname(filePath) || 'no-extension';
  const normalizedPath = filePath.replace(/\\/g, '/');
  
  let skipReason = null;
  
  if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
    skipReason = 'ignored-directory';
  } else if (config.extensionsToIgnore.includes(path.extname(filePath))) {
    skipReason = 'ignored-extension';
  } else if (matchesPattern(filePath, config.filesToIgnore)) {
    skipReason = 'ignored-pattern';
  } else if (gitignore.ignores(normalizedPath)) {
    skipReason = 'gitignore';
  } else if (isBinaryPath(filePath)) {
    skipReason = 'binary-file';
  } else if (!config.includeHidden && fileName.startsWith('.')) {
    skipReason = 'hidden-file';
  }

  if (skipReason) {
    stats.skippedFiles++;
    stats.skippedFileTypes.set(fileExt, (stats.skippedFileTypes.get(fileExt) || 0) + 1);
    stats.skipReasons.set(skipReason, (stats.skipReasons.get(skipReason) || 0) + 1);
    
    if (!stats.skippedFilesDetails.has(skipReason)) {
      stats.skippedFilesDetails.set(skipReason, []);
    }
    stats.skippedFilesDetails.get(skipReason).push({ file: filePath, ext: fileExt });
    
    if (skipReason === 'binary-file') stats.binaryFiles++;
    return { skipped: true, reason: skipReason };
  }

  try {
    const content = await readFileWithSizeCheck(filePath, parseSize(config.maxFileSize));
    const fileContent = `--- File: /${normalizedPath} ---\n\n${content}\n\n`;
    
    stats.includedFiles++;
    stats.includedFileTypes.set(fileExt, (stats.includedFileTypes.get(fileExt) || 0) + 1);
    return { content: fileContent, size: fileContent.length };
  } catch (error) {
    const errorReason = error.message.includes('too large') ?
      'file-too-large' : 'read-error';
    
    stats.errors.push({ file: filePath, error: error.message });
    stats.skippedFiles++;
    stats.skippedFileTypes.set(fileExt, (stats.skippedFileTypes.get(fileExt) || 0) + 1);
    stats.skipReasons.set(errorReason, (stats.skipReasons.get(errorReason) || 0) + 1);
    
    if (!stats.skippedFilesDetails.has(errorReason)) {
      stats.skippedFilesDetails.set(errorReason, []);
    }
    stats.skippedFilesDetails.get(errorReason).push({ file: filePath, ext: fileExt });
    
    if (error.message.includes('too large')) {
      stats.largeFiles++;
    }
    
    return { skipped: true, reason: error.message };
  }
}
// --- –ö–û–ù–ï–¶ –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –§–£–ù–ö–¶–ò–ô ---


// --- –û–°–ù–û–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ö–û–ú–ê–ù–î ---

async function createRepoSnapshot(repoPath, options) {
  const absoluteRepoPath = path.resolve(repoPath);
  const absoluteOutputPath = path.resolve(options.output);
  const originalCwd = process.cwd();

  console.log(`üöÄ Starting snapshot for ${options.dir ? 'directory' : 'repository'}: ${absoluteRepoPath}`);
  console.log(`üìÅ Snapshots will be saved to: ${absoluteOutputPath}`);

  try {
    const config = await loadConfig(options.config);
    config.maxFileSize = options.maxFileSize || config.maxFileSize;
    config.maxTotalSize = options.maxTotalSize || config.maxTotalSize;
    config.maxDepth = options.maxDepth || config.maxDepth;
    config.includeHidden = options.includeHidden || false;
    
    let allFiles = [];
    let gitignore = null;
    let isGitRepo = false;
    
    // Check if it's a git repository (unless --dir is explicitly used)
    if (!options.dir) {
      await checkGitAvailability();
      isGitRepo = await checkGitRepository(absoluteRepoPath);
      
      if (!isGitRepo) {
        console.log('‚ÑπÔ∏è  Not a git repository, switching to directory mode');
        options.dir = true;
      }
    }
    
    process.chdir(absoluteRepoPath);
    console.log('‚úÖ Successfully changed working directory');

    if (options.dir) {
      console.log('üìã Scanning directory recursively...');
      allFiles = await scanDirectoryRecursively(absoluteRepoPath, config);
      gitignore = ignore(); // Empty gitignore for directory mode
      console.log(`üìä Found ${allFiles.length} total files in the directory`);
    } else {
      gitignore = await loadGitignore(absoluteRepoPath);
      console.log('üìã Fetching file list from Git...');
      const { stdout } = await execa('git', ['ls-files']);
      allFiles = stdout.split('\n').filter(Boolean);
      console.log(`üìä Found ${allFiles.length} total files in the repository`);
    }

    const stats = {
      totalFiles: allFiles.length,
      includedFiles: 0,
      skippedFiles: 0,
      binaryFiles: 0,
      largeFiles: 0,
      errors: [],
      includedFileTypes: new Map(),
      skippedFileTypes: new Map(),
      skipReasons: new Map(),
      skippedFilesDetails: new Map()
    };
    let snapshotContent = '';

    if (options.tree) {
      console.log('üå≥ Generating directory tree...');
      const tree = await generateDirectoryTree(absoluteRepoPath, '', allFiles, 0, config.maxDepth, config);
      snapshotContent += 'Directory Structure:\n\n';
      snapshotContent += tree;
      snapshotContent += '\n\n';
    }

    console.log('üìù Processing files...');
    const limit = pLimit(config.concurrency);
    const progressBar = options.verbose ?
      null : new SingleBar({
      format: 'Progress |{bar}| {percentage}% | {value}/{total} files | ETA: {eta}s',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    if (progressBar) progressBar.start(allFiles.length, 0);

    const filePromises = allFiles.map((filePath, index) => 
      limit(async () => {
        const result = await processFile(filePath, config, gitignore, stats);
        
        if (progressBar) {
          progressBar.update(index + 1);
        } else if (options.verbose) {
          if (result.skipped) {
            
            console.log(`‚è≠Ô∏è  Skipping: ${filePath} (${result.reason})`);
          } else {
            console.log(`‚úÖ Processed: ${filePath}`);
          }
        }
        
        return result;
      })
    );
    const results = await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    const contentArray = [];
    let totalSize = 0;
    const maxTotalSize = parseSize(config.maxTotalSize);
    for (const result of results) {
        if (result.status === 'rejected') {
            console.warn(`‚ö†Ô∏è Promise rejected: ${result.reason}`);
            continue;
        }
      if (result.value && result.value.content) {
        if (totalSize + result.value.size > maxTotalSize) {
          console.warn(`‚ö†Ô∏è  Warning: Approaching size limit. Some files may be excluded.`);
          break;
        }
        contentArray.push(result.value.content);
        totalSize += result.value.size;
      }
    }
    
    // Add the header to the beginning of the file content
    const repoName = path.basename(absoluteRepoPath);
    const header = generateSnapshotHeader(stats, repoName, options.aiHeader !== false, config);
    snapshotContent = header + snapshotContent + contentArray.join('');

    const totalChars = snapshotContent.length;
    const estimatedTokens = Math.round(totalChars / 4);

    const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
    const extension = options.format === 'json' ? 'json' : 'md'; // Changed default to md
    let outputFilename = `${repoName}_snapshot_${timestamp}.${extension}`;
    if (options.compress) {
      outputFilename += '.gz';
    }

    const fullOutputFilePath = path.join(absoluteOutputPath, outputFilename);
    let finalContent = snapshotContent;
    if (options.format === 'json') {
      const jsonData = {
        repository: repoName,
        timestamp: new Date().toISOString(),
        stats: {
          ...stats,
          includedFileTypes: Object.fromEntries(stats.includedFileTypes),
          skippedFileTypes: Object.fromEntries(stats.skippedFileTypes),
          skipReasons: Object.fromEntries(stats.skipReasons),
          skippedFilesDetails: Object.fromEntries(
 
            Array.from(stats.skippedFilesDetails.entries()).map(([reason, files]) => [
              reason, 
              files.map(({file, ext}) => ({file, ext}))
            ])
          )
        },
        content: snapshotContent
      };
      finalContent = JSON.stringify(jsonData, null, 2);
    }

    await fs.mkdir(absoluteOutputPath, { recursive: true });
    if (options.compress) {
      const compressed = await gzip(finalContent);
      await fs.writeFile(fullOutputFilePath, compressed);
    } else {
      await fs.writeFile(fullOutputFilePath, finalContent);
    }

    console.log('\nüìä Snapshot Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Snapshot created successfully!`);
    console.log(`üìÑ File saved to: ${fullOutputFilePath}`);
    console.log(`üìà Included text files: ${stats.includedFiles} of ${stats.totalFiles}`);
    console.log(`‚è≠Ô∏è  Skipped files: ${stats.skippedFiles}`);
    console.log(`üî¢ Binary files skipped: ${stats.binaryFiles}`);
    console.log(`üìè Large files skipped: ${stats.largeFiles}`);
    if (options.tree) console.log('üå≥ Directory tree included');
    if (options.compress) console.log('üóúÔ∏è  File compressed with gzip');
    console.log(`üìä Total characters: ${totalChars.toLocaleString('en-US')}`);
    console.log(`üéØ Estimated tokens: ~${estimatedTokens.toLocaleString('en-US')}`);
    console.log(`üíæ File size: ${formatSize(totalChars)}`);
    
    if (stats.includedFileTypes.size > 0) {
      console.log('\nüìã Included File Types Distribution:');
      const sortedIncludedTypes = Array.from(stats.includedFileTypes.entries())
        .sort(([,a], [,b]) => b - a)
        .slice(0, 10);
      for (const [ext, count] of sortedIncludedTypes) {
        console.log(`  ${ext}: ${count} files`);
      }
    }

    if (stats.skippedFileTypes.size > 0) {
      console.log('\n‚è≠Ô∏è  Skipped File Types Distribution:');
      const sortedSkippedTypes = Array.from(stats.skippedFileTypes.entries())
        .sort(([,a], [,b]) => b - a)
        .slice(0, 10);
      for (const [ext, count] of sortedSkippedTypes) {
        console.log(`  ${ext}: ${count} files`);
      }
    }

    if (stats.skipReasons.size > 0) {
      console.log('\nüìä Skip Reasons:');
      const sortedReasons = Array.from(stats.skipReasons.entries())
        .sort(([,a], [,b]) => b - a);
      const reasonLabels = {
        'ignored-directory': 'Ignored directories',
        'ignored-extension': 'Ignored extensions',
        'ignored-pattern': 'Ignored patterns',
        'gitignore': 'Gitignore rules',
        'binary-file': 'Binary files',
        'hidden-file': 'Hidden files',
        'file-too-large': 'Files too large',
        'read-error': 'Read errors'
      };
      for (const [reason, count] of sortedReasons) {
        const label = reasonLabels[reason] || reason;
        console.log(`  ${label}: ${count} files`);
        
        if (stats.skippedFilesDetails.has(reason)) {
          const files = stats.skippedFilesDetails.get(reason);
          const filesToShow = files.slice(0, 10);
          
          for (const {file, ext} of filesToShow) {
            console.log(`    ‚Ä¢ ${file} (${ext})`);
          }
          
          if (files.length > 10) {
            console.log(`    ... and ${files.length - 10} more files`);
          }
        }
        console.log();
      }
    }

    if (stats.errors.length > 0) {
      console.log('\n‚ö†Ô∏è  Errors encountered:');
      stats.errors.slice(0, 5).forEach(({ file, error }) => {
        console.log(`  ${file}: ${error}`);
      });
      if (stats.errors.length > 5) {
        console.log(`  ... and ${stats.errors.length - 5} more errors`);
      }
    }
    
    console.log('='.repeat(50));
  } catch (error) {
    console.error('\n‚ùå An error occurred:');
    if (error.code === 'ENOENT' && error.path && error.path.includes('.git')) {
      console.error(`Error: The path "${absoluteRepoPath}" does not seem to be a Git repository.`);
    } else if (error.message.includes('Git is not installed')) {
      console.error('Error: Git is not installed or not available in PATH.');
      console.error('Please install Git and ensure it\'s available in your system PATH.');
    } else if (error.message.includes('Not a git repository')) {
      console.error(error.message);
      console.error('Please run this command from within a Git repository or provide a valid repository path.');
    } else {
      console.error(error.message);
      if (options.verbose) {
        console.error(error.stack);
      }
    }
    process.exit(1);
  } finally {
    process.chdir(originalCwd);
  }
}

async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  console.log(`üîÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è  No files found to restore in the snapshot');
      return;
    }

    // Apply filters if specified
    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è  No files remaining after applying filters');
        return;
      }
    }

    // Validate file paths for security
    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
 
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
 
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
     
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è  Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}


// --- CLI SETUP ---
const program = new Command();

program
  .name('eck-snapshot')
  .description('A CLI tool to create and restore single-file text snapshots of a Git repository.')
  .version('3.0.0');

// Snapshot command (existing)
program
  .command('snapshot', { isDefault: true })
  .description('Create a snapshot of a Git repository (default command).')
  .argument('[repoPath]', 'Path to the git repository to snapshot.', process.cwd())
.option('-o, --output <dir>', 'Output directory for the snapshot file.', path.join(__dirname, 'snapshots'))
  .option('--no-tree', 'Do not include the directory tree in the snapshot.')
  .option('-v, --verbose', 'Show detailed processing information, including skipped files.')
  .option('--max-file-size <size>', 'Maximum file size to include (e.g., 10MB)', '10MB')
  .option('--max-total-size <size>', 'Maximum total snapshot size (e.g., 100MB)', '100MB')
  .option('--max-depth <number>', 'Maximum directory depth for tree generation', (val) => parseInt(val), 10)
  .option('--config <path>', 'Path to configuration file')
  .option('--compress', 'Compress output file with gzip')
  .option('--include-hidden', 'Include hidden files (starting with .)')
  .option('--format <type>', 'Output format: md, json', 'md')
  .option('--no-ai-header', 'Skip AI instruction header (create clean snapshot)')
  .option('-d, --dir', 'Directory mode: scan directory recursively (auto-enabled if no git repo found)')
  .action((repoPath, options) => createRepoSnapshot(repoPath, options));

program
  .command('restore')
  .description('Restore files and directories from a snapshot file')
  .argument('<snapshot_file>', 'Path to the snapshot file (.txt, .json, or .gz)')
  .argument('[target_directory]', 'Directory to restore the files into', process.cwd())
  .option('-f, --force', 'Force overwrite of existing files without confirmation')
  .option('-v, --verbose', 'Show detailed processing information')
  .option('--dry-run', 'Show what would be restored without actually writing files')
  .option('--include <patterns...>', 'Include only files matching these patterns (supports wildcards)')
  .option('--exclude <patterns...>', 'Exclude files matching these patterns (supports wildcards)')
  .option('--concurrency <number>', 'Number of concurrent file operations', (val) => parseInt(val), 10)
  .action((snapshotFile, targetDir, options) => restoreSnapshot(snapshotFile, targetDir, options));

program.parse(process.argv);