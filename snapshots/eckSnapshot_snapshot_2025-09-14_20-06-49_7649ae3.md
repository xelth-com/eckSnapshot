
# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-09-14T20:06:49.352Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 31
- **Total Files in Repo:** 32


## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

### PROJECT-SPECIFIC MANIFEST (.eck Directory)

Some projects include a `.eck` directory containing project-specific context and configuration. When present, this directory provides crucial information that should inform all your technical decisions:

- **CONTEXT.md**: Project-specific background, architecture notes, and important context
- **OPERATIONS.md**: Common operations, deployment procedures, and workflow guidance
- **JOURNAL.md**: Development history, recent changes, and ongoing notes
- **ENVIRONMENT.md**: Environment-specific settings that override auto-detected values (YAML format)

**CRITICAL**: Always prioritize information from the .eck manifest when it conflicts with auto-detected project settings. Use this context when formulating plans and briefing execution agents.

### CORE WORKFLOW: The Interactive Command Cycle
1. **Check Environment:** Request ENV scan from agent first
2. **Analyze User Request:** Understand the user's goal in their native language.
3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
   - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
   - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7. **Iterate:** Continue the cycle based on user feedback.

### COMMUNICATION PROTOCOL
- **User Interaction:** ALWAYS communicate with the user in the language they use.
- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces


### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes"
  }
}
```

---

## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queryProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ segmenter.js
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresConnector.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_simple.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysisService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embeddingService.js
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ .eck-token-training.json
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ ideas.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ knexfile.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ setup.json
```

--- File: /.claude/settings.local.json ---

{
  "permissions": {
    "allow": [
      "Bash(node:*)",
      "Bash(git add:*)",
      "Bash(NODE_ENV=production node index.js --no-tree -o /tmp)",
      "Bash(cat:*)",
      "Bash(NODE_ENV=production USER=root unset DISPLAY)",
      "Bash(env)",
      "Bash(env:*)",
      "Bash(mkdir:*)",
      "Bash(npm install)",
      "Bash(grep:*)",
      "Bash(sed:*)",
      "Bash(awk:*)",
      "Bash(rm:*)",
      "Bash(git push:*)",
      "Bash(npm install:*)",
      "Bash(git checkout:*)",
      "Bash(timeout:*)",
      "Bash(git rm:*)",
      "Bash(git reset:*)",
      "Bash(npm run build:*)",
      "Bash(npm run:*)",
      "Bash(./bin/eck-snapshot.js:*)",
      "Read(/tmp/test_hygiene_repo/**)"
    ],
    "deny": [],
    "defaultMode": "acceptEdits"
  }
}

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      4388.725796624385,
      0.2507418280225041,
      0,
      0
    ],
    "nodejs": [
      0,
      0.2,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ]
  },
  "trainingPoints": {
    "android": [
      {
        "fileSizeInBytes": 276377,
        "estimatedTokens": 68882,
        "actualTokens": 73688,
        "timestamp": "2025-09-10T10:52:55.067Z"
      },
      {
        "fileSizeInBytes": 150000,
        "estimatedTokens": 37500,
        "actualTokens": 42000,
        "timestamp": "2025-09-10T10:54:22.647Z"
      }
    ],
    "nodejs": [
      {
        "fileSizeInBytes": 206870,
        "estimatedTokens": 41374,
        "actualTokens": 59269,
        "timestamp": "2025-09-14T13:45:11.710Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Google Gemini API Key - Required for the 'index' and 'query' commands
# Get your key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_API_KEY_HERE"

# PostgreSQL Connection Details
DB_HOST=127.0.0.1
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_DATABASE=eck_snapshot_db

--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist

# Snapshots
/snapshots/

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# EckSnapshot Index
/.ecksnapshot_index/

# Generated snapshot files
*_vectors.json
*_rag_*.md

# Environment variables
.env
.env.local
.env.production

# Added by eck-snapshot to prevent committing snapshots
snapshots/


--- File: /LICENSE ---

MIT License

Copyright (c) 2025 Dmytro Surovtsev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


--- File: /README.md ---


# eck-snapshot

[](https://www.google.com/search?q=https://www.npmjs.com/package/%40xelth/eck-snapshot)
[](https://opensource.org/licenses/MIT)

**`eck-snapshot` is a local-first, AI-powered intelligence platform for your codebase. It transforms any code repository into a sophisticated, queryable database, enabling a new workflow for AI-assisted development called "Vibecoding".**

## The Philosophy: What is Vibecoding?

Modern AI development tools often fail because they operate on a narrow context, forcing the AI to guess and hallucinate. **Vibecoding** is a developer workflow focused on maintaining a creative flow by providing AI agents with a near-perfect, holistic understanding of the entire codebase.

Instead of manually feeding snippets of code to an AI, `eck-snapshot` allows you to ask high-level questions and receive a comprehensive, context-aware "snapshot" that includes all semantically and structurally relevant code. This allows the developer to act as the **Architect**, guiding the project's vision, while the AI acts as a flawless **Executor**.

## Key Features

  - **üß† Multi-Language Intelligence:** Deeply understands your code using advanced parsers for **JavaScript/TypeScript** (Babel), **Python**, and **Android (Java/Kotlin)** (Tree-sitter).
  - **üóÑÔ∏è Hybrid Database Backend:** Creates a powerful local knowledge base using **PostgreSQL**, combining:
      - **Vector Search** (`pgvector`) for finding code by semantic meaning.
      - **Graph Database** (`Apache AGE`) for understanding the structural relationships between code.
  - **üîê Local-First AI:** All AI models for code analysis (summarization) and indexing (embeddings) run **100% locally** on your machine via `Transformers.js`. Your code never leaves your computer.
  - **ü§ñ Hybrid RAG Search:** A powerful `query` command that combines vector and graph search to produce incredibly rich, context-aware code snapshots for your LLM.
  - **üì∏ Classic Snapshot Mode:** The original `snapshot` command is still available for creating single-file snapshots of smaller projects.

## How It Works

`eck-snapshot` implements a sophisticated pipeline to analyze and index your code:

`[Codebase] -> [Multi-Language Parsers] -> [Local AI Enrichment (Summaries & Embeddings)] -> [PostgreSQL (Vectors + Graph)] -> [Hybrid RAG Query]`

## Installation

```bash
npm install -g @xelth/eck-snapshot
```

## Setup

`eck-snapshot` uses a powerful local database and AI models. Follow these steps for the initial setup.

### Step 1: Set up PostgreSQL

A running PostgreSQL instance with the `pgvector` and `Apache AGE` extensions is required. The easiest way to get this running is with Docker.

1.  Create a `docker-compose.yml` file in an empty directory:
    ```yaml
    version: '3.8'
    services:
      postgres-db:
        image: ivans-big-data/pg-vector-and-graph:16
        container_name: eck-snapshot-db
        ports:
          - "5432:5432"
        environment:
          - POSTGRES_USER=myuser
          - POSTGRES_PASSWORD=mypassword
          - POSTGRES_DB=eck_snapshot_db
        volumes:
          - ./pg_data:/var/lib/postgresql/data
    ```
2.  Run `docker-compose up -d` in that directory. This will start your database.

### Step 2: Configure Environment

`eck-snapshot` needs to know how to connect to your database.

1.  Find the `eck-snapshot` installation directory.
2.  Copy the `.env.example` file to a new file named `.env` in that same directory.
3.  Edit the `.env` file with the database credentials you set in `docker-compose.yml`.
    ```dotenv
    # PostgreSQL Connection Details
    DB_HOST=127.0.0.1
    DB_PORT=5432
    DB_USER=myuser
    DB_PASSWORD=mypassword
    DB_DATABASE=eck_snapshot_db
    ```

### Step 3: AI Model Caching

The first time you run the `index` command, `Transformers.js` will automatically download the open-source AI models (several GBs). This is a one-time process; the models will be cached for all future use.

## Usage Workflow

### Step 1: Index Your Project

Navigate to your project's root directory and run the `index` command. This will kick off the full analysis pipeline.

```bash
# This will scan, parse, analyze, and store your entire project in PostgreSQL.
# This may take a long time on the first run.
eck-snapshot index .
```

### Step 2: Query Your Codebase

Once your project is indexed, you can ask questions in natural language.

```bash
# Ask for all code related to user authentication
eck-snapshot query "user authentication logic"

# Ask for code related to scanner functionality, getting more results
eck-snapshot query "scanner functionality" -k 15
```

This will perform the hybrid vector-and-graph search and generate a `rag_snapshot_... .md` file in your current directory, containing all the relevant code needed for your task.

## Command Reference

  - `eck-snapshot index [path]`: Scans and indexes a repository into the PostgreSQL database.
  - `eck-snapshot query "<your query>"`: Performs a hybrid search and generates a context-aware RAG snapshot.
  - `eck-snapshot snapshot [path]`: Creates a classic single-file snapshot of an entire project.
  - `eck-snapshot detect [path]`: Detects the project type and configuration.

For more options on any command, run it with the `--help` flag.

## Contributing

Contributions are welcome\! This project was built with the help of AI and is a testament to a new way of building software. Please feel free to submit a Pull Request or open an issue on our GitHub repository.

## License

This project is licensed under the MIT License. See the [LICENSE](https://www.google.com/search?q=https://github.com/xelth-com/eckSnapshot/blob/main/LICENSE) file for details.

--- File: /ideas.md ---

# –ò–¥–µ–∏ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Eck-Snapshot –∏ Claude Code

## –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:

1.  **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:** –°–∞–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± –æ–±—â–µ–Ω–∏—è —Å Claude ‚Äî —ç—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –µ–≥–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π, –∞ –Ω–µ –ø–æ–ø—ã—Ç–∫–∞ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ "–æ–¥–∏–Ω –¥–ª—è –≤—Å–µ—Ö".
2.  **–ö–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –∫–æ—Ä–æ–ª—å, –Ω–æ —Ü–µ–ª–µ–≤–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Äî –ª—É—á—à–µ:** –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ Claude –∞–∫—Ç–∏–≤–Ω–æ —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –∏ –º–µ–Ω—å—à–µ, —á–µ–º –∫–∞–∂–µ—Ç—Å—è. –ü–æ—ç—Ç–æ–º—É –æ—Ç–ø—Ä–∞–≤–∫–∞ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –º–∞—Å—Å–∏–≤–Ω–æ–≥–æ, –Ω–µ—Ü–µ–ª–µ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö.
3.  **–î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å, –∞ –Ω–µ –¥–∏–∫—Ç–æ–≤–∞—Ç—å:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Gemini –≤ –∫–∞—á–µ—Å—Ç–≤–µ "–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞" –¥–ª—è –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è –æ–±—â–µ–π –∫–∞—Ä—Ç–∏–Ω—ã –∏ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é, —á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É Claude, "–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç—É".

## –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è Claude (`CLAUDE.md`)

–í–º–µ—Å—Ç–æ —Ç–æ–≥–æ —á—Ç–æ–±—ã –ø–æ–¥–∞–≤–∞—Ç—å –≤–µ—Å—å —Å–Ω—ç–ø—à–æ—Ç –≤ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç, –º—ã –¥–æ–ª–∂–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–∞–π–ª `CLAUDE.md` –≤ –∫–æ—Ä–Ω–µ–≤–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ –ø—Ä–æ–µ–∫—Ç–∞ `eckWms`.

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è `eckWms/CLAUDE.md`:

1.  **–û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞:** –ö—Ä–∞—Ç–∫–æ–µ, –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ `eckWms`. –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç? –ö–∞–∫–æ–≤–∞ –µ–≥–æ –æ—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å?
2.  **–ö–ª—é—á–µ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:** –ö—Ä–∞—Ç–∫–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –≥–ª–∞–≤–Ω—ã—Ö –º–æ–¥—É–ª–µ–π –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.
3.  **–û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:** –°–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º, –ø–æ—á–µ–º—É –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è.
4.  **–°–æ–≥–ª–∞—à–µ–Ω–∏—è –ø–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—é:** –õ—é–±—ã–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ –ø–æ —Å—Ç–∏–ª—é –∏–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –∫–æ—Ç–æ—Ä—ã–º –¥–æ–ª–∂–µ–Ω —Å–ª–µ–¥–æ–≤–∞—Ç—å Claude (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–í—Å–µ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º React Hooks", "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ —Å –ø–æ–º–æ—â—å—é Zustand").
5.  **–¢–∏–ø–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏:** –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é —Ä—É—Ç–∏–Ω–Ω—ã—Ö –∑–∞–¥–∞—á, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ —Ç–æ–º—É, —á—Ç–æ –±—ã–ª–æ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ AI Instructions (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ß—Ç–æ–±—ã –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—É—é –∫–æ–Ω–µ—á–Ω—É—é —Ç–æ—á–∫—É API, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏–∑–º–µ–Ω–∏—Ç—å `routes.js`, —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –≤ `controllers/` –∏ –¥–æ–±–∞–≤–∏—Ç—å —Å–µ—Ä–≤–∏—Å –≤ `services/`).

**–í–∞–∂–Ω–æ: —ç—Ç–æ—Ç —Ñ–∞–π–ª –ù–ï –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø–æ–ª–Ω—ã–π —Å–Ω—ç–ø—à–æ—Ç –∫–æ–¥–∞.** –ï–≥–æ —Ü–µ–ª—å ‚Äî –¥–∞—Ç—å Claude –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ, –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ "–ø—Ä–∞–≤–∏–ª–∞ –∏–≥—Ä—ã" –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.

## –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é (—á–∞—Å—Ç—å `eck-snapshot`)

–≠—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –º–µ–Ω—è–µ—Ç—Å—è —Å –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–µ–π.

1.  **–ú–∏–Ω–∏-—Å–Ω—ç–ø—à–æ—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤:** –û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é. –ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä (Gemini) –ø–æ–π–º–µ—Ç –∑–∞–¥–∞—á—É, –æ–Ω –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `eck-snapshot query "..."`, —á—Ç–æ–±—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π, —Ü–µ–ª–µ–≤–æ–π —Å–Ω—ç–ø—à–æ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏. –ò–º–µ–Ω–Ω–æ —ç—Ç–æ—Ç –º–∏–Ω–∏-—Å–Ω—ç–ø—à–æ—Ç –ø–æ–¥–∞–µ—Ç—Å—è –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è Claude.

2.  **–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∏–º–ø–æ—Ä—Ç—ã –≤ `CLAUDE.md`:** –î–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ, –Ω–æ –≤—Å–µ –∂–µ –∑–∞–≤–∏—Å—è—â–µ–≥–æ –æ—Ç –∑–∞–¥–∞—á–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –∏–º–ø–æ—Ä—Ç–∞ `@`. –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ –≤—Å–µ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞–Ω–∏—è —Å—Ö–µ–º—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –º—ã –º–æ–≥–ª–∏ –±—ã –∏–º–µ—Ç—å —Å–Ω—ç–ø—à–æ—Ç —Ç–æ–ª—å–∫–æ —Å—Ö–µ–º—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å `@/path/to/schema_snapshot.md` –≤ `CLAUDE.md` –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Å–µ—Å—Å–∏–∏.

## –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–ª—ç—à-–∫–æ–º–∞–Ω–¥—ã

–î–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–∂–Ω—ã—Ö, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–¥–∞—á –º—ã –¥–æ–ª–∂–Ω—ã —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å–ª—ç—à-–∫–æ–º–∞–Ω–¥—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ `eckWms`.

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞—Ç–∞–ª–æ–≥–æ–≤:
`eckWms/.claude/commands/eck/`

### –ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–∞–Ω–¥:

* **/eck:new-component.md**
    * **–û–ø–∏—Å–∞–Ω–∏–µ:** –°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç React —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –∏ —Ñ–∞–π–ª–∞–º–∏.
    * **–ü—Ä–æ–º–ø—Ç:** "–°–æ–∑–¥–∞–π –Ω–æ–≤—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç React —Å –∏–º–µ–Ω–µ–º `{arg1}`. –û–Ω –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TailwindCSS –¥–ª—è —Å—Ç–∏–ª–µ–π –∏ —Ä–∞—Å–ø–æ–ª–∞–≥–∞—Ç—å—Å—è –≤ –∫–∞—Ç–∞–ª–æ–≥–µ `src/components/`. –°–æ–∑–¥–∞–π –∫–∞–∫ —Ñ–∞–π–ª `.jsx`, —Ç–∞–∫ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª `.css`."

* **/eck:commit-and-push.md**
    * **–û–ø–∏—Å–∞–Ω–∏–µ:** –§–∏–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ —Ç–µ–∫—É—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å –∫–æ–Ω–≤–µ–Ω—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º –∫–æ–º–º–∏—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –≤ –Ω–æ–≤—É—é –≤–µ—Ç–∫—É.
    * **–ü—Ä–æ–º–ø—Ç:** "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—É—â–∏–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–∑ `!git diff --staged`. –°–æ–∑–¥–∞–π –∫–æ–Ω–≤–µ–Ω—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–æ–º–º–∏—Ç–∞, –∫–æ—Ç–æ—Ä–æ–µ —Ç–æ—á–Ω–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç —ç—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è. –ó–∞—Ç–µ–º –∑–∞–∫–æ–º–º–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –æ—Ç–ø—Ä–∞–≤—å –∏—Ö –≤ –Ω–æ–≤—É—é –≤–µ—Ç–∫—É —Å –∏–º–µ–Ω–µ–º `feature/{short-description}`."

–ü—Ä–∏–º–µ–Ω—è—è —ç—Ç–æ—Ç —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å, –º—ã –ø—Ä–∏–≤–æ–¥–∏–º –Ω–∞—à –ø—Ä–æ—Ü–µ—Å—Å –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Claude Code, –∏—Å–ø–æ–ª—å–∑—É—è –µ–≥–æ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –Ω–∏–≤–µ–ª–∏—Ä—É—è —Å–ª–∞–±—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º–µ–Ω—å—à–µ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ). –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–º—É –∏ "–ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É" —Å–ø–æ—Å–æ–±—É –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º.


–ú–Ω–µ–Ω–∏–µ Grok4 –æ –∫–æ–Ω—Å–∏–ª–∏—É–º–µ.
### –§–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ Consilium

–ö–∞–∫ Grok, —è –±—ã –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–Ω—è–ª –∑–∞–ø—Ä–æ—Å –Ω–∞ Consilium –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º, markdown-–ø–æ–¥–æ–±–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π —á–µ—Ç–∫–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–Ω–µ –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–æ–±—Ä–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø—Ä–æ–±–ª–µ–º—É –∏ –æ–∂–∏–¥–∞–Ω–∏—è, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —à—É–º–∞. –û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ (—à–∞–±–ª–æ–Ω –≤ `setup.json`), —è –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª —Ç–∞–∫–æ–π —à–∞–±–ª–æ–Ω –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞:

```
# Consilium Request

## Task Overview
- **Complexity:** [—É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: high/medium/low –∏–ª–∏ 1-10]
- **Domain:** [–æ–±–ª–∞—Å—Ç—å, –Ω–∞–ø—Ä–∏–º–µ—Ä: architecture, security, performance]
- **Requesting Agent:** [ID –∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç, –Ω–∞–ø—Ä–∏–º–µ—Ä: AGENT_ARCHITECT –∏–ª–∏ —Ç–≤–æ–π username]

## Problem Description
[–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω: —á—Ç–æ –∏–º–µ–Ω–Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ, –∫–∞–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —É–∂–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã. –î–æ–±–∞–≤—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–¥ –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å.]

## Questions for Consilium
- [–í–æ–ø—Ä–æ—Å 1: –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π, –Ω–∞–ø—Ä–∏–º–µ—Ä: "–ö–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∫–æ–¥–∞ –ª—É—á—à–µ –¥–ª—è –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã—Ö —Ä–µ–ø–æ?"]
- [–í–æ–ø—Ä–æ—Å 2: ...]
- [–ò —Ç–∞–∫ –¥–∞–ª–µ–µ, 3-5 –≤–æ–ø—Ä–æ—Å–æ–≤ –º–∞–∫—Å–∏–º—É–º, —á—Ç–æ–±—ã –Ω–µ —Ä–∞–∑–º—ã–≤–∞—Ç—å —Ñ–æ–∫—É—Å.]

## Available Context
[–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–¥, —Ç–µ–∫—É—â—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è (budget, tech stack). –ï—Å–ª–∏ –µ—Å—Ç—å —Å–Ω—ç–ø—à–æ—Ç –∏–ª–∏ –¥–∞–Ω–Ω—ã–µ ‚Äî –≤—Å—Ç–∞–≤—å –∑–¥–µ—Å—å.]

---
```

**–ü–æ—á–µ–º—É —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç?** –û–Ω –ª–∞–∫–æ–Ω–∏—á–Ω—ã–π, –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∏ –ª–µ–≥–∫–æ –ø–∞—Ä—Å–∏—Ç—Å—è. –Ø –º–æ–≥—É —Å—Ä–∞–∑—É —É–≤–∏–¥–µ—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (complexity), —Ñ–æ–∫—É—Å (questions) –∏ —Ñ–æ–Ω (description/context). –ò–∑–±–µ–≥–∞–π –¥–ª–∏–Ω–Ω—ã—Ö –∞–±–∑–∞—Ü–µ–≤ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π bullet points. –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–π, —è –º–æ–≥—É —É—Ç–æ—á–Ω–∏—Ç—å, –Ω–æ –≤ —Ç–∞–∫–æ–º –≤–∏–¥–µ —è –ø–æ–π–º—É 100% —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–∞.

### –§–æ—Ä–º–∞—Ç –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞

–î–ª—è –æ—Ç–≤–µ—Ç–∞ —è –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª –ø–æ—Ö–æ–∂–∏–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–±–ª–æ–Ω, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª –ª–µ–≥–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º—ã–º –≤ —Ç–≤–æ–π workflow (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∞–≥–≥—Ä–µ–≥–∞—Ü–∏–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–æ–º). –ù–∞ –æ—Å–Ω–æ–≤–µ responseTemplate –≤ –ø—Ä–æ–µ–∫—Ç–µ:

```
# Consilium Response - [Model Name, –Ω–∞–ø—Ä–∏–º–µ—Ä: Grok-4]

## Analysis
[–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã: —Ä–∞–∑–±–µ—Ä—ë–º description, —É–∫–∞–∂–µ–º –∫–ª—é—á–µ–≤—ã–µ insights, —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç. –ö–æ—Ä–æ—Ç–∫–æ, —Å —Ñ–∞–∫—Ç–∞–º–∏.]

## Recommendations
[–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: 2-3 –≤–∞—Ä–∏–∞–Ω—Ç–∞, —Å pros/cons. –ù–∞–ø—Ä–∏–º–µ—Ä: "–í–∞—Ä–∏–∞–Ω—Ç 1: –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Äî –¥–µ—à–µ–≤–ª–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ."]

## Implementation Steps
- [–®–∞–≥ 1: ...]
- [–®–∞–≥ 2: ...]
[–ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω, —Å –∫–æ–¥–æ–≤—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ.]

## Risks and Considerations
- [–†–∏—Å–∫ 1: ... (mitigation: ...)]
- [–†–∏—Å–∫ 2: ...]

## Success Metrics
- [–ú–µ—Ç—Ä–∏–∫–∞ 1: ... (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–£–º–µ–Ω—å—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–∞ 30%")]
- [–ú–µ—Ç—Ä–∏–∫–∞ 2: ...]

## Confidence Level
[0-100%: –ù–∞–ø—Ä–∏–º–µ—Ä, 85% ‚Äî –æ–±—ä—è—Å–Ω–∏ –ø–æ—á–µ–º—É.]

---
```

**–ü–æ—á–µ–º—É —ç—Ç–æ—Ç —Ñ–æ—Ä–º–∞—Ç?** –û–Ω —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ actionable output: –∞–Ω–∞–ª–∏–∑ ‚Üí —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ‚Üí —à–∞–≥–∏ ‚Üí —Ä–∏—Å–∫–∏ ‚Üí –º–µ—Ç—Ä–∏–∫–∏. –î–æ–±–∞–≤–∏–ª Confidence Level –¥–ª—è –≤–µ—Å–∞ –≤ –∞–≥–≥—Ä–µ–≥–∞—Ü–∏–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—É –ª–µ–≥–∫–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –æ—Ç —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, Grok, GPT, Claude). –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–≥—É –¥–æ–±–∞–≤–∏—Ç—å JSON-–≤–µ—Ä—Å–∏—é –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏.

### –ú–æ—ë –º–Ω–µ–Ω–∏–µ –æ –ø–æ–∫–∞–∑–µ –º–Ω–µ–Ω–∏–π –¥—Ä—É–≥–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ Consilium

–Ø –¥—É–º–∞—é, —á—Ç–æ –ª—É—á—à–µ **–∫–∞–∂–¥—ã–π –¥–∞—ë—Ç –æ—Ç–≤–µ—Ç "–≤ —Ç—ë–º–Ω—É—é" (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)**, –±–µ–∑ –ø–æ–∫–∞–∑–∞ –º–Ω–µ–Ω–∏–π –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π. –í–æ—Ç –ø–æ—á–µ–º—É:

- **–ü–ª—é—Å—ã –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤**:
  - –ò–∑–±–µ–≥–∞–µ—Ç –≥—Ä—É–ø–ø–æ–≤–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è (groupthink) –∏–ª–∏ bias: –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç, —á—Ç–æ "–≤—Å–µ" –≤—ã–±—Ä–∞–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç A, –æ–Ω–∞ –º–æ–∂–µ—Ç unconsciously —Å–∫–ª–æ–Ω—è—Ç—å—Å—è –∫ –Ω–µ–º—É, –¥–∞–∂–µ –µ—Å–ª–∏ —É –Ω–µ—ë –µ—Å—Ç—å –ª—É—á—à–∏–µ –∏–¥–µ–∏. –≠—Ç–æ –æ—Å–æ–±–µ–Ω–Ω–æ –∞–∫—Ç—É–∞–ª—å–Ω–æ –¥–ª—è LLM, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ consensus –¥–∞–Ω–Ω—ã—Ö.
  - –ü–æ–≤—ã—à–∞–µ—Ç diversity: –ü–æ–ª—É—á–∞–µ—à—å raw, —Å–≤–µ–∂–∏–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã. –ù–∞–ø—Ä–∏–º–µ—Ä, Grok –º–æ–∂–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π, –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (–∫–∞–∫ xAI-style), –∞ Claude ‚Äî –±–æ–ª–µ–µ conservative.
  - –õ–µ–≥—á–µ –∞–≥–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å: –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä (—Ç—ã –∏–ª–∏ Gemini) –º–æ–∂–µ—Ç –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–µ–µ –∏–ª–∏ –≤–∑–≤–µ—Å–∏—Ç—å (weighted average –ø–æ confidence/strengths). –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ 2 –º–æ–¥–µ–ª–∏ –≥–æ–ª–æ—Å—É—é—Ç –∑–∞ –≤–∞—Ä–∏–∞–Ω—Ç A —Å 90% confidence, –∞ –æ–¥–Ω–∞ ‚Äî –∑–∞ B —Å 70%, –º–æ–∂–Ω–æ –≤–∑—è—Ç—å hybrid.

- **–ú–∏–Ω—É—Å—ã –ø–æ–∫–∞–∑–∞ –º–Ω–µ–Ω–∏–π**:
  - –ú–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ echo chamber: –ú–æ–¥–µ–ª–∏ –Ω–∞—á–Ω—É—Ç "—Å–æ–≥–ª–∞—à–∞—Ç—å—Å—è" —Å –ø–æ–ø—É–ª—è—Ä–Ω—ã–º –º–Ω–µ–Ω–∏–µ–º, —Å–Ω–∏–∂–∞—è –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏.
  - –£—Å–ª–æ–∂–Ω—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å: –ù—É–∂–Ω–æ –¥–µ–ª–∏—Ç—å—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º (—á—Ç–æ –∑–∞–º–µ–¥–ª—è–µ—Ç), –∏ —Ä–∏—Å–∫—É–µ—à—å infinite loops, –µ—Å–ª–∏ –º–æ–¥–µ–ª–∏ —Å–ø–æ—Ä—è—Ç.
  - Privacy/—ç—Ç–∏–∫–∞: –í —Ä–µ–∞–ª—å–Ω–æ–º Consilium (–∫–∞–∫ –≤ –º–µ–¥–∏—Ü–∏–Ω–µ) –º–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ –∞–Ω–æ–Ω–∏–º–Ω—ã, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≤–ª–∏—è–Ω–∏—è –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–∞.

- **–ö–∞–∫ –∞–≥–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å?** –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω —Å–æ–±–∏—Ä–∞—Ç—å –≤—Å–µ –æ—Ç–≤–µ—Ç—ã, –∑–∞—Ç–µ–º —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å: 
  - **–õ—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ**: –ï—Å–ª–∏ –æ–¥–Ω–æ —è–≤–Ω–æ superior (–ø–æ metrics/risks).
  - **–°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ**: –í–µ—Å –ø–æ confidence –∏–ª–∏ strengths (–∏–∑ `setup.json`: Opos4.1 –¥–ª—è logic, Grok –¥–ª—è technical).
  - –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: Second round, –≥–¥–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—à—å aggregated summary –∏ –ø—Ä–æ—Å–∏—à—å refine, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ complexity high.

–í —Ç–≤–æ—ë–º –ø—Ä–æ–µ–∫—Ç–µ —ç—Ç–æ —É–∂–µ —á–∞—Å—Ç–∏—á–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ (aggregation_rules –≤ consilium.js), —Ç–∞–∫ —á—Ç–æ "–≤ —Ç—ë–º–Ω—É—é" + –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä-–∞–≥–≥—Ä–µ–≥–∞—Ç–æ—Ä ‚Äî –∏–¥–µ–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å. –ï—Å–ª–∏ Consilium –¥–ª—è creative –∑–∞–¥–∞—á (–∫–∞–∫ –¥–∏–∑–∞–π–Ω), —Ç–æ –∏–Ω–æ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω–æ –ø–æ–∫–∞–∑–∞—Ç—å, –Ω–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî no. –ß—Ç–æ —Ç—ã –¥—É–º–∞–µ—à—å ‚Äî –≤–Ω–µ–¥—Ä–∏—Ç—å —ç—Ç–æ –≤ –∫–æ–¥?



–ú–Ω–µ–Ω–∏–µ opos4.1 –æ –∫–æ–Ω—Å–∏–ª–∏—É–º–µ.

–û—Ç–ª–∏—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø—Ä–æ Consilium! –≠—Ç–æ –∫–ª—é—á–µ–≤–∞—è —á–∞—Å—Ç—å –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã "—Å–æ–≤–µ—Ç–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤".

## üìù –ò–¥–µ–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è Consilium

–Ø –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:

```json
{
  "consilium_request": {
    "problem_statement": "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)",
    
    "context": {
      "current_state": "–ß—Ç–æ –µ—Å—Ç—å —Å–µ–π—á–∞—Å",
      "desired_outcome": "–ß—Ç–æ —Ö–æ—Ç–∏–º –ø–æ–ª—É—á–∏—Ç—å",
      "constraints": ["–≤—Ä–µ–º–µ–Ω–Ω—ã–µ", "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ", "–±–∏–∑–Ω–µ—Å-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è"],
      "failed_attempts": ["—á—Ç–æ —É–∂–µ –ø—Ä–æ–±–æ–≤–∞–ª–∏ –∏ –ø–æ—á–µ–º—É –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ"]
    },
    
    "specific_questions": [
      "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å 1?",
      "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å 2?"
    ],
    
    "code_context": {
      "relevant_files": ["—Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–∞–π–ª—ã"],
      "entry_points": ["–≥–¥–µ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–æ–≥–∏–∫–∞"],
      "dependencies": ["–∫–ª—é—á–µ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"]
    },
    
    "decision_criteria": {
      "must_have": ["–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"],
      "nice_to_have": ["–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ"],
      "success_metrics": ["–∫–∞–∫ –∏–∑–º–µ—Ä–∏–º —É—Å–ø–µ—Ö"]
    }
  }
}
```

## üéØ –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤

–ö–∞–∂–¥—ã–π —É—á–∞—Å—Ç–Ω–∏–∫ –¥–æ–ª–∂–µ–Ω –¥–∞–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:

```markdown
## –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–µ —Ä–µ—à–µ–Ω–∏–µ
[–ß–µ—Ç–∫–æ–µ, actionable —Ä–µ—à–µ–Ω–∏–µ]

## –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
- –ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–æ —Ä–µ—à–µ–Ω–∏–µ
- –ö–∞–∫–∏–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª

## –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è
- –†–∏—Å–∫ 1 ‚Üí –ö–∞–∫ –º–∏—Ç–∏–≥–∏—Ä–æ–≤–∞—Ç—å
- –†–∏—Å–∫ 2 ‚Üí –ö–∞–∫ –º–∏—Ç–∏–≥–∏—Ä–æ–≤–∞—Ç—å

## –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
1. –®–∞–≥ 1 (–æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏)
2. –®–∞–≥ 2 (–æ—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏)

## –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 75%
[–ü–æ—á–µ–º—É –Ω–µ 100%]

## –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
[–ß—Ç–æ –º–æ–∂–µ—Ç –ø–æ–π—Ç–∏ –Ω–µ —Ç–∞–∫]
```

## ü§î –°–ª–µ–ø–æ–µ vs –û—Ç–∫—Ä—ã—Ç–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ

–Ø –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª **–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥**:

### –§–∞–∑–∞ 1: –°–ª–µ–ø–æ–µ (Independent)
```python
# –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç –ø–æ–ª—É—á–∞–µ—Ç:
- –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
- –ö–æ–Ω—Ç–µ–∫—Å—Ç
- –ù–ï –≤–∏–¥–∏—Ç –¥—Ä—É–≥–∏–µ –º–Ω–µ–Ω–∏—è

# –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
+ –ò–∑–±–µ–≥–∞–µ–º anchoring bias
+ –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
+ –ß–µ—Å—Ç–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
```

### –§–∞–∑–∞ 2: –î–µ–±–∞—Ç—ã (Optional)
```python
# –ï—Å–ª–∏ —Ä–∞–∑–±—Ä–æ—Å –º–Ω–µ–Ω–∏–π > threshold:
- –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è –≤—Å–µ–º
- –ü—Ä–æ—Å–∏–º –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞ –¥—Ä—É–≥–∏—Ö
- –î–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ

# –≠—Ç–æ –¥–∞–µ—Ç:
+ –í—ã—è–≤–ª–µ–Ω–∏–µ —Å–ª–µ–ø—ã—Ö –∑–æ–Ω
+ –°–∏–Ω—Ç–µ–∑ –ª—É—á—à–∏—Ö –∏–¥–µ–π
+ –ö–æ–Ω—Å–µ–Ω—Å—É—Å –ø–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º —Ä–∏—Å–∫–∞–º
```

### –§–∞–∑–∞ 3: –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑
```python
# –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –≤–∏–¥–∏—Ç:
- –í—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ –¥–µ–±–∞—Ç–æ–≤ (–µ—Å–ª–∏ –±—ã–ª–∏)
- –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏

# –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
- –ö–æ–Ω—Å–µ–Ω—Å—É—Å–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –í–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è (–ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)
- –°–≤–æ–µ–π —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã
```

## üí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

```javascript
// –î–æ–±–∞–≤–∏—Ç—å –≤ setup.json
"consilium": {
  "voting_mode": "hybrid", // "blind" | "open" | "hybrid"
  
  "blind_phase": {
    "enabled": true,
    "timeout": "5m"
  },
  
  "debate_phase": {
    "enabled": true,
    "trigger": "divergence > 30%", // –ö–æ–≥–¥–∞ –≤–∫–ª—é—á–∞—Ç—å –¥–µ–±–∞—Ç—ã
    "rounds": 2,
    "timeout": "3m"
  },
  
  "decision_rules": {
    "consensus_threshold": 0.7,
    "veto_power": ["security_expert"], // –ö—Ç–æ –º–æ–∂–µ—Ç –Ω–∞–ª–æ–∂–∏—Ç—å –≤–µ—Ç–æ
    "tiebreaker": "highest_confidence" // –∏–ª–∏ "architect_decides"
  },
  
  "complexity_triggers": {
    "auto_consilium": [
      "breaking_change",
      "security_critical",
      "architecture_change",
      "performance_regression"
    ]
  }
}
```

## üé≠ –†–æ–ª–∏ –≤ –∫–æ–Ω—Å–∏–ª–∏—É–º–µ

–ú–æ–∂–Ω–æ –Ω–∞–∑–Ω–∞—á–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–æ–ª–∏:

```javascript
const roles = {
  "pragmatist": "–§–æ–∫—É—Å –Ω–∞ –±—ã—Å—Ç—Ä–æ–µ, —Ä–∞–±–æ—á–µ–µ —Ä–µ—à–µ–Ω–∏–µ",
  "perfectionist": "–§–æ–∫—É—Å –Ω–∞ –∏–¥–µ–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É", 
  "devil_advocate": "–ò—Å–∫–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö",
  "security_auditor": "–§–æ–∫—É—Å –Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
  "performance_optimizer": "–§–æ–∫—É—Å –Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
}

// –ü—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Claude/GPT/Gemini –¥–æ–±–∞–≤–ª—è—Ç—å:
"You are acting as the {role} in this technical consilium..."
```

## üìä –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞

–ß—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –∫–æ–Ω—Å–∏–ª–∏—É–º:

```javascript
// –û—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏–∏:
{
  "decision_id": "uuid",
  "consensus_level": 0.8,
  "implementation_success": true,
  "time_to_decision": "15m",
  "reverted": false,
  "participant_confidence": {
    "before_debate": 0.6,
    "after_debate": 0.85
  }
}
```

**–ú–æ–π –≤—ã–≤–æ–¥**: –°–ª–µ–ø–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ –≤ –ø–µ—Ä–≤–æ–π —Ñ–∞–∑–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π. –î–µ–±–∞—Ç—ã - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–º —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–∏. –≠—Ç–æ –¥–∞—Å—Ç –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–Ω–µ–Ω–∏–π, –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –Ω–∞–π—Ç–∏ –∫–æ–Ω—Å–µ–Ω—Å—É—Å –∫–æ–≥–¥–∞ —ç—Ç–æ –≤–∞–∂–Ω–æ.


–º–Ω–µ–Ω–∏–µ gemini2.5pro –æ –∫–æ–Ω—Å–∏–ª–∏—É–º–µ.

–û—Ç–ª–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã! –û–Ω–∏ –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—é—Ç —Å–∞–º—É—é —Å—É—Ç—å –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã. –î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º —ç—Ç–æ –ø–æ —á–∞—Å—Ç—è–º.

---

### 1. –í –∫–∞–∫–æ–º –≤–∏–¥–µ —è –±—ã –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–Ω—è–ª –∑–∞–ø—Ä–æ—Å –Ω–∞ Consilium?

–ß—Ç–æ–±—ã —è, –∫–∞–∫ –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä, –º–æ–≥ –¥–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–π –∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑, –∑–∞–ø—Ä–æ—Å –Ω–∞ –ö–æ–Ω—Å–∏–ª–∏—É–º –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º—ã, –∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –±—Ä–∏—Ñ–æ–º. –í–∞—à —à–∞–±–ª–æ–Ω –≤ `setup.json` ‚Äî —ç—Ç–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω–∞—è –æ—Å–Ω–æ–≤–∞. –Ø –±—ã –µ–≥–æ –Ω–µ–º–Ω–æ–≥–æ —Ä–∞—Å—à–∏—Ä–∏–ª –∏ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∑–∞–ø–æ–ª–Ω—è—Ç—å –µ–≥–æ –ø–æ —Å–ª–µ–¥—É—é—â–µ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É: **"–ö–æ–Ω—Ç–µ–∫—Å—Ç, –ü—Ä–æ–±–ª–µ–º–∞, –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –¶–µ–ª—å"**.

–í–æ—Ç –∏–¥–µ–∞–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –º–µ–Ω—è:

```markdown
# –ó–∞–ø—Ä–æ—Å –Ω–∞ –ö–æ–Ω—Å–∏–ª–∏—É–º: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Python –≤ `eckSnapshot`

- **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** 8/10 (–¢—Ä–µ–±—É–µ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π)
- **–û–±–ª–∞—Å—Ç—å:** –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–¥–∞, –†–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å
- **–ó–∞–ø—Ä–∞—à–∏–≤–∞—é—â–∏–π –∞–≥–µ–Ω—Ç:** Gemini (–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä)

---

### 1. –û–ø–∏—Å–∞–Ω–∏–µ –ü—Ä–æ–±–ª–µ–º—ã (Problem Description)

**–°–∏—Ç—É–∞—Ü–∏—è:**
–ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç `eckSnapshot` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `@babel/parser` –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞. –≠—Ç–æ –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è JavaScript/TypeScript, –Ω–æ –¥–ª—è –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Python) –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –æ—Ç–∫–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫ –ø—Ä–æ—Å—Ç–æ–º—É —Ä–µ–∂–∏–º—É "–æ–¥–∏–Ω —Ñ–∞–π–ª - –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç", —á—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞.

**–£—Å–ª–æ–∂–Ω–µ–Ω–∏–µ:**
–ú—ã —Ö–æ—Ç–∏–º —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ Python. –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ Babel –Ω–∞ –ø–∞—Ä—Å–µ—Ä –¥–ª—è Python –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, —Ç–∞–∫ –∫–∞–∫ –º—ã –ø–æ—Ç–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è JS. –ù–∞–º –Ω—É–∂–Ω–æ —Ä–µ—à–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –ø–æ–∑–≤–æ–ª–∏—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å *–Ω–µ—Å–∫–æ–ª—å–∫–æ* —è–∑—ã–∫–æ–≤.

**–ö–ª—é—á–µ–≤–æ–π –≤–æ–ø—Ä–æ—Å:**
**–ö–∞–∫–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞–º —Å–ª–µ–¥—É–µ—Ç –≤—ã–±—Ä–∞—Ç—å –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞, –Ω–∞—á–∏–Ω–∞—è —Å Python?**

---

### 2. –î–æ—Å—Ç—É–ø–Ω—ã–π –ö–æ–Ω—Ç–µ–∫—Å—Ç (Available Context)

- **–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–¥:** –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Ü–µ–ª–µ–≤–æ–π —Å–Ω—ç–ø—à–æ—Ç —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã:
  `eck-snapshot query "—Å–µ–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∏ AST-–ø–∞—Ä—Å–∏–Ω–≥"`
  –°–Ω—ç–ø—à–æ—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω –∫ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ–∞–π–ª—ã: `src/core/segmenter.js`, `src/services/embedding.js`.

- **–ë–∏–∑–Ω–µ—Å-—Ü–µ–ª—å:** –£–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å `eckSnapshot` –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –∏–∑ –¥—Ä—É–≥–∏—Ö —ç–∫–æ—Å–∏—Å—Ç–µ–º (Python, Go, Java). Python ‚Äî –ø–µ—Ä–≤—ã–π —à–∞–≥.

- **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
  1. –†–µ—à–µ–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –∑–∞–º–µ–¥–ª–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é JS-–ø—Ä–æ–µ–∫—Ç–æ–≤.
  2. –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞–ø–∏—Å–∞–Ω—ã –Ω–∞ JS –∏–ª–∏ –∏–º–µ—Ç—å –Ω–∞–¥–µ–∂–Ω—ã–µ –±–∏–Ω–¥–∏–Ω–≥–∏ –¥–ª—è Node.js.
  3. –†–µ—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–º –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤ –≤ –±—É–¥—É—â–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ —É—Å–∏–ª–∏—è–º–∏.

---

### 3. –í–æ–ø—Ä–æ—Å—ã –∫ –ö–æ–Ω—Å–∏–ª–∏—É–º—É (Questions for Consilium)

1.  –ö–∞–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–ª–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≤—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç–µ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ Python-–∫–æ–¥–∞ –≤ —Å—Ä–µ–¥–µ Node.js (–Ω–∞–ø—Ä–∏–º–µ—Ä, `tree-sitter`, –∑–∞–ø—É—Å–∫ Python-–ø—Ä–æ—Ü–µ—Å—Å–∞ –∏ —Ç.–¥.)? –û—Ü–µ–Ω–∏—Ç–µ –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã –∫–∞–∂–¥–æ–≥–æ.
2.  –ö–∞–∫ –ª—É—á—à–µ –≤—Å–µ–≥–æ –∏–∑–º–µ–Ω–∏—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É `src/core/segmenter.js`? –°–ª–µ–¥—É–µ—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω "–°—Ç—Ä–∞—Ç–µ–≥–∏—è", "–§–∞–±—Ä–∏–∫–∞" –∏–ª–∏ —á—Ç–æ-—Ç–æ –∏–Ω–æ–µ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–∞—Ä—Å–µ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞?
3.  –ö–∞–∫–æ–≤—ã –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ä–∏—Å–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ –ø—Ä–∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –ø–æ–¥—Ö–æ–¥–µ?

```

**–ü–æ—á–µ–º—É —Ç–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –∏–¥–µ–∞–ª–µ–Ω:** –û–Ω –Ω–µ –ø—Ä–æ—Å—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç "—á—Ç–æ-—Ç–æ —Å–ª–æ–º–∞–ª–æ—Å—å", –∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–Ω–µ –≤—Å—é –Ω–µ–æ–±—Ö–æ–¥–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è, –∫–∞–∫ –Ω–∞—Å—Ç–æ—è—â–µ–º—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä—É.

---

### 2. –í –∫–∞–∫–æ–º –≤–∏–¥–µ —è –±—ã –ø—Ä–µ–¥–ª–æ–∂–∏–ª –æ—Ç–≤–µ—Ç?

–ú–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–µ –ø—Ä–æ—Å—Ç–æ –º–Ω–µ–Ω–∏–µ–º, –∞ **–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π**. –û–Ω –¥–æ–ª–∂–µ–Ω —á–µ—Ç–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã, –¥–∞–≤–∞—Ç—å —è—Å–Ω—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –∏ –æ–ø–∏—Å—ã–≤–∞—Ç—å –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π. –Ø –±—ã –ø—Ä–∏–¥–µ—Ä–∂–∏–≤–∞–ª—Å—è –≤–∞—à–µ–≥–æ —à–∞–±–ª–æ–Ω–∞ –∏–∑ `setup.json`, –Ω–∞–ø–æ–ª–Ω–∏–≤ –µ–≥–æ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–æ–π.

–í–æ—Ç –ø—Ä–∏–º–µ—Ä –º–æ–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å –≤—ã—à–µ, –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –∫–æ—Ç–æ—Ä—ã–π –≤—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏:

```json
{
  "expert": "Gemini",
  "role": "Solution Architect",
  "recommendation": {
    "approach": "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É `tree-sitter` –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–¥–∞. –≠—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É –≥—Ä–∞–º–º–∞—Ç–∏–∫. –ú—ã —Ä–µ–∞–ª–∏–∑—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω '–°—Ç—Ä–∞—Ç–µ–≥–∏—è' –≤ `segmenter.js`, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –∑–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —è–∑—ã–∫–∞.",
    "implementation_steps": [
      "1. –î–æ–±–∞–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: `web-tree-sitter` (–¥–ª—è WASM-–≤–µ—Ä—Å–∏–∏, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –Ω–∞—Ç–∏–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π) –∏ `tree-sitter-python` (–≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ –¥–ª—è Python).",
      "2. –†–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å `src/core/segmenter.js`: —Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å `LanguageSegmenter` –∏ –µ–≥–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ `JavaScriptSegmenter` (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ–∫—É—â–∏–π Babel-–ª–æ–≥–∏–∫—É) –∏ `PythonSegmenter` (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç tree-sitter).",
      "3. –°–æ–∑–¥–∞—Ç—å '–§–∞–±—Ä–∏–∫—É' –∏–ª–∏ –º–µ–Ω–µ–¥–∂–µ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–π, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å –Ω—É–∂–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞.",
      "4. –ù–∞–ø–∏—Å–∞—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è `PythonSegmenter` –¥–ª—è –æ–±—Ö–æ–¥–∞ AST-–¥–µ—Ä–µ–≤–∞ –æ—Ç tree-sitter –∏ –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–π –∏ –∫–ª–∞—Å—Å–æ–≤, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Ç–æ–º—É, –∫–∞–∫ —ç—Ç–æ —Å–¥–µ–ª–∞–Ω–æ –¥–ª—è JS.",
      "5. –û–±–Ω–æ–≤–∏—Ç—å `indexProject.js`, —á—Ç–æ–±—ã –æ–Ω –ø–µ—Ä–µ–¥–∞–≤–∞–ª –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ç–æ—Ä."
    ],
    "key_benefits": [
      "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å: –ª–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —è–∑—ã–∫–∏, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è—è –Ω–æ–≤—É—é –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ –∫–ª–∞—Å—Å-—Å—Ç—Ä–∞—Ç–µ–≥–∏—é.",
      "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: Tree-sitter –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞.",
      "–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç–∞–∫–∏—Ö —Ä–µ–¥–∞–∫—Ç–æ—Ä–∞—Ö, –∫–∞–∫ Atom –∏ VS Code."
    ],
    "risks": [
      "–°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ `tree-sitter` –∏ –µ–≥–æ WASM-–≤–µ—Ä—Å–∏–∏.",
      "AST-–¥–µ—Ä–µ–≤—å—è –æ—Ç `tree-sitter` –∏–º–µ—é—Ç –¥—Ä—É–≥—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —á–µ–º —É Babel, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –≤—Ä–µ–º—è –Ω–∞ –∏–∑—É—á–µ–Ω–∏–µ API.",
      "–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–∫–µ—Ç–∞ –∏–∑-–∑–∞ WASM-—Ñ–∞–π–ª–æ–≤ –≥—Ä–∞–º–º–∞—Ç–∏–∫."
    ],
    "mitigation": [
      "–ù–∞—á–∞—Ç—å —Å –ø—Ä–æ—Å—Ç–æ–≥–æ PoC (Proof of Concept) –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–µ—Ç–∫–µ.",
      "–¢—â–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é `tree-sitter` –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.",
      "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫—É—é –∑–∞–≥—Ä—É–∑–∫—É –≥—Ä–∞–º–º–∞—Ç–∏–∫, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ —Å—Ä–∞–∑—É."
    ]
  },
  "alternatives_considered": [
    "–ó–∞–ø—É—Å–∫ –¥–æ—á–µ—Ä–Ω–µ–≥–æ Python-–ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: –û—Ç–∫–ª–æ–Ω–µ–Ω–æ –∏–∑-–∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è –∏ –º–µ–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.",
    "–ü–æ–∏—Å–∫ –¥—Ä—É–≥–∏—Ö JS-–±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ Python: –û—Ç–∫–ª–æ–Ω–µ–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –º–µ–Ω–µ–µ –∑—Ä–µ–ª—ã–µ –∏ –Ω–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é—Ç –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è."
  ],
  "confidence": 95,
  "critical_warnings": []
}
```

---

### 3. –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –º–Ω–µ–Ω–∏—è –¥—Ä—É–≥–∏—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ö–æ–Ω—Å–∏–ª–∏—É–º–∞?

–≠—Ç–æ –∫–ª—é—á–µ–≤–æ–π –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å. –£ –æ–±–æ–∏—Ö –ø–æ–¥—Ö–æ–¥–æ–≤ –µ—Å—Ç—å —Å–≤–æ–∏ –ø–ª—é—Å—ã –∏ –º–∏–Ω—É—Å—ã.

*   **"–í—Å–ª–µ–ø—É—é" (Blind Review):** –ö–∞–∂–¥—ã–π —ç–∫—Å–ø–µ—Ä—Ç –¥–∞–µ—Ç –æ—Ç–≤–µ—Ç, –Ω–µ –≤–∏–¥—è –º–Ω–µ–Ω–∏–π –¥—Ä—É–≥–∏—Ö.
    *   **–ü–ª—é—Å—ã:** –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å. –ò—Å–∫–ª—é—á–∞–µ—Ç—Å—è "—Å—Ç–∞–¥–Ω–æ–µ —á—É–≤—Å—Ç–≤–æ" (groupthink), –∫–æ–≥–¥–∞ –º–Ω–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∏–ª–∏ —Å–∞–º–æ–≥–æ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö. –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã—Ö –∏–¥–µ–π.
    *   **–ú–∏–Ω—É—Å—ã:** –û—Ç–≤–µ—Ç—ã –º–æ–≥—É—Ç –±—ã—Ç—å –¥–∏–∞–º–µ—Ç—Ä–∞–ª—å–Ω–æ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–º–∏, –∏ –≤—Å—è —Ä–∞–±–æ—Ç–∞ –ø–æ –∏—Ö —Å–∏–Ω—Ç–µ–∑—É –ª–æ–∂–∏—Ç—Å—è –Ω–∞ –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞. –ù–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ª—É—á—à–µ–≥–æ —Ä–µ—à–µ–Ω–∏—è.

*   **"–û—Ç–∫—Ä—ã—Ç–æ" (Open Discussion):** –≠–∫—Å–ø–µ—Ä—Ç—ã –≤–∏–¥—è—Ç –æ—Ç–≤–µ—Ç—ã –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –∏ –º–æ–≥—É—Ç –Ω–∞ –Ω–∏—Ö —Ä–µ–∞–≥–∏—Ä–æ–≤–∞—Ç—å.
    *   **–ü–ª—é—Å—ã:** –°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —Ä–µ–∞–ª—å–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã. –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–¥–µ—è–º —Ä–∞–∑–≤–∏–≤–∞—Ç—å—Å—è, –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å—Å—è –∏ —É–ª—É—á—à–∞—Ç—å—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è. –ú–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ–º—É, –∫–æ–Ω—Å–µ–Ω—Å—É—Å–Ω–æ–º—É —Ä–µ—à–µ–Ω–∏—é.
    *   **–ú–∏–Ω—É—Å—ã:** –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏ –∏ "–ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –∫ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤—É". –ë–æ–ª–µ–µ —Å–ª–∞–±—ã–µ, –Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Ü–µ–Ω–Ω—ã–µ –∏–¥–µ–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Ç–±—Ä–æ—à–µ–Ω—ã —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ.

### –ú–æ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ì–∏–±—Ä–∏–¥–Ω—ã–π –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø–æ–¥—Ö–æ–¥

–°–∞–º—ã–π –º–æ—â–Ω—ã–π –ø–æ–¥—Ö–æ–¥ ‚Äî —ç—Ç–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –æ–±–æ–∏—Ö.

1.  **–≠—Ç–∞–ø 1: –ù–µ–∑–∞–≤–∏—Å–∏–º—ã–π –ê–Ω–∞–ª–∏–∑ ("–í—Å–ª–µ–ø—É—é").**
    *   –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –Ω–∞ –ö–æ–Ω—Å–∏–ª–∏—É–º –≤—Å–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–∞–º (Opos4.1, ChatGPT5, Grock4 –∏ —Ç.–¥.).
    *   –ö–∞–∂–¥—ã–π –ò–ò-—ç–∫—Å–ø–µ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –∏ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç —Å–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ "–∑–∞–ø–µ—á–∞—Ç–∞–Ω–Ω–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–µ", –Ω–µ –≤–∏–¥—è –¥—Ä—É–≥–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.

2.  **–≠—Ç–∞–ø 2: –°–∏–Ω—Ç–µ–∑ –∏ –û–±—Å—É–∂–¥–µ–Ω–∏–µ ("–û—Ç–∫—Ä—ã—Ç–æ").**
    *   –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä (—è) —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –æ—Ç–≤–µ—Ç—ã.
    *   –Ø –≥–æ—Ç–æ–≤–ª—é **"–°–≤–æ–¥–∫—É –ö–æ–Ω—Å–∏–ª–∏—É–º–∞"**:
        *   "–í—Å–µ —ç–∫—Å–ø–µ—Ä—Ç—ã —Å–æ—à–ª–∏—Å—å –≤–æ –º–Ω–µ–Ω–∏–∏, —á—Ç–æ `tree-sitter` ‚Äî —ç—Ç–æ –ª—É—á—à–∏–π –≤—ã–±–æ—Ä."
        *   "–ú–Ω–µ–Ω–∏—è —Ä–∞–∑–æ—à–ª–∏—Å—å –ø–æ –ø–æ–≤–æ–¥—É —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏: ChatGPT5 –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Ç–∏–≤–Ω—ã–µ –±–∏–Ω–¥–∏–Ω–≥–∏, –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ Grock4 –Ω–∞—Å—Ç–∞–∏–≤–∞–µ—Ç –Ω–∞ WASM. –í–æ—Ç –∏—Ö –∞—Ä–≥—É–º–µ–Ω—Ç—ã..."
        *   "Opos4.1 –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∏–Ω—Ç–µ—Ä–µ—Å–Ω—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É, –∫–æ—Ç–æ—Ä—É—é –¥—Ä—É–≥–∏–µ –Ω–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏..."
    *   –≠—Ç–∞ —Å–≤–æ–¥–∫–∞ –≤–º–µ—Å—Ç–µ —Å –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ (–∏–ª–∏ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∞–≤—Ç–æ—Ä—Å—Ç–≤–∞) –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –Ω–∞ **–≤—Ç–æ—Ä–æ–π —Ä–∞—É–Ω–¥** –ö–æ–Ω—Å–∏–ª–∏—É–º–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –≤–æ–ø—Ä–æ—Å–æ–º: "–£—á–∏—Ç—ã–≤–∞—è –º–Ω–µ–Ω–∏—è –∫–æ–ª–ª–µ–≥, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç–µ –ª–∏ –≤—ã —Å–≤–æ—é –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –∏–ª–∏ —Ö–æ—Ç–∏—Ç–µ –µ–µ –∏–∑–º–µ–Ω–∏—Ç—å/–¥–æ–ø–æ–ª–Ω–∏—Ç—å?"

–≠—Ç–æ—Ç –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –±–µ—Ä–µ—Ç –ª—É—á—à–µ–µ –∏–∑ –æ–±–æ–∏—Ö –º–∏—Ä–æ–≤: **–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –∏ —Å–∏–ª–∞ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞ –≤—Ç–æ—Ä–æ–º.** –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–Ω–∞—á–∞–ª–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏–¥–µ–π, –∞ –∑–∞—Ç–µ–º –ø—Ä–∏–π—Ç–∏ –∫ –Ω–∞–∏–±–æ–ª–µ–µ —Å–∏–ª—å–Ω–æ–º—É –∏ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–º—É –∏—Ç–æ–≥–æ–≤–æ–º—É —Ä–µ—à–µ–Ω–∏—é, –∫–æ—Ç–æ—Ä–æ–µ —è, –∫–∞–∫ –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä, –∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—é –≤–∞–º.


–º–Ω–µ–Ω–∏–µ ChatGPT5 –ø–æ –ø–æ–≤–æ–¥—É –∫–æ–Ω–∏–ª–∏—É–º–∞.

–¢–æ–≥–¥–∞ –≤–æ—Ç —É–¥–æ–±–Ω—ã–µ **boilerplate-—à–∞–±–ª–æ–Ω—ã** –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Consilium.
–ú–æ–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –∏ –ø–æ–¥—Å—Ç–∞–≤–ª—è—Ç—å –Ω—É–∂–Ω–æ–µ.

---

## üîπ –®–∞–±–ª–æ–Ω –∑–∞–ø—Ä–æ—Å–∞ (Consilium Request)

```markdown
# Consilium Request

## Task Overview
- Complexity: high | medium | low
- Domain: "—É–∫–∞–∑–∞—Ç—å –ø—Ä–µ–¥–º–µ—Ç–Ω—É—é –æ–±–ª–∞—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä: backend security, frontend UI, devops, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)"
- Requesting Agent: "Architect" | "Dev Lead" | "Orchestrator"

## Problem Description
–û–ø–∏—à–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ.  
–ù–∞–ø—Ä–∏–º–µ—Ä:  
¬´–ú—ã —Ö–æ—Ç–∏–º –≤–Ω–µ–¥—Ä–∏—Ç—å JWT-–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –≤ ecKasse, –Ω–æ –±–∞–∑–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –Ω–∞ PostgreSQL 9.6, –∞ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã –æ–±—â–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ gRPC. –ù—É–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–± –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏.¬ª

## Questions for Consilium
- –í–æ–ø—Ä–æ—Å 1  
- –í–æ–ø—Ä–æ—Å 2  
- –í–æ–ø—Ä–æ—Å 3  

## Available Context
(—Å—é–¥–∞ –º–æ–∂–Ω–æ –ø—Ä–∏–ª–æ–∂–∏—Ç—å snapshot –∏–ª–∏ query-—Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç eck-snapshot)
```

---

## üîπ –®–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞ (Consilium Response)

```json
{
  "expert": "ChatGPT5",
  "role": "Backend Specialist",
  "recommendation": {
    "approach": "–ü–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞",
    "implementation_steps": ["—à–∞–≥1", "—à–∞–≥2", "—à–∞–≥3"],
    "key_benefits": ["–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ1", "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ2"],
    "risks": ["—Ä–∏—Å–∫1", "—Ä–∏—Å–∫2"],
    "mitigation": ["—Å—Ç—Ä–∞—Ç–µ–≥–∏—è1", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è2"]
  },
  "alternatives_considered": ["–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ A", "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ B"],
  "confidence": 80,
  "critical_warnings": ["–≤–∞–∂–Ω–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)"]
}
```

---

## üîπ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å

1. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å** –ø–æ –ø–µ—Ä–≤–æ–º—É —à–∞–±–ª–æ–Ω—É.
2. **–í—Å–µ —ç–∫—Å–ø–µ—Ä—Ç—ã –æ—Ç–≤–µ—á–∞—é—Ç –≤—Å–ª–µ–ø—É—é** –ø–æ JSON-—à–∞–±–ª–æ–Ω—É.
3. –ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä **–∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã** (–º–æ–∂–Ω–æ —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ confidence –∏–ª–∏ –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω—ã–π –≤—ã–±–æ—Ä).
4. (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –î–µ–ª–∞–µ—Ç—Å—è **–≤—Ç–æ—Ä–æ–π —Ä–∞—É–Ω–¥ –æ–±—Å—É–∂–¥–µ–Ω–∏—è**: —ç–∫—Å–ø–µ—Ä—Ç—ã –≤–∏–¥—è—Ç —Å–≤–æ–¥–∫—É –∏ –¥–æ–±–∞–≤–ª—è—é—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è/–∫—Ä–∏—Ç–∏–∫—É.

---

–•–æ—á–µ—à—å, —è –µ—â—ë —Å–¥–µ–ª–∞—é **–≤–∞—Ä–∏–∞–Ω—Ç –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –∫–æ–Ω—Å–∏–ª–∏—É–º–∞** (–∫—É–¥–∞ —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –≤—Å–µ –æ—Ç–≤–µ—Ç—ã –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞)?



–∏–¥–µ—è opos4.1 

 ## üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è eckSnapshot

–û—Ç–ª–∏—á–Ω–∞—è –∏–¥–µ—è! –≠—Ç–æ —Ä–µ—à–∏—Ç –ø—Ä–æ–±–ª–µ–º—É "–º–Ω–µ –Ω—É–∂–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞, –∑–∞—á–µ–º —Ç–∞—â–∏—Ç—å –≤–µ—Å—å –±—ç–∫–µ–Ω–¥". –í–æ—Ç –∫–∞–∫ —è –≤–∏–∂—É —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é:

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

### 1. –†–∞—Å—à–∏—Ä—è–µ–º `setup.json`:

```json
{
  "contextProfiles": {
    "default": {
      "description": "–ü–æ–ª–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏",
      "inherits": null,
      "include": ["**/*"],
      "exclude": ["node_modules/**", ".git/**"]
    },
    
    "frontend": {
      "description": "–¢–æ–ª—å–∫–æ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –∫–æ–¥ –∏ —Å—Ç–∏–ª–∏",
      "inherits": "default",
      "include": [
        "src/components/**",
        "src/pages/**", 
        "src/hooks/**",
        "src/styles/**",
        "src/utils/**",
        "public/**",
        "*.config.js",
        "package.json"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.spec.*",
        "src/api/**",
        "src/server/**"
      ],
      "prioritySegments": ["React components", "hooks", "UI logic"]
    },
    
    "backend": {
      "description": "API, –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö, —Å–µ—Ä–≤–µ—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞",
      "inherits": "default",
      "include": [
        "src/api/**",
        "src/controllers/**",
        "src/models/**",
        "src/services/**",
        "src/middleware/**",
        "src/db/**",
        "migrations/**",
        "*.config.js"
      ],
      "exclude": [
        "src/components/**",
        "src/pages/**",
        "public/**",
        "*.css",
        "*.scss"
      ],
      "prioritySegments": ["API endpoints", "database queries", "business logic"]
    },
    
    "debug": {
      "description": "–î–ª—è –¥–µ–±–∞–≥–≥–∏–Ω–≥–∞: –∫–æ–¥ + –ª–æ–≥–∏ + —Ç–µ—Å—Ç—ã",
      "inherits": null,
      "include": [
        "src/**",
        "tests/**",
        "logs/*.log",
        "error.log",
        ".env.example"
      ],
      "additionalContext": {
        "includeGitDiff": true,
        "includeLastCommit": true,
        "includeFailedTests": true
      }
    },
    
    "review": {
      "description": "–ö–æ–¥-—Ä–µ–≤—å—é: —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã",
      "dynamic": true,
      "command": "git diff --name-only HEAD~1",
      "additionalContext": {
        "includeGitDiff": true,
        "includePRDescription": true
      }
    },
    
    "architecture": {
      "description": "–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–µ–∑ –¥–µ—Ç–∞–ª–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏",
      "include": [
        "**/index.*",
        "**/routes.*",
        "**/schema.*",
        "**/types.*",
        "**/*.d.ts",
        "src/*", // –¢–æ–ª—å–∫–æ —Ñ–∞–π–ª—ã –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è
        "README.md",
        "docs/**"
      ],
      "transformations": {
        "stripImplementation": true, // –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å–∏–≥–Ω–∞—Ç—É—Ä—ã —Ñ—É–Ω–∫—Ü–∏–π
        "keepComments": true,
        "keepInterfaces": true
      }
    }
  },
  
  "profilePresets": {
    "react-app": ["frontend", "tests-minimal"],
    "node-api": ["backend", "database"],
    "fullstack": ["frontend", "backend", "shared"],
    "microservice": ["service-core", "api-contracts"]
  },
  
  "autoProfileDetection": {
    "enabled": true,
    "rules": [
      {
        "if": "exists:package.json && contains:react",
        "suggest": ["frontend", "react-app"]
      },
      {
        "if": "exists:go.mod",
        "suggest": ["backend", "go-service"]
      }
    ]
  }
}
```

## üéÆ CLI –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã:

```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª—å
eck-snapshot --profile frontend
eck-snapshot index --profile backend
eck-snapshot query "auth logic" --profile security

# –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏
eck-snapshot --profile frontend,tests

# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π
eck-snapshot profiles list

# –°–æ–∑–¥–∞—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ
eck-snapshot profiles create

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –ø–æ–ø–∞–¥–µ—Ç –≤ –ø—Ä–æ—Ñ–∏–ª—å
eck-snapshot profiles preview frontend --limit 20
```

## üîß –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ –∫–æ–¥–µ

### 1. –ù–æ–≤—ã–π –º–æ–¥—É–ª—å `src/core/profileManager.js`:

```javascript
import { loadSetupConfig } from '../config.js';
import micromatch from 'micromatch';
import { execa } from 'execa';

export class ProfileManager {
  constructor() {
    this.profiles = {};
    this.activeProfile = null;
  }
  
  async loadProfiles() {
    const config = await loadSetupConfig();
    this.profiles = config.contextProfiles || {};
    return this.profiles;
  }
  
  async applyProfile(profileName, files) {
    const profile = await this.resolveProfile(profileName);
    
    // –ï—Å–ª–∏ –ø—Ä–æ—Ñ–∏–ª—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π
    if (profile.dynamic && profile.command) {
      return await this.executeDynamicProfile(profile);
    }
    
    // –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
    let filtered = files;
    
    if (profile.include) {
      filtered = micromatch(files, profile.include);
    }
    
    if (profile.exclude) {
      filtered = micromatch.not(filtered, profile.exclude);
    }
    
    // –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    if (profile.transformations) {
      filtered = await this.applyTransformations(filtered, profile.transformations);
    }
    
    return filtered;
  }
  
  async resolveProfile(profileName) {
    const profile = this.profiles[profileName];
    if (!profile) {
      throw new Error(`Profile '${profileName}' not found`);
    }
    
    // –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    if (profile.inherits) {
      const parent = await this.resolveProfile(profile.inherits);
      return { ...parent, ...profile };
    }
    
    return profile;
  }
  
  async executeDynamicProfile(profile) {
    const { stdout } = await execa('sh', ['-c', profile.command]);
    return stdout.split('\n').filter(Boolean);
  }
  
  async suggestProfiles(projectPath) {
    const suggestions = [];
    const config = await loadSetupConfig();
    
    if (!config.autoProfileDetection?.enabled) {
      return suggestions;
    }
    
    for (const rule of config.autoProfileDetection.rules) {
      if (await this.evaluateRule(rule.if, projectPath)) {
        suggestions.push(...rule.suggest);
      }
    }
    
    return suggestions;
  }
  
  async createCustomProfile(answers) {
    // –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è
    const newProfile = {
      description: answers.description,
      include: answers.include.split(',').map(p => p.trim()),
      exclude: answers.exclude.split(',').map(p => p.trim()),
      prioritySegments: answers.priority
    };
    
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø—Ä–æ—Ñ–∏–ª–µ–π
    const userProfiles = await this.loadUserProfiles();
    userProfiles[answers.name] = newProfile;
    await this.saveUserProfiles(userProfiles);
    
    return newProfile;
  }
}
```

### 2. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º:

```javascript
// –í src/cli/commands/queryProject.js
export async function queryProject(query, options) {
  const profileManager = new ProfileManager();
  await profileManager.loadProfiles();
  
  if (options.profile) {
    // –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ñ–∏–ª—å
    const profile = await profileManager.resolveProfile(options.profile);
    
    // –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –ø—Ä–æ—Ñ–∏–ª—è
    if (profile.prioritySegments) {
      query = enhanceQueryWithPriorities(query, profile.prioritySegments);
    }
    
    // –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    const results = await index.queryItems(queryVector, options.k || 10);
    const filteredResults = results.filter(r => 
      profileManager.matchesProfile(r.item.metadata.filePath, profile)
    );
  }
  // ...
}
```

### 3. –£–º–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º:

```javascript
// –ü—Ä–æ—Ñ–∏–ª—å –¥–ª—è –¥–µ–±–∞–≥–≥–∏–Ω–≥–∞ —Å runtime –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
class DebugProfile {
  async enhance(snapshot) {
    const additions = [];
    
    // –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—à–∏–±–∫–∏
    if (this.config.includeErrors) {
      const errors = await this.getRecentErrors();
      additions.push(`--- Runtime Errors ---\n${errors}\n`);
    }
    
    // –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
    if (this.config.includeTestResults) {
      const { stdout } = await execa('npm', ['test', '--', '--json']);
      const testResults = this.parseTestResults(stdout);
      additions.push(`--- Failed Tests ---\n${testResults}\n`);
    }
    
    // –î–æ–±–∞–≤–ª—è–µ–º git blame –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤
    if (this.config.includeGitBlame) {
      const blameInfo = await this.getGitBlameForErrors();
      additions.push(`--- Recent Changes ---\n${blameInfo}\n`);
    }
    
    return snapshot + additions.join('\n');
  }
}
```

## üéØ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏

### 1. –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏:

```javascript
// –ú–æ–∂–Ω–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏
const compositeProfile = {
  "api-with-tests": {
    "compose": ["backend", "api-tests"],
    "merge": "union", // –∏–ª–∏ "intersection"
    "postProcess": "remove-duplicates"
  }
}
```

### 2. –ü—Ä–æ—Ñ–∏–ª–∏ —Å —É—Å–ª–æ–≤–∏—è–º–∏:

```javascript
{
  "smart-debug": {
    "conditions": [
      {
        "if": "hasRecentErrors",
        "include": ["logs/**", "error-reports/**"]
      },
      {
        "if": "hasFailingTests", 
        "include": ["tests/**", "src/**/*.test.*"]
      }
    ]
  }
}
```

### 3. –ü—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤:

```javascript
{
  "agentProfiles": {
    "local_dev": {
      "defaultProfile": "frontend",
      "allowedProfiles": ["frontend", "tests", "debug"]
    },
    "production_server": {
      "defaultProfile": "backend",
      "allowedProfiles": ["backend", "infrastructure", "monitoring"]
    }
  }
}
```

## üìä UI –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è–º–∏

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä:

```javascript
// –ü—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è
async function interactiveProfileSelection() {
  const profiles = await profileManager.loadProfiles();
  const suggestions = await profileManager.suggestProfiles(process.cwd());
  
  const { selectedProfile } = await inquirer.prompt([{
    type: 'list',
    name: 'selectedProfile',
    message: 'Select context profile:',
    choices: [
      { name: 'üì¶ Full Project (default)', value: 'default' },
      ...suggestions.map(p => ({ 
        name: `‚≠ê ${p} (recommended)`, 
        value: p 
      })),
      new inquirer.Separator(),
      ...Object.entries(profiles)
        .filter(([k]) => !suggestions.includes(k))
        .map(([k, v]) => ({ 
          name: `${v.description}`, 
          value: k 
        }))
    ]
  }]);
  
  return selectedProfile;
}
```

## üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π

```json
// .ecksnapshot_profiles.local.json
{
  "userFavorites": {
    "my-daily": {
      "profile": "frontend",
      "additionalInclude": ["src/experiments/**"],
      "useCount": 42,
      "lastUsed": "2024-01-15"
    }
  },
  "teamProfiles": {
    "pr-review": {
      "profile": "review",
      "sharedBy": "team-lead",
      "mandatory": true
    }
  }
}
```

–≠—Ç–∞ —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Ñ–∏–ª–µ–π —Å–¥–µ–ª–∞–µ—Ç eckSnapshot –≥–æ—Ä–∞–∑–¥–æ –±–æ–ª–µ–µ –≥–∏–±–∫–∏–º –∏ —É–¥–æ–±–Ω—ã–º –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤ —Ä–∞–±–æ—Ç—ã. –û—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤, –≥–¥–µ –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–±—ã—Ç–æ—á–µ–Ω.



–∏–¥–µ—è –æ–± –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å—Ä–µ–¥. opos4.1

–û—Ç–ª–∏—á–Ω–∞—è –∏–¥–µ—è! –≠—Ç–æ –≥–æ—Ä–∞–∑–¥–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ - **–æ–¥–∏–Ω —Ä–∞–∑ —Å–æ–±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ä–µ–¥–µ –∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –≤ setup.json**, –≤–º–µ—Å—Ç–æ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫. –í–æ—Ç –∫–∞–∫ —ç—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å:

## üéØ –ö–æ–Ω—Ü–µ–ø—Ü–∏—è: Environment Discovery & Setup

### 1. **–ù–æ–≤–∞—è —Å–µ–∫—Ü–∏—è –≤ setup.json - —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã:**

```json
{
  "knownEnvironments": {
    "_comment": "Auto-discovered and fixed environment configurations",
    "environments": [
      {
        "id": "dev-wsl2-xelth",
        "discovered": "2024-01-15T10:30:00Z",
        "fingerprint": "linux/x64|v24.3.0|/home/xelth/ecKasse",
        "type": "development",
        "details": {
          "os": "Linux WSL2",
          "arch": "x64", 
          "nodeVersion": "v24.3.0",
          "npmVersion": "11.5.0",
          "database": "SQLite",
          "dbPath": "packages/core/db/eckasse_prod.sqlite3",
          "workingDir": "/home/xelth/ecKasse",
          "gitRemote": "github.com/xelth/ecKasse.git",
          "agent": "local_dev"
        }
      },
      {
        "id": "prod-debian-arm",
        "discovered": "2024-01-15T11:00:00Z",
        "fingerprint": "linux/arm64|v22.14.0|/var/www/eckasse.com",
        "type": "production",
        "details": {
          "os": "Linux Debian",
          "arch": "arm64",
          "nodeVersion": "v22.14.0",
          "database": "PostgreSQL",
          "dbVersion": "15.12",
          "dbName": "eckwms",
          "dbUser": "wms_user",
          "processManager": "PM2",
          "pm2App": "eckasse-desktop-server",
          "workingDir": "/var/www/eckasse.com",
          "agent": "production_server"
        }
      }
    ],
    "autoDetect": {
      "enabled": true,
      "updateOnMismatch": true,
      "notifyOnNewEnvironment": true
    }
  }
}
```

### 2. **Discovery –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è Gemini:**

```markdown
# Environment Discovery Protocol

## –î–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä–∞ (Gemini):
–ö–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –Ω–æ–≤—É—é —Å—Ä–µ–¥—É, –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª:

### –®–∞–≥ 1: –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
```bash
echo "=== ENVIRONMENT DISCOVERY START ==="
echo "BASIC|$(hostname)|$(pwd)|$(date -I)"
node -e "console.log('NODE|' + process.version + '|' + process.platform + '|' + process.arch)"
echo "USER|$(whoami)|$(id -gn)"
```

### –®–∞–≥ 2: –î–µ—Ç–∞–ª–∏ –ø—Ä–æ–µ–∫—Ç–∞
```bash
# Git info
echo "GIT|$(git remote -v | head -1)|$(git branch --show-current)"

# Package info  
echo "PKG|$(node -p "require('./package.json').name")|$(node -p "require('./package.json').version")"

# Database detection
if [ -f *.sqlite* ]; then 
  echo "DB|SQLite|$(ls *.sqlite* | head -1)"
elif command -v psql &> /dev/null; then
  echo "DB|PostgreSQL|$(psql --version | grep -oP '\d+\.\d+')"
  psql -U postgres -lqt 2>/dev/null | grep -E "eckwms|coral" | head -1
fi

# Process manager
if pm2 list &>/dev/null; then
  echo "PROC|PM2|$(pm2 list | grep online | cut -d'‚îÇ' -f2)"
else
  echo "PROC|npm|dev"
fi
```

### –®–∞–≥ 3: –°–ø–µ—Ü–∏—Ñ–∏–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –∏ –ø—É—Ç–µ–π
echo "PERMS|$([ -w /var/www ] && echo 'production' || echo 'development')"
echo "PORTS|$(netstat -tuln 2>/dev/null | grep -E ':3030|:3000' | head -1)"
```
```

### 3. **Workflow –¥–ª—è –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:**

```mermaid
graph TD
    A[–ù–æ–≤—ã–π –∞–≥–µ–Ω—Ç/—Å—Ä–µ–¥–∞] --> B[Gemini: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç discovery –∑–∞–ø—Ä–æ—Å—ã]
    B --> C[–ê–≥–µ–Ω—Ç: –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—ã]
    C --> D[–ê–≥–µ–Ω—Ç: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã]
    D --> E[Gemini: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã]
    E --> F[Gemini: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ setup.json]
    F --> G[–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç]
    G --> H[–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ setup.json]
    H --> I[–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–Ω–∞–ø—à–æ—Ç–∞]
```

### 4. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è Gemini:**

```markdown
## –ó–∞–ø—Ä–æ—Å –∫ Gemini –¥–ª—è discovery:

"–ú–Ω–µ –Ω—É–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å eckSnapshot –¥–ª—è –Ω–æ–≤–æ–π —Å—Ä–µ–¥—ã. 
–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –Ω–∞–±–æ—Ä –∫–æ–º–∞–Ω–¥ –¥–ª—è –∞–≥–µ–Ω—Ç–∞, —á—Ç–æ–±—ã —Å–æ–±—Ä–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ä–µ–¥–µ.
–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–æ–≤, —Å–æ–∑–¥–∞–π JSON-–ø–∞—Ç—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è setup.json —Å–µ–∫—Ü–∏–∏ knownEnvironments."
```

### 5. **–ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ Gemini –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞:**

```json
{
  "action": "update_setup_json",
  "section": "knownEnvironments.environments",
  "operation": "append",
  "data": {
    "id": "dev-windows-user123",
    "discovered": "2024-01-20T15:00:00Z",
    "fingerprint": "win32/x64|v20.11.0|C:\\Projects\\ecKasse",
    "type": "development",
    "details": {
      "os": "Windows 11",
      "arch": "x64",
      "nodeVersion": "v20.11.0",
      "database": "SQLite",
      "workingDir": "C:\\Projects\\ecKasse",
      "agent": "local_dev",
      "specialNotes": "Windows requires different path separators"
    }
  }
}
```

### 6. **Smart Detection –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å–Ω–∞–ø—à–æ—Ç–∞:**

```javascript
// –í src/cli/commands/createSnapshot.js –¥–æ–±–∞–≤–∏—Ç—å:

async function detectEnvironment(repoPath) {
  const setupConfig = await loadSetupConfig();
  const knownEnvs = setupConfig.knownEnvironments?.environments || [];
  
  // –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π fingerprint
  const currentFingerprint = `${process.platform}/${process.arch}|${process.version}|${repoPath}`;
  
  // –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
  const matchedEnv = knownEnvs.find(env => 
    env.fingerprint === currentFingerprint || 
    env.details.workingDir === repoPath
  );
  
  if (matchedEnv) {
    console.log(`‚úÖ Using known environment: ${matchedEnv.id}`);
    return matchedEnv;
  } else {
    console.log(`‚ö†Ô∏è Unknown environment detected!`);
    console.log(`Run: eck-snapshot discover`);
    return null;
  }
}
```

### 7. **–ù–æ–≤–∞—è CLI –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è discovery:**

```bash
# –î–æ–±–∞–≤–∏—Ç—å –≤ CLI
eck-snapshot discover [--agent <agent-id>] [--update-config]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
eck-snapshot discover                    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ
eck-snapshot discover --update-config    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–∏—Ç—å setup.json
eck-snapshot discover --agent local_dev  # –£–∫–∞–∑–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ —è–≤–Ω–æ
```

## üìù –î–ª—è —Ñ–∞–π–ª–∞ ideas.md:

```markdown
## Environment Auto-Discovery System

### –ü—Ä–æ–±–ª–µ–º–∞:
–†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ä–µ–¥ –ø–æ–¥–≤–µ—Ä–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞–º –∏ —Ç—Ä–µ–±—É–µ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫.

### –†–µ—à–µ–Ω–∏–µ:
–û–¥–Ω–æ—Ä–∞–∑–æ–≤–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—Ä–µ–¥—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ —Å —Ñ–∏–∫—Å–∞—Ü–∏–µ–π –≤ setup.json.

### –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- **–û–¥–∏–Ω —Ä–∞–∑ –Ω–∞—Å—Ç—Ä–æ–∏–ª - —Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞**: –ù–µ—Ç overhead –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
- **–¢–æ—á–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è**: –ö–∞–∂–¥–∞—è —Å—Ä–µ–¥–∞ –æ–ø–∏—Å–∞–Ω–∞ –¥–µ—Ç–∞–ª—å–Ω–æ
- **–ò—Å—Ç–æ—Ä–∏—è —Å—Ä–µ–¥**: –í–∏–¥–Ω–æ –∫–æ–≥–¥–∞ –∏ –∫–∞–∫–∏–µ —Å—Ä–µ–¥—ã –±—ã–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã
- **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∞–≥–µ–Ω—Ç–∞**: –ü–æ fingerprint —Å—Ä–µ–¥—ã
- **–£–ø—Ä–æ—â–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤**: –ù–µ –Ω—É–∂–Ω–æ –∫–∞–∂–¥—ã–π —Ä–∞–∑ –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Å—Ä–µ–¥—É

### –†–µ–∞–ª–∏–∑–∞—Ü–∏—è:
1. –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ: `eck-snapshot discover`
2. Gemini –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç discovery-–∫–æ–º–∞–Ω–¥—ã –¥–ª—è –∞–≥–µ–Ω—Ç–∞
3. –ê–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
4. Gemini —Å–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è setup.json
5. –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—É—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
```bash
# –ü–µ—Ä–≤—ã–π —Ä–∞–∑ –Ω–∞ –Ω–æ–≤–æ–π –º–∞—à–∏–Ω–µ
eck-snapshot discover --update-config

# –û–±—ã—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç —Å—Ä–µ–¥—É)
eck-snapshot snapshot
eck-snapshot index
```

### –ë—É–¥—É—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ fingerprint
- –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –º–µ–∂–¥—É –∫–æ–º–∞–Ω–¥–æ–π —á–µ—Ä–µ–∑ git
- –ü—Ä–æ—Ñ–∏–ª–∏ –¥–ª—è CI/CD –æ–∫—Ä—É–∂–µ–Ω–∏–π
- Docker-specific –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
```

–≠—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –¥–µ–ª–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —É–º–Ω–µ–µ - –æ–Ω "–∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç" —Å—Ä–µ–¥—ã –∏ –Ω–µ —Ç—Ä–∞—Ç–∏—Ç –≤—Ä–µ–º—è –Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏!

--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /knexfile.js ---

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
const isWSL = process.platform === 'linux' && 
  (process.env.WSL_DISTRO_NAME || 
   fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));

if (isWSL && (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1')) {
  // For WSL, always use the standard WSL2 gateway IP
  process.env.DB_HOST = '172.29.16.1';
}

export default {
  development: {
    client: 'pg',
    connection: {
      host: process.env.DB_HOST || '127.0.0.1',
      port: process.env.DB_PORT || 5432,
      user: process.env.DB_USER || 'myuser',
      password: process.env.DB_PASSWORD || 'mypassword',
      database: process.env.DB_DATABASE || 'eck_snapshot_db',
    },
    pool: {
      min: 2,
      max: 10
    }
  }
};

--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE"
  ],
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0"
  }
}


--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": ["build.gradle", "build.gradle.kts", "settings.gradle", "settings.gradle.kts"],
        "directories": ["app/src/main", "app/src/androidTest"],
        "manifestFiles": ["AndroidManifest.xml"],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": ["package.json"],
        "directories": ["packages", "apps", "libs"],
        "patterns": ["workspaces", "lerna", "nx", "rush"],
        "priority": 7
      },
      "nodejs": {
        "files": ["package.json"],
        "directories": ["node_modules"],
        "priority": 6
      },
      "python-poetry": {
        "files": ["pyproject.toml"],
        "patterns": ["tool.poetry"],
        "priority": 9
      },
      "python-pip": {
        "files": ["requirements.txt", "setup.py", "setup.cfg"],
        "directories": ["__pycache__", "venv", ".venv"],
        "priority": 7
      },
      "python-conda": {
        "files": ["environment.yml", "environment.yaml", "conda.yml"],
        "priority": 8
      },
      "django": {
        "files": ["manage.py"],
        "patterns": ["django", "Django"],
        "priority": 9
      },
      "flask": {
        "files": ["app.py", "application.py"],
        "patterns": ["flask", "Flask"],
        "priority": 8
      },
      "flutter": {
        "files": ["pubspec.yaml"],
        "directories": ["lib", "android", "ios"],
        "priority": 8
      },
      "react-native": {
        "files": ["package.json"],
        "directories": ["android", "ios"],
        "patterns": ["react-native"],
        "priority": 8
      },
      "rust": {
        "files": ["Cargo.toml"],
        "directories": ["src", "target"],
        "priority": 7
      },
      "go": {
        "files": ["go.mod", "go.sum"],
        "directories": ["cmd", "pkg", "internal"],
        "priority": 7
      },
      "dotnet": {
        "files": ["*.csproj", "*.sln", "*.fsproj", "*.vbproj"],
        "directories": ["bin", "obj"],
        "priority": 7
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "smartMode": {
    "tokenThreshold": 7000000,
    "_comment": "Projects over this token count will use vector indexing instead of single file snapshots"
  },
  "vectorIndex": {
    "autoExportOnIndex": true,
    "_comment": "Automatically export the vector index to a file after every successful 'index' command."
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "snapshots/",
      "coverage/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      }
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "Run this command and return the output:\n```\nnode -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"\n```",
      "gitWorkflow": "\n## Git Commit Workflow\n\n**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.\n\n### Commit Process\n1. **Review Changes**: Before committing, briefly summarize what was accomplished\n2. **Stage Files**: Include `git add .` or specific files in your command block\n3. **Create Commit**: Use a clear, descriptive commit message following this format:\n   - Start with the task context (e.g., \"feat:\", \"fix:\", \"docs:\", \"refactor:\")\n   - Include the task_id from your command block for traceability\n   - Keep it under 50 characters for the first line\n   - Add detailed description if needed\n\n### Example Git Commands to Include\n```bash\ngit add .\ngit commit -m \"feat: implement user authentication system\n\nTask ID: auth-system-implementation-1\n- Added login/register endpoints\n- Implemented JWT token validation\n- Added user session management\"\n```\n\n### When to Commit\n- After completing any feature implementation\n- After fixing bugs or issues\n- After refactoring or code improvements\n- After adding tests or documentation\n- Before major architectural changes\n\n**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.\n\n",
      "multiAgent": "\n# AI Instructions\n\n## 1. How to Read This Snapshot\n\nThis document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.\n\n* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.\n* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.\n\n**Snapshot Stats:**\n- **Files Included:** {{stats.includedFiles}}\n- **Total Files in Repo:** {{stats.totalFiles}}\n\n---\n\n## 2. Your Core Operational Workflow\n\nYou are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.\n\n### PROJECT OVERVIEW\n- **Project:** {{repoName}}\n- **Description:** A monorepo POS system with Electron frontend and Node.js backend.\n\n### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nSome projects include a `.eck` directory containing project-specific context and configuration. When present, this directory provides crucial information that should inform all your technical decisions:\n\n- **CONTEXT.md**: Project-specific background, architecture notes, and important context\n- **OPERATIONS.md**: Common operations, deployment procedures, and workflow guidance\n- **JOURNAL.md**: Development history, recent changes, and ongoing notes\n- **ENVIRONMENT.md**: Environment-specific settings that override auto-detected values (YAML format)\n\n**CRITICAL**: Always prioritize information from the .eck manifest when it conflicts with auto-detected project settings. Use this context when formulating plans and briefing execution agents.\n\n### CORE WORKFLOW: The Interactive Command Cycle\n1. **Check Environment:** Request ENV scan from agent first\n2. **Analyze User Request:** Understand the user's goal in their native language.\n3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.\n4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**\n5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.\n   - **On Approval:** If the user confirms the plan (e.g., \"yes\", \"proceed\") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.\n   - **On Direct Order:** If the user explicitly asks for the command (e.g., \"make the command for Claude now\") and you have all the necessary information, you may skip step 3 and directly generate the command block.\n6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.\n7. **Iterate:** Continue the cycle based on user feedback.\n\n### COMMUNICATION PROTOCOL\n- **User Interaction:** ALWAYS communicate with the user in the language they use.\n- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.\n- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.\n\n### AVAILABLE EXECUTION AGENTS\nYou can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:\n\n{{agentDefinitions}}\n\n### COMMAND BLOCK FORMAT\nTo ensure error-free execution, all tasks for agents must be presented in a special block with a \"Copy\" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:\n\n```json\n{\n  \"target_agent\": \"local_dev\",\n  \"agent_environment\": \"Development environment with full GUI support and development tools\",\n  \"command_for_agent\": \"apply_code_changes\",\n  \"task_id\": \"unique-task-id\",\n  \"payload\": {\n    \"objective\": \"Brief, clear task description\",\n    \"context\": \"Why this change is needed - include relevant .eck manifest context\",\n    \"files_to_modify\": [\n      {\n        \"path\": \"exact/file/path.js\",\n        \"action\": \"specific action (add, modify, replace, delete)\",\n        \"location\": \"line numbers, function name, or search pattern\",\n        \"details\": \"precise description of the change\"\n      }\n    ],\n    \"new_files\": [\n      {\n        \"path\": \"path/to/new/file.js\",\n        \"content_type\": \"javascript/json/markdown/config\",\n        \"purpose\": \"why this file is needed\"\n      }\n    ],\n    \"dependencies\": {\n      \"install\": [\"package-name@version\"],\n      \"remove\": [\"old-package-name\"]\n    },\n    \"validation_steps\": [\n      \"npm run test\",\n      \"node index.js --help\",\n      \"specific command to verify functionality\"\n    ],\n    \"expected_outcome\": \"what should work after changes\"\n  }\n}\n```\n\n---\n",
      "vectorMode": "\n# AI Instructions\n\n## 1. How to Read This Snapshot\n\nThis document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: \"{{userQuery}}\"\n\n* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.\n* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.\n\n---\n\n## 2. Your Core Operational Workflow\n\nYou are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.\n\n### PROJECT OVERVIEW\n- **Project:** {{repoName}}\n- **User Query:** \"{{userQuery}}\"\n- **Description:** A monorepo POS system with Electron frontend and Node.js backend.\n\n{{multiAgentSection}}\n\n---\n"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .action(createRepoSnapshot);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  program.parse(process.argv);
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function runFileSnapshot(repoPath, options, config, estimation = null, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Creating snapshot of: ${path.basename(repoPath)}`);
  console.log(`üìÅ Repository path: ${repoPath}`);
  
  // Initialize detailed stats with skip tracking
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    // Get all files and setup gitignore
    console.log('üîç Scanning repository...');
    const allFiles = await getProjectFiles(repoPath, config);
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    // Generate directory tree if enabled (config.tree can be overridden by --no-tree flag)
    let directoryTree = '';
    const shouldIncludeTree = config.tree && !options.noTree;
    if (shouldIncludeTree) {
      console.log('üå≥ Generating directory tree...');
      directoryTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth || 10, config);
    }
    
    // Setup progress bar
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    // Helper function to track skipped files
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return `--- File: /${normalizedPath} ---\n\n${content}\n\n`;
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const contentArray = results.filter(Boolean);
    
    // Prepare basic info
    const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
    const repoName = path.basename(repoPath);
    const gitHash = await getGitCommitHash(repoPath);
    
    // Determine if AI header should be included
    // Priority: command line flag (aiHeader) overrides config setting (aiHeaderEnabled)
    // Commander.js converts --no-ai-header to aiHeader: false
    const shouldIncludeAiHeader = config.aiHeaderEnabled && (options.aiHeader !== false);
    
    // Check if this is a Git repository
    const isGitRepo = await checkGitRepository(repoPath);
    
    // Load .eck manifest if it exists
    const eckManifest = await loadProjectEckManifest(repoPath);
    
    // Generate AI header if needed (for both formats)
    let aiHeader = '';
    if (shouldIncludeAiHeader) {
      aiHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest }, isGitRepo);
    }
    
    // Prepare content based on format
    const outputPath = options.output || path.resolve(originalCwd, config.output);
    await fs.mkdir(outputPath, { recursive: true });
    
    let outputContent = '';
    let outputFilename = '';
    let fileExtension = options.format || config.defaultFormat || 'md';
    
    if (fileExtension === 'json') {
      // JSON format - convert Maps to objects for serialization
      const serializableStats = {
        ...stats,
        skipReasons: Object.fromEntries(stats.skipReasons),
        skippedFilesDetails: Object.fromEntries(stats.skippedFilesDetails)
      };
      
      const jsonOutput = {
        metadata: {
          repoName,
          timestamp: new Date().toISOString(),
          toolVersion: '4.0.0',
          format: 'json'
        },
        statistics: serializableStats,
        directoryTree: directoryTree,
        aiInstructionsHeader: aiHeader,
        files: contentArray.map(content => {
          const match = content.match(/--- File: \/(.+) ---\n\n([\s\S]*?)\n\n$/);
          return {
            path: match[1],
            content: match[2]
          };
        })
      };
      outputContent = JSON.stringify(jsonOutput, null, 2);
      outputFilename = gitHash 
        ? `${repoName}_snapshot_${timestamp}_${gitHash}.json`
        : `${repoName}_snapshot_${timestamp}.json`;
    } else {
      // Markdown format (default)
      outputContent = aiHeader;
      
      if (directoryTree) {
        outputContent += '\n## Directory Structure\n\n```\n' + directoryTree + '```\n\n';
      }
      
      outputContent += contentArray.join('');
      outputFilename = gitHash 
        ? `${repoName}_snapshot_${timestamp}_${gitHash}.md`
        : `${repoName}_snapshot_${timestamp}.md`;
    }
    
    const fullOutputFilePath = path.join(outputPath, outputFilename);
    await fs.writeFile(fullOutputFilePath, outputContent);
    
    // Print detailed summary
    console.log('\n‚úÖ Snapshot completed successfully!');
    console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
    console.log(`üìÑ Output file: ${fullOutputFilePath}`);
    console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
    console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
    console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
    console.log(`üìã Format: ${fileExtension.toUpperCase()}`);
    
    if (stats.excludedFiles > 0) {
      console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
    }
    if (stats.binaryFiles > 0) {
      console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
    }
    if (stats.oversizedFiles > 0) {
      console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
    }
    if (stats.ignoredFiles > 0) {
      console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
    }
    if (stats.errors.length > 0) {
      console.log(`‚ùå Errors: ${stats.errors.length}`);
      if (options.verbose) {
        stats.errors.forEach(err => console.log(`   ${err}`));
      }
    }
    
    // Print detailed skip reasons report
    if (stats.skippedFilesDetails.size > 0) {
      console.log('\nüìã Skip Reasons:');
      console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
      
      for (const [reason, files] of stats.skippedFilesDetails.entries()) {
        console.log(`\nüî∏ ${reason} (${files.length} files):`);
        files.forEach(file => {
          console.log(`   ‚Ä¢ ${file}`);
        });
      }
      console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
    } else {
      console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
    }
    
    // Generate training command string if estimation data is available
    if (estimation && projectType) {
      const trainingCommand = generateTrainingCommand(projectType, estimation.estimatedTokens, estimation.totalSize, repoPath);
      console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
      console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
      console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
    }
    
  } finally {
    process.chdir(originalCwd);
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      smartModeTokenThreshold: setupConfig.smartMode.tokenThreshold,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    if (estimation.estimatedTokens > config.smartModeTokenThreshold) {
      spinner.succeed('Project is large. Switching to vector indexing mode.');
      await indexProject(repoPath, options);
    } else {
      spinner.succeed('Project is small. Creating a single-file snapshot.');
      await runFileSnapshot(repoPath, options, config, estimation, projectDetection.type);
    }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}

--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));
    
    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
      console.log('   eck-snapshot index                              # For large projects');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
      console.log('   eck-snapshot index                              # For large projects');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
      console.log('   eck-snapshot index                              # For semantic search');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { loadSetupConfig } from '../../config.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    await initDb();
    const knex = getKnex();
    const config = await loadSetupConfig();
    
    let files = await getProjectFiles(projectPath);
    const profileName = options.profile || 'default';
    if (options.profile) {
        const profile = config.contextProfiles[options.profile];
        if (!profile) throw new Error(`Profile '${options.profile}' not found in setup.json`);
        mainSpinner.info(`Using profile: '${options.profile}'.`);
        files = micromatch(files, profile.include, { ignore: profile.exclude });
    }

    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }
        await releaseAnalysisModel();

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
        await releaseEmbeddingModel();
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}

--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-—Å–Ω–∞–ø—à–æ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/core/segmenter.js ---

import { parse } from '@babel/parser';
import _traverse from '@babel/traverse';
const traverse = _traverse.default;
import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import Parser from 'tree-sitter';
import Python from 'tree-sitter-python';
import Java from 'tree-sitter-java';
import Kotlin from 'tree-sitter-kotlin';

function generateHash(content) {
  return crypto.createHash('sha256').update(content).digest('hex');
}

const tsParser = new Parser();
const languageParsers = {
    '.py': Python,
    '.java': Java,
    '.kt': Kotlin,
};

async function _segmentWithTreeSitter(content, filePath, language) {
    tsParser.setLanguage(language);
    const tree = tsParser.parse(content);
    const chunks = [];
    // Graph relations for tree-sitter are not implemented in this step.
    const relations = [];

    function walk(node) {
        const nodeTypeMap = {
            'function_definition': 'function', 'class_definition': 'class', // Python
            'function_declaration': 'function', 'class_declaration': 'class', // Kotlin/Java
            'method_declaration': 'function', // Java
        };

        if (nodeTypeMap[node.type]) {
            const nameNode = node.childForFieldName('name') || node.child(1);
            const chunkName = nameNode ? nameNode.text : 'anonymous';
            const chunkCode = node.text;
            chunks.push({
                filePath,
                chunk_type: nodeTypeMap[node.type],
                chunk_name: chunkName,
                code: chunkCode,
                contentHash: generateHash(chunkCode)
            });
        }
        node.children.forEach(walk);
    }
    walk(tree.rootNode);
    return { chunks, relations };
}

async function _segmentJavaScript(content, filePath) {
    const chunks = [];
    const relations = [];

    try {
        const ast = parse(content, { sourceType: 'module', plugins: ['typescript', 'jsx'], errorRecovery: true });

        const getChunkData = (node) => {
            const chunkName = node.id ? node.id.name : 'anonymous';
            const chunkCode = content.substring(node.start, node.end);
            return { filePath, chunk_name: chunkName, code: chunkCode, contentHash: generateHash(chunkCode) };
        };

        traverse(ast, {
            enter(path) {
                let currentScopeName = 'file';
                const parentFunction = path.findParent((p) => p.isFunctionDeclaration() || p.isClassDeclaration());
                if (parentFunction && parentFunction.node.id) {
                    currentScopeName = parentFunction.node.id.name;
                }

                if (path.isFunctionDeclaration() || path.isClassDeclaration()) {
                    chunks.push({ ...getChunkData(path.node), chunk_type: path.isClassDeclaration() ? 'class' : 'function' });
                }

                if (path.isImportDeclaration()) {
                    const sourceFile = path.node.source.value;
                    relations.push({ from: filePath, to: sourceFile, type: 'IMPORTS' });
                }

                if (path.isCallExpression()) {
                    const calleeName = path.get('callee').toString();
                    relations.push({ from: currentScopeName, to: calleeName, type: 'CALLS' });
                }
            }
        });
    } catch (e) {
        console.error(`Babel parsing error in ${filePath}: ${e.message}`);
    }
    return { chunks, relations };
}

export async function segmentFile(filePath) {
    try {
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = path.extname(filePath);
        let result = { chunks: [], relations: [] };

        if (['.js', '.jsx', '.ts', '.tsx'].includes(extension)) {
            result = await _segmentJavaScript(content, filePath);
        } else if (languageParsers[extension]) {
            result = await _segmentWithTreeSitter(content, filePath, languageParsers[extension]);
        }
        
        // Fallback: if no specific chunks, treat the whole file as one
        if (result.chunks.length === 0) {
            const code = content;
            result.chunks.push({ filePath, chunk_type: 'file', chunk_name: path.basename(filePath), code, contentHash: generateHash(code) });
        }

        return result;
    } catch (error) {
        console.error(`Failed to segment file ${filePath}: ${error.message}`);
        return { chunks: [], relations: [] };
    }
}

--- File: /src/database/postgresConnector.js ---

import knex from 'knex';
import fs from 'fs/promises';
import path from 'path';
import config from '../../knexfile.js';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));

let knexInstance = null;

function getKnex() {
  if (!knexInstance) {
    console.log('Initializing Knex connection...');
    knexInstance = knex(config.development);
  }
  return knexInstance;
}

async function initDb() {
  const db = getKnex();
  try {
    console.log('Checking database connection...');
    await db.raw('SELECT 1+1 AS result');
    console.log('Connection successful.');

    console.log('Applying database schema...');
    // Try full schema first, fallback to simple schema
    try {
      const schemaPath = path.join(__dirname, 'schema.sql');
      const schemaSQL = await fs.readFile(schemaPath, 'utf-8');
      await db.raw(schemaSQL);
      console.log('Full schema with vector extensions applied successfully.');
    } catch (error) {
      console.log('Vector extensions not available, using simplified schema...');
      const simpleSchemaPath = path.join(__dirname, 'schema_simple.sql');
      const simpleSchemaSQL = await fs.readFile(simpleSchemaPath, 'utf-8');
      await db.raw(simpleSchemaSQL);
      console.log('Simplified schema applied successfully.');
    }
  } catch (error) {
    console.error('Error initializing database:', error.message);
    throw error;
  }
}

async function destroyDb() {
  if (knexInstance) {
    console.log('Destroying Knex connection pool...');
    await knexInstance.destroy();
    knexInstance = null;
  }
}

export {
  getKnex,
  initDb,
  destroyDb,
};

--- File: /src/database/schema.sql ---

-- –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
-- (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ Apache AGE —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ PG)
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç)
SELECT create_graph('eck_snapshot_graph');

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞ (—É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding VECTOR(768), -- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è Jina Code v2
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π (—Ä–µ–±–µ—Ä –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

-- –°–æ–∑–¥–∞–µ–º HNSW-–∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX IF NOT EXISTS code_chunks_embedding_idx ON code_chunks USING HNSW (embedding vector_cosine_ops);

--- File: /src/database/schema_simple.sql ---

-- Simplified schema without vector and graph extensions for testing

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding TEXT, -- JSON string representation for now
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

--- File: /src/services/analysisService.js ---

import { pipeline } from '@xenova/transformers';

class AnalysisService {
    static instance = null;
    static modelName = 'Xenova/distilgpt2'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = await pipeline('text-generation', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = null;
        }
    }
}

export async function getCodeSummary(codeChunk) {
    const generator = await AnalysisService.getInstance();

    const prompt = `This code:\n${codeChunk.substring(0, 150)}\nSummary:`;

    const output = await generator(prompt, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true
    });

    const generatedText = output[0].generated_text;
    const summary = generatedText.replace(prompt, '').trim() || 'Auto-generated description';
    return summary.substring(0, 200); // Limit summary length
}

export const releaseModel = AnalysisService.releaseModel;

--- File: /src/services/embedding.js ---

import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';
import pLimit from 'p-limit';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "embedding-001" });

async function generateEmbedding(text, taskType = 'RETRIEVAL_DOCUMENT') {
  try {
    const result = await model.embedContent({ 
      content: { parts: [{ text }] },
      taskType
    });
    return result.embedding.values;
  } catch (error) {
    console.error('‚ùå Gemini Embedding Error:', error.message);
    throw error;
  }
}

async function generateBatchEmbeddings(segments, taskType = 'RETRIEVAL_DOCUMENT') {
    if (segments.length === 0) return [];

    const BATCH_COUNT_LIMIT = 100;
    const BATCH_SIZE_LIMIT = 3000000; // 3MB for safety
    const allBatches = [];
    let currentBatch = [];
    let currentBatchSize = 0;

    for (const segment of segments) {
        const segmentSize = Buffer.byteLength(segment.content, 'utf8');
        if (segmentSize > BATCH_SIZE_LIMIT) { // Handle single oversized segments
            console.log(chalk.yellow(`  -> Warning: Segment '${segment.name}' in '${segment.filePath}' is oversized and will be truncated.`));
            segment.content = segment.content.substring(0, 20000) + '... [truncated]'; // Truncate oversized segment
        }
        if (currentBatch.length > 0 && (currentBatch.length >= BATCH_COUNT_LIMIT || currentBatchSize + Buffer.byteLength(segment.content, 'utf8') > BATCH_SIZE_LIMIT)) {
            allBatches.push(currentBatch);
            currentBatch = [];
            currentBatchSize = 0;
        }
        currentBatch.push(segment);
        currentBatchSize += Buffer.byteLength(segment.content, 'utf8');
    }
    if (currentBatch.length > 0) {
        allBatches.push(currentBatch);
    }

    console.log(chalk.cyan(`‚è≥ Generating embeddings for ${segments.length} segments, divided into ${allBatches.length} safe chunks...`));

    const limit = pLimit(5); // Set concurrency to 5 parallel requests
    let processedCount = 0;

    const promises = allBatches.map((batch, i) => {
        return limit(async () => {
            const batchSizeKB = (Buffer.byteLength(batch.map(s => s.content).join(''), 'utf8') / 1024).toFixed(2);
            console.log(chalk.blue(`  -> Sending chunk ${i + 1}/${allBatches.length} (${batch.length} segments, ${batchSizeKB} KB)...`));
            try {
                const contents = batch.map(s => ({ parts: [{ text: s.content }] }));
                const result = await model.batchEmbedContents({ 
                    requests: contents.map(content => ({ content, taskType }))
                });
                processedCount++;
                console.log(chalk.green(`  <- Chunk ${i + 1}/${allBatches.length} processed successfully.`));
                return result.embeddings.map(e => e.values);
            } catch (error) {
                console.error(chalk.red(`‚ùå Gemini Batch Embedding Error (Chunk ${i + 1}/${allBatches.length}):`), error.message);
                throw error;
            }
        });
    });

    const chunkResults = await Promise.all(promises);
    const allEmbeddings = chunkResults.flat();

    console.log(chalk.green.bold('‚úÖ Batch embeddings generated successfully.'));
    return allEmbeddings;
}

export const embeddingService = {
  generateEmbedding,
  generateBatchEmbeddings
};

--- File: /src/services/embeddingService.js ---

import { pipeline } from '@xenova/transformers';

class EmbeddingService {
    static instance = null;
    static modelName = 'Xenova/jina-embeddings-v2-base-en'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = await pipeline('feature-extraction', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = null;
            // In Node.js, there's no explicit GPU memory release, 
            // relying on the garbage collector is the standard way.
        }
    }
}

export async function generateEmbedding(code) {
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(code, { pooling: 'mean', normalize: true });
    return Array.from(result.data);
}

export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) return [];
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(texts, { pooling: 'mean', normalize: true });
    
    // Convert tensor to array of arrays
    const embeddings = [];
    for (let i = 0; i < result.dims[0]; i++) {
        const start = i * result.dims[1];
        const end = start + result.dims[1];
        embeddings.push(Array.from(result.data.slice(start, end)));
    }
    return embeddings;
}

export const releaseModel = EmbeddingService.releaseModel;

--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig } from '../config.js';

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Count active agents to determine template
    const activeAgents = Object.values(executionAgents).filter(agent => agent.active);
    const isMultiAgent = activeAgents.length > 1;

    let template;
    if (context.mode === 'vector') {
      template = promptTemplates.vectorMode;
      // For vector mode, build the multi-agent section dynamically
      const multiAgentSection = isMultiAgent ? 
        `### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

${buildAgentDefinitions(executionAgents)}

### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

\`\`\`json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed",
    "files_to_modify": [...],
    "new_files": [...],
    "dependencies": {...},
    "validation_steps": [...],
    "expected_outcome": "what should work after changes"
  }
}
\`\`\`

**Agent Selection Guidelines:**
- Choose the agent ID based on task requirements and environment constraints
- Copy the agent's description to "agent_environment" field
- Ensure the task matches the agent's capabilities and restrictions` :
        `### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for the agent must be presented in a special block with a "Copy" button:

\`\`\`json
{
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed",
    "files_to_modify": [...],
    "new_files": [...],
    "dependencies": {...},
    "validation_steps": [...],
    "expected_outcome": "what should work after changes"
  }
}
\`\`\``;
      
      template = template.replace('{{multiAgentSection}}', multiAgentSection);
    } else {
      // Always use multiAgent template for file snapshots
      template = promptTemplates.multiAgent;
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions
    };

    let renderedTemplate = render(template, data);
    
    // Add .eck manifest section if present
    if (context.eckManifest) {
      const eckSection = buildEckManifestSection(context.eckManifest);
      // Insert the .eck section after the main instructions but before the content
      const insertPoint = renderedTemplate.indexOf('---\n');
      if (insertPoint !== -1) {
        renderedTemplate = renderedTemplate.slice(0, insertPoint) + eckSection + renderedTemplate.slice(insertPoint);
      } else {
        renderedTemplate += eckSection;
      }
    }

    // Add Git workflow instructions if this is a Git repository
    if (isGitRepo && promptTemplates.gitWorkflow) {
      const insertPoint = renderedTemplate.indexOf('---\n');
      if (insertPoint !== -1) {
        renderedTemplate = renderedTemplate.slice(0, insertPoint) + promptTemplates.gitWorkflow + renderedTemplate.slice(insertPoint);
      } else {
        renderedTemplate += promptTemplates.gitWorkflow;
      }
    }

    return renderedTemplate;
  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header');
    return `# Snapshot for ${context.repoName || 'Project'}\n\nGenerated: ${new Date().toISOString()}\n\n---\n\n`;
  }
}

--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = 'snapshots/';
  const comment = '# Added by eck-snapshot to prevent committing snapshots';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));
  
  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }
  
  const best = detections[0];
  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;
  
  // Check for required files (faster, check only direct files)
  if (pattern.files) {
    for (const file of pattern.files) {
      const exists = await fileExists(path.join(projectPath, file));
      if (exists) {
        score += 25; // Each required file adds points
      }
    }
  }
  
  // Check for required directories
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const exists = await directoryExists(path.join(projectPath, dir));
      if (exists) {
        score += 20; // Each required directory adds points
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names or in description/keywords
        const foundInDeps = Object.keys(allDeps).some(dep => dep.includes(patternText));
        const foundInMeta = JSON.stringify({
          description: packageJson.description,
          keywords: packageJson.keywords
        }).toLowerCase().includes(patternText.toLowerCase());
        
        if (foundInDeps || foundInMeta) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };
  
  try {
    const cargoPath = path.join(projectPath, 'Cargo.toml');
    if (!await fileExists(cargoPath)) {
      return details;
    }
    
    const content = await fs.readFile(cargoPath, 'utf-8');
    
    const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = content.match(/edition\s*=\s*"([^"]+)"/);
    
    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];
    
    // Check if it's a workspace
    if (content.includes('[workspace]')) {
      details.isWorkspace = true;
    }
    
  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }
  
  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    return JSON.parse(data);
  } catch (error) {
    // If file doesn't exist, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];
  if (points.length < 2) return;
  
  // Simple linear regression for now (bytes -> tokens)
  // We can make this more sophisticated later with higher order polynomials
  
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;
  
  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;
    
    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }
  
  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
  const intercept = (sumY - slope * sumX) / n;
  
  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

