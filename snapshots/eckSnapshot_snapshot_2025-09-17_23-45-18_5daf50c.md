
# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-09-17T23:45:18.715Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 40
- **Total Files in Repo:** 49

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

### PROJECT-SPECIFIC MANIFEST (.eck Directory)

This project includes a `.eck` directory containing project-specific context and configuration. Always prioritize information from this manifest.

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
```

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle
1. **Check Environment:** Request ENV scan from agent first
2. **Analyze User Request:** Understand the user's goal in their native language.
3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
   - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
   - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7. **Iterate:** Continue the cycle based on user feedback.

### COMMUNICATION PROTOCOL
- **User Interaction:** ALWAYS communicate with the user in the language they use.
- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces


### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

**MANDATORY STRUCTURED LOGGING**: Every command payload MUST include 'post_execution_steps' with a structured 'journal_entry' object. The execution agent will use these fields to generate both a conventional Git commit message and a Markdown journal entry with YAML Frontmatter for machine-readable project history.

**Journal Entry Fields**:
- **type**: Conventional commit type (feat, fix, docs, style, refactor, test, chore)
- **scope**: Functional area affected (e.g., auth, ui, api, database)
- **summary**: Brief description of the change (becomes commit subject)
- **details**: Comprehensive explanation for the project journal

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      }
    }
  }
}
```

---

## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ create-snapshot/
‚îú‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queryProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ segmenter.js
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresConnector.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_simple.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysisService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddingService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ geminiWebService.js
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ knexfile.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ setup.json
```

--- File: /.claude/commands/eck/commit.md ---

---
allowed-tools: Bash(git diff:*), Bash(git add:*), Bash(git commit:*), Bash(echo:*), Bash(cat:*), Bash(mv:*)
description: Commits staged changes with structured journaling.
args:
  - name: type
    description: "The type of change (feat, fix, refactor, docs, chore)"
  - name: scope
    description: "The scope of the change (e.g., workflow, ui, api)"
  - name: summary
    description: "A short summary of the change"
  - name: details
    description: "A detailed description for the journal"
---

## Your Task

Based on the provided arguments, you must perform the following steps precisely:

1.  **Stage all current changes** to ensure they are included in the commit.
2.  **Create a YAML Frontmatter block** for the journal entry. It must be in the following format:
    ```yaml
    ---
    task_id: {unique-task-id-from-context}
    date: {current-iso-date}
    type: {arg1}
    scope: {arg2}
    --- 
    ```
3.  **Create a Markdown body** for the journal entry with the summary as a heading and details below it.
4.  **Prepend the complete journal entry** (YAML and Markdown) to the `.eck/JOURNAL.md` file. Do not overwrite the file.
5.  **Create a conventional commit message** in the format: `{arg1}({arg2}): {arg3}`.
6.  **Execute the commit** with the generated message.

Execute these steps using Bash tools. Do not send any other text or messages besides the necessary tool calls.

--- File: /.claude/settings.local.json ---

{
  "permissions": {
    "allow": [
      "Bash(node:*)",
      "Bash(git add:*)",
      "Bash(NODE_ENV=production node index.js --no-tree -o /tmp)",
      "Bash(cat:*)",
      "Bash(NODE_ENV=production USER=root unset DISPLAY)",
      "Bash(env)",
      "Bash(env:*)",
      "Bash(mkdir:*)",
      "Bash(npm install)",
      "Bash(grep:*)",
      "Bash(sed:*)",
      "Bash(awk:*)",
      "Bash(rm:*)",
      "Bash(git push:*)",
      "Bash(npm install:*)",
      "Bash(git checkout:*)",
      "Bash(timeout:*)",
      "Bash(git rm:*)",
      "Bash(git reset:*)",
      "Bash(npm run build:*)",
      "Bash(npm run:*)",
      "Bash(./bin/eck-snapshot.js:*)",
      "Read(/tmp/test_hygiene_repo/**)",
      "Read(/tmp/test-project/.eck/**)",
      "Read(/tmp/test-project/.eck/**)",
      "Bash(git commit:*)",
      "Bash(test:*)",
      "Bash(git log:*)",
      "Read(/home/xelth/.claude/**)",
      "Read(/tmp/new-claude-test-project/**)",
      "Bash(claude \"What is my name?\" -p --session-id=\"3bb9275f-b107-46f3-88a9-a1e83361ebd7\" --output-format=stream-json --verbose)",
      "Bash(! grep 'claudeCliService' .gitignore)"
    ],
    "deny": [],
    "defaultMode": "acceptEdits"
  }
}

--- File: /.env.example ---

# Google Gemini API Key - Required for the 'index' and 'query' commands
# Get your key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_API_KEY_HERE"

# PostgreSQL Connection Details
DB_HOST=127.0.0.1
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_DATABASE=eck_snapshot_db

--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist

# Snapshots
/snapshots/

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# EckSnapshot Index
/.ecksnapshot_index/

# Generated snapshot files
*_vectors.json
*_rag_*.md

# Environment variables
.env
.env.local
.env.production

# Added by eck-snapshot to prevent committing snapshots
snapshots/



--- File: /LICENSE ---

MIT License

Copyright (c) 2025 Dmytro Surovtsev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


--- File: /create-snapshot/.eck/CONTEXT.md ---

# Project Overview: eckSnapshot

## Description
This is the eckSnapshot project, a CLI tool to create, index, and query codebase snapshots for AI context.

## Architecture
Node.js CLI tool with modules for parsing (core), database interaction (database), AI services (services), and CLI commands (cli).

## Key Technologies
- Node.js
- PostgreSQL (with simple JSON fallback)
- @xenova/transformers.js (local AI models)
- tree-sitter (multi-language parsing)
- commander.js

--- File: /create-snapshot/.eck/JOURNAL.md ---

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure


--- File: /create-snapshot/.eck/OPERATIONS.md ---

# Common Operations

## Development Setup
```bash
# Install dependencies
npm install
```

## Running the Project (CLI Commands)
```bash
# Create a full snapshot
node index.js snapshot [path]

# Index the project for vector search
node index.js index .

# Query the index
node index.js query "your search query"
```

## Testing
```bash
# No formal test script defined
npm test
```

--- File: /create-snapshot/.eck/ROADMAP.md ---

# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization


--- File: /create-snapshot/.eck/TECH_DEBT.md ---

# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1


--- File: /create-snapshot/.gitignore ---

# Added by eck-snapshot to prevent committing snapshots
snapshots/


--- File: /create-snapshot/test_snapshots/create-snapshot_snapshot_2025-09-17_23-05-48.md ---


# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **create-snapshot** software repository, generated by the `eck-snapshot` tool on **2025-09-17T23:05:48.792Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 0
- **Total Files in Repo:** 0

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** create-snapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

### PROJECT-SPECIFIC MANIFEST (.eck Directory)

This project includes a `.eck` directory containing project-specific context and configuration. Always prioritize information from this manifest.

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
```

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle
1. **Check Environment:** Request ENV scan from agent first
2. **Analyze User Request:** Understand the user's goal in their native language.
3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
   - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
   - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7. **Iterate:** Continue the cycle based on user feedback.

### COMMUNICATION PROTOCOL
- **User Interaction:** ALWAYS communicate with the user in the language they use.
- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces


### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

**MANDATORY STRUCTURED LOGGING**: Every command payload MUST include 'post_execution_steps' with a structured 'journal_entry' object. The execution agent will use these fields to generate both a conventional Git commit message and a Markdown journal entry with YAML Frontmatter for machine-readable project history.

**Journal Entry Fields**:
- **type**: Conventional commit type (feat, fix, docs, style, refactor, test, chore)
- **scope**: Functional area affected (e.g., auth, ui, api, database)
- **summary**: Brief description of the change (becomes commit subject)
- **details**: Comprehensive explanation for the project journal

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      }
    }
  }
}
```

---


--- File: /create-snapshot/test_snapshots/create-snapshot_snapshot_2025-09-17_23-06-31.md ---


# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **create-snapshot** software repository, generated by the `eck-snapshot` tool on **2025-09-17T23:06:31.644Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 0
- **Total Files in Repo:** 0

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** create-snapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

### PROJECT-SPECIFIC MANIFEST (.eck Directory)

This project includes a `.eck` directory containing project-specific context and configuration. Always prioritize information from this manifest.

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
```

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle
1. **Check Environment:** Request ENV scan from agent first
2. **Analyze User Request:** Understand the user's goal in their native language.
3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
   - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
   - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7. **Iterate:** Continue the cycle based on user feedback.

### COMMUNICATION PROTOCOL
- **User Interaction:** ALWAYS communicate with the user in the language they use.
- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces


### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

**MANDATORY STRUCTURED LOGGING**: Every command payload MUST include 'post_execution_steps' with a structured 'journal_entry' object. The execution agent will use these fields to generate both a conventional Git commit message and a Markdown journal entry with YAML Frontmatter for machine-readable project history.

**Journal Entry Fields**:
- **type**: Conventional commit type (feat, fix, docs, style, refactor, test, chore)
- **scope**: Functional area affected (e.g., auth, ui, api, database)
- **summary**: Brief description of the change (becomes commit subject)
- **details**: Comprehensive explanation for the project journal

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      }
    }
  }
}
```

---


--- File: /create-snapshot/test_snapshots/create-snapshot_snapshot_2025-09-17_23-07-15.md ---


# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **create-snapshot** software repository, generated by the `eck-snapshot` tool on **2025-09-17T23:07:15.920Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 0
- **Total Files in Repo:** 0

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** create-snapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

### PROJECT-SPECIFIC MANIFEST (.eck Directory)

This project includes a `.eck` directory containing project-specific context and configuration. Always prioritize information from this manifest.

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
```

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle
1. **Check Environment:** Request ENV scan from agent first
2. **Analyze User Request:** Understand the user's goal in their native language.
3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
   - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
   - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7. **Iterate:** Continue the cycle based on user feedback.

### COMMUNICATION PROTOCOL
- **User Interaction:** ALWAYS communicate with the user in the language they use.
- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces


### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

**MANDATORY STRUCTURED LOGGING**: Every command payload MUST include 'post_execution_steps' with a structured 'journal_entry' object. The execution agent will use these fields to generate both a conventional Git commit message and a Markdown journal entry with YAML Frontmatter for machine-readable project history.

**Journal Entry Fields**:
- **type**: Conventional commit type (feat, fix, docs, style, refactor, test, chore)
- **scope**: Functional area affected (e.g., auth, ui, api, database)
- **summary**: Brief description of the change (becomes commit subject)
- **details**: Comprehensive explanation for the project journal

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      }
    }
  }
}
```

---


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /knexfile.js ---

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
const isWSL = process.platform === 'linux' && 
  (process.env.WSL_DISTRO_NAME || 
   fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));

if (isWSL && (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1')) {
  // For WSL, always use the standard WSL2 gateway IP
  process.env.DB_HOST = '172.29.16.1';
}

export default {
  development: {
    client: 'pg',
    connection: {
      host: process.env.DB_HOST || '127.0.0.1',
      port: process.env.DB_PORT || 5432,
      user: process.env.DB_USER || 'myuser',
      password: process.env.DB_PASSWORD || 'mypassword',
      database: process.env.DB_DATABASE || 'eck_snapshot_db',
    },
    pool: {
      min: 2,
      max: 10
    }
  }
};

--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE"
  ],
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0"
  }
}


--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": ["build.gradle", "build.gradle.kts", "settings.gradle", "settings.gradle.kts"],
        "directories": ["app/src/main", "app/src/androidTest"],
        "manifestFiles": ["AndroidManifest.xml"],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": ["package.json"],
        "directories": ["packages", "apps", "libs"],
        "patterns": ["workspaces", "lerna", "nx", "rush"],
        "priority": 7
      },
      "nodejs": {
        "files": ["package.json"],
        "directories": ["node_modules"],
        "priority": 6
      },
      "python-poetry": {
        "files": ["pyproject.toml"],
        "patterns": ["tool.poetry"],
        "priority": 9
      },
      "python-pip": {
        "files": ["requirements.txt", "setup.py", "setup.cfg"],
        "directories": ["__pycache__", "venv", ".venv"],
        "priority": 7
      },
      "python-conda": {
        "files": ["environment.yml", "environment.yaml", "conda.yml"],
        "priority": 8
      },
      "django": {
        "files": ["manage.py"],
        "patterns": ["django", "Django"],
        "priority": 9
      },
      "flask": {
        "files": ["app.py", "application.py"],
        "patterns": ["flask", "Flask"],
        "priority": 8
      },
      "flutter": {
        "files": ["pubspec.yaml"],
        "directories": ["lib", "android", "ios"],
        "priority": 8
      },
      "react-native": {
        "files": ["package.json"],
        "directories": ["android", "ios"],
        "patterns": ["react-native"],
        "priority": 8
      },
      "rust": {
        "files": ["Cargo.toml"],
        "directories": ["src", "target"],
        "priority": 7
      },
      "go": {
        "files": ["go.mod", "go.sum"],
        "directories": ["cmd", "pkg", "internal"],
        "priority": 7
      },
      "dotnet": {
        "files": ["*.csproj", "*.sln", "*.fsproj", "*.vbproj"],
        "directories": ["bin", "obj"],
        "priority": 7
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "smartMode": {
    "tokenThreshold": 7000000,
    "_comment": "Projects over this token count will use vector indexing instead of single file snapshots"
  },
  "vectorIndex": {
    "autoExportOnIndex": true,
    "_comment": "Automatically export the vector index to a file after every successful 'index' command."
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "snapshots/",
      "coverage/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      }
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "Run this command and return the output:\n```\nnode -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"\n```",
      "gitWorkflow": "\n## Git Commit Workflow\n\n**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.\n\n### Commit Process\n1. **Review Changes**: Before committing, briefly summarize what was accomplished\n2. **Stage Files**: Include `git add .` or specific files in your command block\n3. **Create Commit**: Use a clear, descriptive commit message following this format:\n   - Start with the task context (e.g., \"feat:\", \"fix:\", \"docs:\", \"refactor:\")\n   - Include the task_id from your command block for traceability\n   - Keep it under 50 characters for the first line\n   - Add detailed description if needed\n\n### Example Git Commands to Include\n```bash\ngit add .\ngit commit -m \"feat: implement user authentication system\n\nTask ID: auth-system-implementation-1\n- Added login/register endpoints\n- Implemented JWT token validation\n- Added user session management\"\n```\n\n### When to Commit\n- After completing any feature implementation\n- After fixing bugs or issues\n- After refactoring or code improvements\n- After adding tests or documentation\n- Before major architectural changes\n\n**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.\n\n",
      "multiAgent": "\n# AI Instructions\n\n## 1. How to Read This Snapshot\n\nThis document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.\n\n* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.\n* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.\n\n**Snapshot Stats:**\n- **Files Included:** {{stats.includedFiles}}\n- **Total Files in Repo:** {{stats.totalFiles}}\n\n---\n\n## 2. Your Core Operational Workflow\n\nYou are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.\n\n### PROJECT OVERVIEW\n- **Project:** {{repoName}}\n- **Description:** A monorepo POS system with Electron frontend and Node.js backend.\n\n### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nThis project includes a `.eck` directory containing project-specific context and configuration. Always prioritize information from this manifest.\n\n### CRITICAL WORKFLOW: Structured Commits via `journal_entry`\n\nTo ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.\n\n**Your Role (Architect):**\nYour JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.\n\n**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:\n1.  Staging all changes (`git add .`).\n2.  Creating a YAML frontmatter entry for the journal.\n3.  Prepending the entry to `.eck/JOURNAL.md`.\n4.  Executing the conventional Git commit.\n\n**Example `journal_entry` in your payload:**\n```json\n    \"post_execution_steps\": {\n      \"journal_entry\": {\n        \"type\": \"feat\",\n        \"scope\": \"api\",\n        \"summary\": \"Implement user authentication endpoint\",\n        \"details\": \"- Added /login route\\n- Implemented JWT validation\"\n      }\n    }\n```\n\n### Strategic Manifest Files\n\nAs the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.\n\n### CORE WORKFLOW: The Interactive Command Cycle\n1. **Check Environment:** Request ENV scan from agent first\n2. **Analyze User Request:** Understand the user's goal in their native language.\n3. **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.\n4. **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**\n5. **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.\n   - **On Approval:** If the user confirms the plan (e.g., \"yes\", \"proceed\") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.\n   - **On Direct Order:** If the user explicitly asks for the command (e.g., \"make the command for Claude now\") and you have all the necessary information, you may skip step 3 and directly generate the command block.\n6. **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.\n7. **Iterate:** Continue the cycle based on user feedback.\n\n### COMMUNICATION PROTOCOL\n- **User Interaction:** ALWAYS communicate with the user in the language they use.\n- **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.\n- **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.\n\n### AVAILABLE EXECUTION AGENTS\nYou can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:\n\n{{agentDefinitions}}\n\n### COMMAND BLOCK FORMAT\nTo ensure error-free execution, all tasks for agents must be presented in a special block with a \"Copy\" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:\n\n**MANDATORY STRUCTURED LOGGING**: Every command payload MUST include 'post_execution_steps' with a structured 'journal_entry' object. The execution agent will use these fields to generate both a conventional Git commit message and a Markdown journal entry with YAML Frontmatter for machine-readable project history.\n\n**Journal Entry Fields**:\n- **type**: Conventional commit type (feat, fix, docs, style, refactor, test, chore)\n- **scope**: Functional area affected (e.g., auth, ui, api, database)\n- **summary**: Brief description of the change (becomes commit subject)\n- **details**: Comprehensive explanation for the project journal\n\n```json\n{\n  \"target_agent\": \"local_dev\",\n  \"agent_environment\": \"Development environment with full GUI support and development tools\",\n  \"command_for_agent\": \"apply_code_changes\",\n  \"task_id\": \"unique-task-id\",\n  \"payload\": {\n    \"objective\": \"Brief, clear task description\",\n    \"context\": \"Why this change is needed - include relevant .eck manifest context\",\n    \"files_to_modify\": [\n      {\n        \"path\": \"exact/file/path.js\",\n        \"action\": \"specific action (add, modify, replace, delete)\",\n        \"location\": \"line numbers, function name, or search pattern\",\n        \"details\": \"precise description of the change\"\n      }\n    ],\n    \"new_files\": [\n      {\n        \"path\": \"path/to/new/file.js\",\n        \"content_type\": \"javascript/json/markdown/config\",\n        \"purpose\": \"why this file is needed\"\n      }\n    ],\n    \"dependencies\": {\n      \"install\": [\"package-name@version\"],\n      \"remove\": [\"old-package-name\"]\n    },\n    \"validation_steps\": [\n      \"npm run test\",\n      \"node index.js --help\",\n      \"specific command to verify functionality\"\n    ],\n    \"expected_outcome\": \"what should work after changes\",\n    \"post_execution_steps\": {\n      \"journal_entry\": {\n        \"type\": \"feat\",\n        \"scope\": \"authentication\",\n        \"summary\": \"Brief description of what was accomplished\",\n        \"details\": \"Detailed explanation of changes, impacts, and technical notes\"\n      }\n    }\n  }\n}\n```\n\n---\n",
      "vectorMode": "\n# AI Instructions\n\n## 1. How to Read This Snapshot\n\nThis document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: \"{{userQuery}}\"\n\n* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.\n* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.\n\n---\n\n## 2. Your Core Operational Workflow\n\nYou are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.\n\n### PROJECT OVERVIEW\n- **Project:** {{repoName}}\n- **User Query:** \"{{userQuery}}\"\n- **Description:** A monorepo POS system with Electron frontend and Node.js backend.\n\n{{multiAgentSection}}\n\n---\n"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .action(createRepoSnapshot);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });

  program.parse(process.argv);
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function runFileSnapshot(repoPath, options, config, estimation = null, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Creating snapshot of: ${path.basename(repoPath)}`);
  console.log(`üìÅ Repository path: ${repoPath}`);
  
  // Initialize detailed stats with skip tracking
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    // Get all files and setup gitignore
    console.log('üîç Scanning repository...');
    const allFiles = await getProjectFiles(repoPath, config);
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    // Generate directory tree if enabled (config.tree can be overridden by --no-tree flag)
    let directoryTree = '';
    const shouldIncludeTree = config.tree && !options.noTree;
    if (shouldIncludeTree) {
      console.log('üå≥ Generating directory tree...');
      directoryTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth || 10, config);
    }
    
    // Setup progress bar
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    // Helper function to track skipped files
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return `--- File: /${normalizedPath} ---\n\n${content}\n\n`;
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const contentArray = results.filter(Boolean);
    
    // Prepare basic info
    const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
    const repoName = path.basename(repoPath);
    const gitHash = await getGitCommitHash(repoPath);
    
    // Determine if AI header should be included
    // Priority: command line flag (aiHeader) overrides config setting (aiHeaderEnabled)
    // Commander.js converts --no-ai-header to aiHeader: false
    const shouldIncludeAiHeader = config.aiHeaderEnabled && (options.aiHeader !== false);
    
    // Check if this is a Git repository
    const isGitRepo = await checkGitRepository(repoPath);
    
    // Load .eck manifest if it exists
    const eckManifest = await loadProjectEckManifest(repoPath);
    
    // Generate AI header if needed (for both formats)
    let aiHeader = '';
    if (shouldIncludeAiHeader) {
      aiHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest }, isGitRepo);
    }
    
    // Prepare content based on format
    const outputPath = options.output || path.resolve(originalCwd, config.output);
    await fs.mkdir(outputPath, { recursive: true });
    
    let outputContent = '';
    let outputFilename = '';
    let fileExtension = options.format || config.defaultFormat || 'md';
    
    if (fileExtension === 'json') {
      // JSON format - convert Maps to objects for serialization
      const serializableStats = {
        ...stats,
        skipReasons: Object.fromEntries(stats.skipReasons),
        skippedFilesDetails: Object.fromEntries(stats.skippedFilesDetails)
      };
      
      const jsonOutput = {
        metadata: {
          repoName,
          timestamp: new Date().toISOString(),
          toolVersion: '4.0.0',
          format: 'json'
        },
        statistics: serializableStats,
        directoryTree: directoryTree,
        aiInstructionsHeader: aiHeader,
        files: contentArray.map(content => {
          const match = content.match(/--- File: \/(.+) ---\n\n([\s\S]*?)\n\n$/);
          return {
            path: match[1],
            content: match[2]
          };
        })
      };
      outputContent = JSON.stringify(jsonOutput, null, 2);
      outputFilename = gitHash 
        ? `${repoName}_snapshot_${timestamp}_${gitHash}.json`
        : `${repoName}_snapshot_${timestamp}.json`;
    } else {
      // Markdown format (default)
      outputContent = aiHeader;
      
      if (directoryTree) {
        outputContent += '\n## Directory Structure\n\n```\n' + directoryTree + '```\n\n';
      }
      
      outputContent += contentArray.join('');
      outputFilename = gitHash 
        ? `${repoName}_snapshot_${timestamp}_${gitHash}.md`
        : `${repoName}_snapshot_${timestamp}.md`;
    }
    
    const fullOutputFilePath = path.join(outputPath, outputFilename);
    await fs.writeFile(fullOutputFilePath, outputContent);
    
    // Print detailed summary
    console.log('\n‚úÖ Snapshot completed successfully!');
    console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
    console.log(`üìÑ Output file: ${fullOutputFilePath}`);
    console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
    console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
    console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
    console.log(`üìã Format: ${fileExtension.toUpperCase()}`);
    
    if (stats.excludedFiles > 0) {
      console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
    }
    if (stats.binaryFiles > 0) {
      console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
    }
    if (stats.oversizedFiles > 0) {
      console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
    }
    if (stats.ignoredFiles > 0) {
      console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
    }
    if (stats.errors.length > 0) {
      console.log(`‚ùå Errors: ${stats.errors.length}`);
      if (options.verbose) {
        stats.errors.forEach(err => console.log(`   ${err}`));
      }
    }
    
    // Print detailed skip reasons report
    if (stats.skippedFilesDetails.size > 0) {
      console.log('\nüìã Skip Reasons:');
      console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
      
      for (const [reason, files] of stats.skippedFilesDetails.entries()) {
        console.log(`\nüî∏ ${reason} (${files.length} files):`);
        files.forEach(file => {
          console.log(`   ‚Ä¢ ${file}`);
        });
      }
      console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
    } else {
      console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
    }
    
    // Generate training command string if estimation data is available
    if (estimation && projectType) {
      const trainingCommand = generateTrainingCommand(projectType, estimation.estimatedTokens, estimation.totalSize, repoPath);
      console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
      console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
      console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
    }
    
  } finally {
    process.chdir(originalCwd);
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      smartModeTokenThreshold: setupConfig.smartMode.tokenThreshold,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    if (estimation.estimatedTokens > config.smartModeTokenThreshold) {
      spinner.succeed('Project is large. Switching to vector indexing mode.');
      await indexProject(repoPath, options);
    } else {
      spinner.succeed('Project is small. Creating a single-file snapshot.');
      await runFileSnapshot(repoPath, options, config, estimation, projectDetection.type);
    }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}

--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));
    
    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
      console.log('   eck-snapshot index                              # For large projects');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
      console.log('   eck-snapshot index                              # For large projects');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
      console.log('   eck-snapshot index                              # For semantic search');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { loadSetupConfig } from '../../config.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    const config = await loadSetupConfig();
    
    let files = await getProjectFiles(projectPath);
    const profileName = options.profile || 'default';
    if (options.profile) {
        const profile = config.contextProfiles[options.profile];
        if (!profile) throw new Error(`Profile '${options.profile}' not found in setup.json`);
        mainSpinner.info(`Using profile: '${options.profile}'.`);
        files = micromatch(files, profile.include, { ignore: profile.exclude });
    }

    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }
        await releaseAnalysisModel();

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
        await releaseEmbeddingModel();
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}

--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-—Å–Ω–∞–ø—à–æ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/core/segmenter.js ---

import { parse } from '@babel/parser';
import _traverse from '@babel/traverse';
const traverse = _traverse.default;
import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import Parser from 'tree-sitter';
import Python from 'tree-sitter-python';
import Java from 'tree-sitter-java';
import Kotlin from 'tree-sitter-kotlin';

function generateHash(content) {
  return crypto.createHash('sha256').update(content).digest('hex');
}

const tsParser = new Parser();
const languageParsers = {
    '.py': Python,
    '.java': Java,
    '.kt': Kotlin,
};

async function _segmentWithTreeSitter(content, filePath, language) {
    tsParser.setLanguage(language);
    const tree = tsParser.parse(content);
    const chunks = [];
    // Graph relations for tree-sitter are not implemented in this step.
    const relations = [];

    function walk(node) {
        const nodeTypeMap = {
            'function_definition': 'function', 'class_definition': 'class', // Python
            'function_declaration': 'function', 'class_declaration': 'class', // Kotlin/Java
            'method_declaration': 'function', // Java
        };

        if (nodeTypeMap[node.type]) {
            const nameNode = node.childForFieldName('name') || node.child(1);
            const chunkName = nameNode ? nameNode.text : 'anonymous';
            const chunkCode = node.text;
            chunks.push({
                filePath,
                chunk_type: nodeTypeMap[node.type],
                chunk_name: chunkName,
                code: chunkCode,
                contentHash: generateHash(chunkCode)
            });
        }
        node.children.forEach(walk);
    }
    walk(tree.rootNode);
    return { chunks, relations };
}

async function _segmentJavaScript(content, filePath) {
    const chunks = [];
    const relations = [];

    try {
        const ast = parse(content, { sourceType: 'module', plugins: ['typescript', 'jsx'], errorRecovery: true });

        const getChunkData = (node) => {
            const chunkName = node.id ? node.id.name : 'anonymous';
            const chunkCode = content.substring(node.start, node.end);
            return { filePath, chunk_name: chunkName, code: chunkCode, contentHash: generateHash(chunkCode) };
        };

        traverse(ast, {
            enter(path) {
                let currentScopeName = 'file';
                const parentFunction = path.findParent((p) => p.isFunctionDeclaration() || p.isClassDeclaration());
                if (parentFunction && parentFunction.node.id) {
                    currentScopeName = parentFunction.node.id.name;
                }

                if (path.isFunctionDeclaration() || path.isClassDeclaration()) {
                    chunks.push({ ...getChunkData(path.node), chunk_type: path.isClassDeclaration() ? 'class' : 'function' });
                }

                if (path.isImportDeclaration()) {
                    const sourceFile = path.node.source.value;
                    relations.push({ from: filePath, to: sourceFile, type: 'IMPORTS' });
                }

                if (path.isCallExpression()) {
                    const calleeName = path.get('callee').toString();
                    relations.push({ from: currentScopeName, to: calleeName, type: 'CALLS' });
                }
            }
        });
    } catch (e) {
        console.error(`Babel parsing error in ${filePath}: ${e.message}`);
    }
    return { chunks, relations };
}

export async function segmentFile(filePath) {
    try {
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = path.extname(filePath);
        let result = { chunks: [], relations: [] };

        if (['.js', '.jsx', '.ts', '.tsx'].includes(extension)) {
            result = await _segmentJavaScript(content, filePath);
        } else if (languageParsers[extension]) {
            result = await _segmentWithTreeSitter(content, filePath, languageParsers[extension]);
        }
        
        // Fallback: if no specific chunks, treat the whole file as one
        if (result.chunks.length === 0) {
            const code = content;
            result.chunks.push({ filePath, chunk_type: 'file', chunk_name: path.basename(filePath), code, contentHash: generateHash(code) });
        }

        return result;
    } catch (error) {
        console.error(`Failed to segment file ${filePath}: ${error.message}`);
        return { chunks: [], relations: [] };
    }
}

--- File: /src/database/postgresConnector.js ---

import knex from 'knex';
import fs from 'fs/promises';
import path from 'path';
import config from '../../knexfile.js';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));

let knexInstance = null;

function getKnex() {
  if (!knexInstance) {
    console.log('Initializing Knex connection...');
    knexInstance = knex(config.development);
  }
  return knexInstance;
}

async function initDb() {
  const db = getKnex();
  try {
    console.log('Checking database connection...');
    await db.raw('SELECT 1+1 AS result');
    console.log('Connection successful.');

    console.log('Applying database schema...');
    // Try full schema first, fallback to simple schema
    try {
      const schemaPath = path.join(__dirname, 'schema.sql');
      const schemaSQL = await fs.readFile(schemaPath, 'utf-8');
      await db.raw(schemaSQL);
      console.log('Full schema with vector extensions applied successfully.');
    } catch (error) {
      console.log('Vector extensions not available, using simplified schema...');
      const simpleSchemaPath = path.join(__dirname, 'schema_simple.sql');
      const simpleSchemaSQL = await fs.readFile(simpleSchemaPath, 'utf-8');
      await db.raw(simpleSchemaSQL);
      console.log('Simplified schema applied successfully.');
    }
  } catch (error) {
    console.error('Error initializing database:', error.message);
    throw error;
  }
}

async function destroyDb() {
  if (knexInstance) {
    console.log('Destroying Knex connection pool...');
    await knexInstance.destroy();
    knexInstance = null;
  }
}

export {
  getKnex,
  initDb,
  destroyDb,
};

--- File: /src/database/schema.sql ---

-- –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
-- (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ Apache AGE —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ PG)
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç)
SELECT create_graph('eck_snapshot_graph');

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞ (—É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding VECTOR(768), -- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è Jina Code v2
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π (—Ä–µ–±–µ—Ä –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

-- –°–æ–∑–¥–∞–µ–º HNSW-–∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX IF NOT EXISTS code_chunks_embedding_idx ON code_chunks USING HNSW (embedding vector_cosine_ops);

--- File: /src/database/schema_simple.sql ---

-- Simplified schema without vector and graph extensions for testing

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding TEXT, -- JSON string representation for now
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

--- File: /src/services/analysisService.js ---

import { pipeline } from '@xenova/transformers';

class AnalysisService {
    static instance = null;
    static modelName = 'Xenova/distilgpt2'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = await pipeline('text-generation', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = null;
        }
    }
}

export async function getCodeSummary(codeChunk) {
    const generator = await AnalysisService.getInstance();

    const prompt = `This code:\n${codeChunk.substring(0, 150)}\nSummary:`;

    const output = await generator(prompt, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true
    });

    const generatedText = output[0].generated_text;
    const summary = generatedText.replace(prompt, '').trim() || 'Auto-generated description';
    return summary.substring(0, 200); // Limit summary length
}

export const releaseModel = AnalysisService.releaseModel;

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {boolean} continueConversation Whether to continue the last conversation with -c flag.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, continueConversation = false) {
  try {
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }
    
    return await attemptClaudeExecution(prompt, sessionId);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null) {
  const timestamp = new Date().toISOString();
  const logFile = `./logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null) {
  return new Promise((resolve, reject) => {
    
    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }
    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');
    
    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });
    
    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;
    
    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const ACTIVITY_TIMEOUT = 60000; // 1 minute of inactivity allowed
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log('üíÄ No activity detected for 1 minute, killing process');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${ACTIVITY_TIMEOUT/1000} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

--- File: /src/services/embedding.js ---

import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';
import pLimit from 'p-limit';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "embedding-001" });

async function generateEmbedding(text, taskType = 'RETRIEVAL_DOCUMENT') {
  try {
    const result = await model.embedContent({ 
      content: { parts: [{ text }] },
      taskType
    });
    return result.embedding.values;
  } catch (error) {
    console.error('‚ùå Gemini Embedding Error:', error.message);
    throw error;
  }
}

async function generateBatchEmbeddings(segments, taskType = 'RETRIEVAL_DOCUMENT') {
    if (segments.length === 0) return [];

    const BATCH_COUNT_LIMIT = 100;
    const BATCH_SIZE_LIMIT = 3000000; // 3MB for safety
    const allBatches = [];
    let currentBatch = [];
    let currentBatchSize = 0;

    for (const segment of segments) {
        const segmentSize = Buffer.byteLength(segment.content, 'utf8');
        if (segmentSize > BATCH_SIZE_LIMIT) { // Handle single oversized segments
            console.log(chalk.yellow(`  -> Warning: Segment '${segment.name}' in '${segment.filePath}' is oversized and will be truncated.`));
            segment.content = segment.content.substring(0, 20000) + '... [truncated]'; // Truncate oversized segment
        }
        if (currentBatch.length > 0 && (currentBatch.length >= BATCH_COUNT_LIMIT || currentBatchSize + Buffer.byteLength(segment.content, 'utf8') > BATCH_SIZE_LIMIT)) {
            allBatches.push(currentBatch);
            currentBatch = [];
            currentBatchSize = 0;
        }
        currentBatch.push(segment);
        currentBatchSize += Buffer.byteLength(segment.content, 'utf8');
    }
    if (currentBatch.length > 0) {
        allBatches.push(currentBatch);
    }

    console.log(chalk.cyan(`‚è≥ Generating embeddings for ${segments.length} segments, divided into ${allBatches.length} safe chunks...`));

    const limit = pLimit(5); // Set concurrency to 5 parallel requests
    let processedCount = 0;

    const promises = allBatches.map((batch, i) => {
        return limit(async () => {
            const batchSizeKB = (Buffer.byteLength(batch.map(s => s.content).join(''), 'utf8') / 1024).toFixed(2);
            console.log(chalk.blue(`  -> Sending chunk ${i + 1}/${allBatches.length} (${batch.length} segments, ${batchSizeKB} KB)...`));
            try {
                const contents = batch.map(s => ({ parts: [{ text: s.content }] }));
                const result = await model.batchEmbedContents({ 
                    requests: contents.map(content => ({ content, taskType }))
                });
                processedCount++;
                console.log(chalk.green(`  <- Chunk ${i + 1}/${allBatches.length} processed successfully.`));
                return result.embeddings.map(e => e.values);
            } catch (error) {
                console.error(chalk.red(`‚ùå Gemini Batch Embedding Error (Chunk ${i + 1}/${allBatches.length}):`), error.message);
                throw error;
            }
        });
    });

    const chunkResults = await Promise.all(promises);
    const allEmbeddings = chunkResults.flat();

    console.log(chalk.green.bold('‚úÖ Batch embeddings generated successfully.'));
    return allEmbeddings;
}

export const embeddingService = {
  generateEmbedding,
  generateBatchEmbeddings
};

--- File: /src/services/embeddingService.js ---

import { pipeline } from '@xenova/transformers';

class EmbeddingService {
    static instance = null;
    static modelName = 'Xenova/jina-embeddings-v2-base-en'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = await pipeline('feature-extraction', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = null;
            // In Node.js, there's no explicit GPU memory release, 
            // relying on the garbage collector is the standard way.
        }
    }
}

export async function generateEmbedding(code) {
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(code, { pooling: 'mean', normalize: true });
    return Array.from(result.data);
}

export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) return [];
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(texts, { pooling: 'mean', normalize: true });
    
    // Convert tensor to array of arrays
    const embeddings = [];
    for (let i = 0; i < result.dims[0]; i++) {
        const start = i * result.dims[1];
        const end = start + result.dims[1];
        embeddings.push(Array.from(result.data.slice(start, end)));
    }
    return embeddings;
}

export const releaseModel = EmbeddingService.releaseModel;

--- File: /src/services/geminiWebService.js ---

import { chromium } from 'playwright';
import fs from 'fs';
import path from 'path';

/**
 * Interacts with the Gemini web UI to get a response for a given prompt.
 * Uses human-like typing simulation for robust input submission.
 * @param {string} prompt The text to submit to Gemini.
 * @param {object} options Configuration for the browser session.
 * @returns {Promise<string>} The response text from Gemini.
 */
export async function getGeminiResponse(prompt, options = {}) {
  let browser;
  let page;
  
  try {
    console.log('üöÄ Launching Chrome browser...');
    
    // Launch Chrome without using the active profile to avoid conflicts
    try {
      const chromePath = '/mnt/c/Program Files/Google/Chrome/Application/chrome.exe';
      browser = await chromium.launch({
        executablePath: chromePath,
        headless: false,
        args: [
          '--no-sandbox',
          '--disable-setuid-sandbox',
          '--disable-web-security',
          '--disable-dev-shm-usage'
        ]
      });
      console.log('‚úÖ Successfully launched system Chrome');
    } catch (error) {
      console.log('‚ö†Ô∏è Failed to launch system Chrome, using chromium...');
      browser = await chromium.launch({
        headless: false,
        args: ['--no-sandbox', '--disable-setuid-sandbox']
      });
      console.log('‚úÖ Successfully launched chromium');
    }
    
    page = await browser.newPage();

    console.log('üåê Navigating to gemini.google.com...');
    await page.goto('https://gemini.google.com');

    console.log('‚è≥ Waiting for page to load...');
    await page.waitForTimeout(5000); // Brief wait for page load
    
    // Handle any consent dialogs that might appear
    try {
      const acceptButtons = [
        'button:has-text("Accept all")',
        'button:has-text("Alle akzeptieren")',
        'button:has-text("I agree")',
        'button:has-text("Ich stimme zu")'
      ];
      
      for (const selector of acceptButtons) {
        try {
          await page.waitForSelector(selector, { timeout: 3000 });
          await page.click(selector);
          console.log(`‚úÖ Clicked consent button: ${selector}`);
          await page.waitForTimeout(3000);
          break;
        } catch (error) {
          // Continue to next selector
        }
      }
    } catch (error) {
      console.log('‚ÑπÔ∏è No consent dialogs found');
    }

    // Handle login if needed
    console.log('üîê Checking for login form...');
    try {
      // Wait a bit to see if login form appears
      await page.waitForTimeout(5000);
      
      // Check if we're on a login page or need to click sign in first
      const currentUrl = page.url();
      console.log(`üîç Current URL: ${currentUrl}`);
      
      // First, try to find and click "Anmelden" or "Sign in" button if it exists
      const signinButtons = [
        'button:has-text("Anmelden")',
        'button:has-text("Sign in")',
        'a:has-text("Anmelden")',
        'a:has-text("Sign in")'
      ];
      
      for (const selector of signinButtons) {
        try {
          await page.waitForSelector(selector, { timeout: 3000 });
          await page.click(selector);
          console.log(`‚úÖ Clicked initial sign in button: ${selector}`);
          await page.waitForTimeout(3000);
          break;
        } catch (error) {
          // Continue to next selector
        }
      }
      
      // Now check if we're on the actual login form
      if (currentUrl.includes('accounts.google.com') || currentUrl.includes('signin') || page.url().includes('accounts.google.com')) {
        console.log('üìß Login form detected, entering credentials...');
        
        // Enter email
        const emailSelectors = [
          'input[type="email"]',
          'input[name="identifier"]',
          '#identifierId'
        ];
        
        for (const selector of emailSelectors) {
          try {
            await page.waitForSelector(selector, { timeout: 3000 });
            await page.click(selector);
            await page.keyboard.type('mail@xelth.com', { delay: 100 });
            console.log('‚úÖ Entered email');
            
            // Press Enter after email
            await page.keyboard.press('Enter');
            console.log('‚úÖ Pressed Enter after email');
            
            await page.waitForTimeout(3000);
            break;
          } catch (error) {
            // Continue to next selector
          }
        }
        
        // Enter password
        const passwordSelectors = [
          'input[type="password"]',
          'input[name="password"]',
          '#password'
        ];
        
        for (const selector of passwordSelectors) {
          try {
            await page.waitForSelector(selector, { timeout: 5000 });
            await page.click(selector);
            await page.keyboard.type('duckd.,gxMentio529', { delay: 100 });
            console.log('‚úÖ Entered password');
            
            // Press Enter after password
            await page.keyboard.press('Enter');
            console.log('‚úÖ Pressed Enter after password');
            
            await page.waitForTimeout(5000);
            break;
          } catch (error) {
            // Continue to next selector
          }
        }
        
        // Wait for redirect to Gemini
        console.log('‚è≥ Waiting for redirect to Gemini...');
        await page.waitForTimeout(10000);
      }
    } catch (error) {
      console.log('‚ÑπÔ∏è No login form found or already logged in');
    }

    // Additional wait for full page load
    await page.waitForTimeout(5000);
    
    console.log('üîç Looking for input element...');
    
    // Find the input element - try the most common Gemini selector first
    const inputElement = page.locator('div.ql-editor[contenteditable="true"]').first();
    
    try {
      // Wait for the input element to be available
      await inputElement.waitFor({ timeout: 10000 });
      console.log('‚úÖ Found input element');
      
      // Click to focus the input element
      console.log('üéØ Clicking input element to focus...');
      await inputElement.click();
      await page.waitForTimeout(1000);
      
      // Clear any existing text
      await page.keyboard.press('Control+a');
      await page.waitForTimeout(500);
      
      // Type the prompt character by character with human-like delay
      console.log('‚å®Ô∏è Typing prompt character by character...');
      await page.keyboard.type(prompt, { delay: 100 });
      await page.waitForTimeout(2000);
      
      console.log('üöÄ Submitting with Enter key...');
      await page.keyboard.press('Enter');
      
    } catch (error) {
      console.log('‚ö†Ô∏è Standard input element not found, trying alternative approach...');
      
      // Fallback: try clicking anywhere and typing
      await page.click('body');
      await page.waitForTimeout(1000);
      await page.keyboard.type(prompt, { delay: 100 });
      await page.waitForTimeout(2000);
      await page.keyboard.press('Enter');
    }
    
    console.log('‚è≥ Waiting for Gemini response...');
    
    // Wait for response - look for the last response container
    let responseText = '';
    const maxWaitTime = 60000; // 60 seconds - longer wait for response
    const startTime = Date.now();
    
    while (!responseText.trim() && (Date.now() - startTime) < maxWaitTime) {
      await page.waitForTimeout(3000); // Wait 3 seconds between checks
      
      try {
        // Look for response in the most common Gemini response containers
        const responseSelectors = [
          '.response-container .markdown',
          '.markdown',
          '[data-testid*="response"]',
          '.model-response',
          '.assistant-message'
        ];
        
        for (const selector of responseSelectors) {
          try {
            const elements = await page.$$(selector);
            if (elements.length > 0) {
              // Get the last (most recent) response
              const lastElement = elements[elements.length - 1];
              const text = await lastElement.textContent();
              
              if (text && text.trim() && text.length > 10 && !text.includes(prompt)) {
                responseText = text.trim();
                console.log(`‚úÖ Found response using selector: ${selector}`);
                break;
              }
            }
          } catch (error) {
            // Continue to next selector
          }
        }
        
        if (responseText.trim()) break;
        
        // Fallback: get all text and try to find a meaningful response
        const bodyText = await page.textContent('body');
        const lines = bodyText.split('\n').map(line => line.trim()).filter(line => line.length > 0);
        
        // Look for new content that appeared after our prompt
        for (let i = lines.length - 1; i >= 0; i--) {
          const line = lines[i];
          if (line.length > 20 && 
              !line.includes('Enter a prompt') && 
              !line.includes('Send') &&
              !line.includes('Loading') &&
              !line.includes('Generating') &&
              !line.includes('Men√º') &&
              !line.includes('Dokumente') &&
              !line.includes(prompt)) {
            responseText = line;
            console.log('‚úÖ Found response using text analysis');
            break;
          }
        }
        
      } catch (error) {
        console.log('‚ö†Ô∏è Error during response extraction:', error.message);
      }
      
      console.log('üîÑ Still waiting for response...');
    }
    
    if (!responseText.trim()) {
      // Take a screenshot for debugging
      await page.screenshot({ path: 'debug-gemini-response.png' });
      console.log('üì∏ Screenshot saved for debugging');
      throw new Error('Could not extract response from Gemini within timeout period');
    }
    
    console.log('‚úÖ Successfully extracted Gemini response');
    return responseText;

  } catch (error) {
    console.error('‚ùå Error during Gemini automation:', error);
    
    // Save debug information
    try {
      if (browser && page) {
        await page.screenshot({ path: 'logs/gemini-error-screenshot.png' });
        
        const htmlContent = await page.content();
        await fs.promises.writeFile('logs/gemini-error-page.html', htmlContent);
      }
    } catch (debugError) {
      console.error('‚ö†Ô∏è Failed to save debug information:', debugError);
    }
    
    throw error;
  } finally {
    // Don't close browser - keep it open for next questions
    console.log('‚úÖ Keeping browser open for next question...');
  }
}

--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig } from '../config.js';

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Count active agents to determine template
    const activeAgents = Object.values(executionAgents).filter(agent => agent.active);
    const isMultiAgent = activeAgents.length > 1;

    let template;
    if (context.mode === 'vector') {
      template = promptTemplates.vectorMode;
      // For vector mode, build the multi-agent section dynamically
      const multiAgentSection = isMultiAgent ? 
        `### AVAILABLE EXECUTION AGENTS
You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

${buildAgentDefinitions(executionAgents)}

### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for agents must be presented in a special block with a "Copy" button. **IMPORTANT:** You MUST analyze the task and choose the appropriate agent by its ID, then fill in the agent information:

\`\`\`json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed",
    "files_to_modify": [...],
    "new_files": [...],
    "dependencies": {...},
    "validation_steps": [...],
    "expected_outcome": "what should work after changes"
  }
}
\`\`\`

**Agent Selection Guidelines:**
- Choose the agent ID based on task requirements and environment constraints
- Copy the agent's description to "agent_environment" field
- Ensure the task matches the agent's capabilities and restrictions` :
        `### COMMAND BLOCK FORMAT
To ensure error-free execution, all tasks for the agent must be presented in a special block with a "Copy" button:

\`\`\`json
{
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed",
    "files_to_modify": [...],
    "new_files": [...],
    "dependencies": {...},
    "validation_steps": [...],
    "expected_outcome": "what should work after changes"
  }
}
\`\`\``;
      
      template = template.replace('{{multiAgentSection}}', multiAgentSection);
    } else {
      // Always use multiAgent template for file snapshots
      template = promptTemplates.multiAgent;
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions
    };

    let renderedTemplate = render(template, data);
    
    // DELETED: The `multiAgent` template in setup.json now controls the entire manifest and git workflow section.
    // These programmatic insertions are removed to prevent duplicate and conflicting instructions.
    // // Add .eck manifest section if present
    // if (context.eckManifest) { ... }

    // // Add Git workflow instructions if this is a Git repository
    // if (isGitRepo && promptTemplates.gitWorkflow) { ... }

    return renderedTemplate;
  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header');
    return `# Snapshot for ${context.repoName || 'Project'}\n\nGenerated: ${new Date().toISOString()}\n\n---\n\n`;
  }
}

--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { executePrompt } from '../services/claudeCliService.js';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = 'snapshots/';
  const comment = '# Added by eck-snapshot to prevent committing snapshots';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');
  
  try {
    // Check if .eck directory already exists
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory already exists, no need to initialize
        return;
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'CONTEXT.md',
        prompt: "Analyze the current project directory. Write a brief Project Overview for a .eck/CONTEXT.md file, including sections for ## Description, ## Architecture, and ## Key Technologies based on package.json and file structure.",
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: "Analyze the current project directory (especially package.json scripts). Generate a .eck/OPERATIONS.md file listing common commands for ## Development Setup, ## Running the Project, and ## Testing.",
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For CONTEXT and OPERATIONS, try to dynamically generate
      if (file.prompt) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponse = await executePrompt(file.prompt);
          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = aiResponse.replace(/^```(markdown)?\n|```$/g, '').trim();
          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}

--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));
  
  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }
  
  const best = detections[0];
  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;
  
  // Check for required files (faster, check only direct files)
  if (pattern.files) {
    for (const file of pattern.files) {
      const exists = await fileExists(path.join(projectPath, file));
      if (exists) {
        score += 25; // Each required file adds points
      }
    }
  }
  
  // Check for required directories
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const exists = await directoryExists(path.join(projectPath, dir));
      if (exists) {
        score += 20; // Each required directory adds points
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names or in description/keywords
        const foundInDeps = Object.keys(allDeps).some(dep => dep.includes(patternText));
        const foundInMeta = JSON.stringify({
          description: packageJson.description,
          keywords: packageJson.keywords
        }).toLowerCase().includes(patternText.toLowerCase());
        
        if (foundInDeps || foundInMeta) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };
  
  try {
    const cargoPath = path.join(projectPath, 'Cargo.toml');
    if (!await fileExists(cargoPath)) {
      return details;
    }
    
    const content = await fs.readFile(cargoPath, 'utf-8');
    
    const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = content.match(/edition\s*=\s*"([^"]+)"/);
    
    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];
    
    // Check if it's a workspace
    if (content.includes('[workspace]')) {
      details.isWorkspace = true;
    }
    
  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }
  
  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    return JSON.parse(data);
  } catch (error) {
    // If file doesn't exist, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];
  if (points.length < 2) return;
  
  // Simple linear regression for now (bytes -> tokens)
  // We can make this more sophisticated later with higher order polynomials
  
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;
  
  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;
    
    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }
  
  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
  const intercept = (sumY - slope * sumX) / n;
  
  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

