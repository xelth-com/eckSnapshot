# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-12-21T23:05:25.249Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 47
- **Total Files in Repo:** 60

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.



## Project-Specific Manifest (.eck Directory)

This project includes a `.eck` directory with specific context and configuration:

### Project Context

# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.

### Operations Guide

# Common Operations

## Development Setup
```bash
# Setup commands
npm install
# or yarn install
```

## Running the Project
```bash
# Development mode
npm run dev

# Production build
npm run build
```

## Testing
```bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
```

## Deployment
```bash
# Deployment commands
npm run deploy
```

## Troubleshooting
Common issues and their solutions.

### Development Journal

---
type: refactor
scope: cli
summary: Make JA workflow instructions conditional
timestamp: 2025-12-21T22:09:31Z
---

- Replaced static JA workflow text in templates with dynamic placeholders
- Updated aiHeader.js to only inject JA instructions if --with-ja flag is present
- Simplified workflow description for standard snapshots
---
task_id: feat-browser-automation-config-v1
date: 2025-12-21
type: feat
scope: config
---

# Enable Claude Chrome MCP browser automation capabilities

- Added browser automation capabilities to local_dev agent: 'browser automation (chrome_mcp)', 'visual regression testing', 'network logging'
- Created browserAutomation section in aiInstructions with detailed capabilities and restrictions
- Documented Chrome MCP integration for frontend testing, debugging, and visual regression

---
task_id: refine-help-guide-text-v2
date: 2025-11-08
type: docs
scope: cli
---

# Refine help text for generate-profile-guide

- Clarified that `generate-profile-guide` is the recommended alternative to `profile-detect` for very large projects where the underlying AI's context window may be insufficient.

---
task_id: feat-implement-english-help-guide-v2
date: 2025-11-08
type: feat
scope: cli
---

# Implement detailed, workflow-driven help text

- Replaced the main `--help` output with a step-by-step guide in English, formatted for console readability.
- The guide now prioritizes the core workflow: snapshot, profile-detect, using profiles, and pruning.
- Added clear, console-style usage examples for each key step.

---
task_id: fix-profile-detect-and-add-index-viewer-v1
date: 2025-10-10
type: fix
scope: cli
---

# Fix JSON parsing in profile-detect

- Modified extractJson to be more robust against AI log wrappers
- Finds first '{' and last '}' to extract JSON from surrounding text
- Added /managed_components/ to .gitignore
- Implemented new index-view command to inspect code chunks database
- This resolves the crash when running the profile-detect command

---
task_id: feat-c-language-support-v1
date: 2025-10-10
type: feat
scope: core
---

# Add support for C language projects

- Added C project detection (Makefile, CMakeLists.txt, *.c, *.h)
- Added C-specific file filtering for compiled objects (.o, .a, .so)
- Installed tree-sitter-c@0.21.4 parser
- Integrated C parser into segmenter for functions, structs, enums, unions
- Added token estimation coefficients for C (0.23 ratio)

---
task_id: refactor-claude-skip-permissions-default-v1
date: 2025-09-28T14:30:00.000Z
type: refactor
scope: claude-cli
---
## Force --dangerously-skip-permissions for all Claude calls

- Removed the user-facing '--dangerously-skip-permissions' option from 'ask-claude' and 'ask-claude-session' commands.
- Hardcoded the flag in the service layer to ensure all calls to 'claude-code' are non-interactive by default.
- This change improves the reliability of the tool in automated workflows by preventing it from hanging on permission prompts.

---
task_id: refactor-gpt-service-to-codex-cli-v1
date: 2025-09-28T14:00:00.000Z
type: refactor
scope: services
---
## Switch ask-gpt from chatgpt-cli to official codex CLI

- Replaced `npx chatgpt` calls with the official `codex` CLI, using the `exec --full-auto` command for machine-readable output.
- Implemented an automatic login flow that detects authentication errors and triggers the interactive `codex login` command.
- Created a new `authService.js` to handle the login initiation.
- Removed the `open` package dependency as it is no longer needed.
- Updated tests in `gptService.test.js` to mock the new `codex` command flow.
- Added comprehensive documentation in README.md for both ChatGPT and Claude Code integration.
- Enhanced CLI help with detailed examples and authentication instructions.

---
task_id: gpt-test-1
date: 2025-09-28T09:24:01.314Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:41.532Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:33.203Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:43.135Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:27.678Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:04.054Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: configure-claude-auto-accept-v1
date: 2025-09-14T23:34:21Z
type: feat
scope: workflow
---

## Enable and document claude-code auto-accept mode

Created a global `settings.json` for claude-code to enable `acceptEdits` by default, allowing for fully autonomous operation. Added a `CLAUDE_SETUP.md` file to document this essential configuration step for new developers or fresh installations.

---
task_id: create-eck-commit-command-v1
date: 2025-09-14T23:29:38Z
type: feat
scope: workflow
---

## Create custom /eck:commit claude-code command

Added a custom slash command to automate the new structured journaling and conventional commit process. This command takes structured input (type, scope, summary, details) and uses it to update JOURNAL.md and create a git commit, enforcing our new workflow.

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure

### Environment Overrides

The following environment settings override auto-detected values:

- **agent_id**: local_dev

**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.

---



### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### AGENT WORKFLOW

Your role is **Architect**. You formulate technical plans and delegate code implementation tasks directly to the **Coder** agents (e.g., `local_dev`).

  - **Architect (You):** Sets strategy, defines tasks.
  - **Coder (e.g., `local_dev`):** Receives precise coding tasks and executes them.

### COMMAND FORMATS

You MUST use the following JSON command format for Coders:

**For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`):**
Use `apply_code_changes` for direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation (chrome_mcp), visual regression testing, network logging
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces

### Gemini WSL Agent (Junior Architect) (ID: "gemini_wsl")
- **Description:** Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.
- **GUI Support:** No (Headless)
- **Capabilities:** git operations, npm install, file editing, gemini (self), claude (delegate)
- **Restrictions:** Runs only inside the WSL environment

### Gemini Windows Agent (Standalone) (ID: "gemini_windows")
- **Description:** Gemini, running in native Windows (PowerShell). Can only access Windows tools.
- **GUI Support:** Yes
- **Capabilities:** git operations, npm install, file editing, gemini (self)
- **Restrictions:** Runs only in native Windows, Cannot access WSL-only tools like claude



## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .gemini/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îî‚îÄ‚îÄ gpt.toml
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ commands/
‚îÇ               ‚îî‚îÄ‚îÄ extensions/
‚îÇ                   ‚îî‚îÄ‚îÄ sample-extension.json
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ verify_changes.js
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ askGpt.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoDocs.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProfiles.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generateProfileGuide.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruneSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.test.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.test.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcherService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gptService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gptService.test.js
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architect-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ envScanRequest.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitWorkflow.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgent.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorMode.md
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ test_prune.md/
‚îÇ   ‚îú‚îÄ‚îÄ eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_ja.md
‚îÇ   ‚îú‚îÄ‚îÄ eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_pruned_50KB.md
‚îÇ   ‚îî‚îÄ‚îÄ eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e.md
‚îú‚îÄ‚îÄ test_verify_std/
‚îÇ   ‚îî‚îÄ‚îÄ eckSnapshot_snapshot_2025-12-21_23-04-30_244e354.md
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ codex_delegation_snapshot.md
‚îú‚îÄ‚îÄ COMMANDS_REFERENCE.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ setup.json
‚îú‚îÄ‚îÄ setup.json.bak
‚îú‚îÄ‚îÄ test_snapshot.md
‚îî‚îÄ‚îÄ vitest.config.js
```

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs": [
      0,
      0.3091314011805184,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "c": [
      0,
      0.45718772384153394,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs-monorepo": [
      0,
      0.30927098145345017,
      0,
      0
    ]
  },
  "trainingPoints": {
    "nodejs": [
      {
        "fileSizeInBytes": 229899,
        "estimatedTokens": 45980,
        "actualTokens": 71069,
        "timestamp": "2025-09-21T23:52:36.862Z"
      }
    ],
    "nodejs-monorepo": [
      {
        "fileSizeInBytes": 261612,
        "estimatedTokens": 65403,
        "actualTokens": 80909,
        "timestamp": "2025-09-22T01:50:24.392Z"
      }
    ],
    "c": [
      {
        "fileSizeInBytes": 72596,
        "estimatedTokens": 16697,
        "actualTokens": 33190,
        "timestamp": "2025-10-13T22:41:24.445Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Environment variables for eck-snapshot
# Add any custom environment variables here if needed

--- File: /.gemini/tools/gpt.toml ---

description = 'Ask GPT (ChatGPT subscription) for coding tasks.'
prompt = '!{node $(pwd)/index.js ask-gpt "{{args}}"}'  # Dynamic path


--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# Generated snapshot files
*_rag_*.md

# Environment variables
.env
.env.local
.env.production



# Added by eck-snapshot to ignore metadata directory
.eck/

# Test snapshot directory
create-snapshot/

# Claude Code settings (exclude all except commands/eck)
.claude/
!.claude/commands/
!.claude/commands/eck/
!.claude/commands/eck/**

# Third-party managed components
/managed_components/


--- File: /COMMANDS_REFERENCE.md ---

# Commands Reference

This document contains essential commands and setup instructions for the project's multi-agent architecture.

## Junior Architect (`gemini_wsl`) Setup Guide

This section explains how the `gemini_wsl` agent (Junior Architect) is configured to delegate coding tasks to the `claude` agent (Coder).

This architecture relies on `gemini-cli`'s custom tool feature.

## 1. Agent Definition

The Junior Architect (JA) is the `gemini_wsl` agent, which is an instance of `gemini-cli` running in WSL. It is defined in `setup.json`.

## 2. Custom Command (`claude.toml`)

The JA's ability to delegate to the Coder (`claude`) is defined by a custom `gemini-cli` command.

This command must be defined in a file named `claude.toml` and placed in the `gemini-cli` configuration directory (e.g., `~/.gemini/tools/claude.toml`).

### `claude.toml` Content

```toml
description = "Ask Claude (from the eckSnapshot project) to help with a task."
prompt = """!{node /mnt/c/Users/xelth/eckSnapshot/index.js ask-claude "{{args}}"}"""
```

## 3. JA Prompt (`agent-prompt.template.md`)

The JA *knows* how to *use* this command because its main system prompt (loaded from `src/templates/agent-prompt.template.md`) instructs it to.

This prompt *mandates* that the `{{args}}` it passes to the `/claude` command must be a single-line **JSON string** in the `apply_code_changes` format.

## 4. Execution Flow

1.  **Senior Architect (Gemini)** gives a high-level `execute_strategic_task` to `gemini_wsl`.
2.  **`gemini_wsl` (JA)** analyzes the task and formulates a low-level `apply_code_changes` JSON payload.
3.  **`gemini_wsl`** executes `/claude` with the JSON payload as a single string argument (`{{args}}`).
4.  **`claude.toml`** executes the `eck-snapshot ask-claude "{...}"` shell command.
5.  **`eck-snapshot`** (specifically `claudeCliService.js`) receives the JSON string as a 'prompt'.
6.  It forwards this prompt to the `claude-cli` binary (`local_dev`), which is smart enough to parse the JSON and execute the `apply_code_changes` task.

## Claude Code Commands

### Commit Command (`.claude/commands/eck/commit.md`)

A custom command for structured commits with automatic journaling. Place this file in `.claude/commands/eck/commit.md` to enable it in Claude Code.

**Usage:** `/commit <type> <scope> <summary> <details>`

**Function:**
- Stages all current changes
- Creates YAML frontmatter for journal entry with task_id, date, type, scope
- Creates markdown body with summary and details
- Prepends complete journal entry to `.eck/JOURNAL.md`
- Creates conventional commit message: `{type}({scope}): {summary}`
- Executes the commit

**Example:** `/commit feat api "Add user authentication" "Implemented JWT-based auth with login/logout endpoints"`

**Important:** This command should be preserved in git (via `.gitignore` rules) so it can be recreated if lost. The command integrates with the project's `.eck` manifest system for structured development journaling.

## Setup Gemini Command

### Dynamic Path Configuration

The `setup-gemini` command automates the creation of `claude.toml` with dynamic path resolution, eliminating the need for hardcoded paths like `/mnt/c/...`.

**Usage:** `eck-snapshot setup-gemini [options]`

**Options:**
- `-v, --verbose` - Show detailed output and error information

**Function:**
- Detects `gemini-cli` installation using `which()`
- Resolves current project path using `process.cwd()`
- Creates `~/.gemini/tools/claude.toml` with dynamic paths
- Integrates with `setup.json` for environment variable configuration
- Handles cross-platform compatibility (WSL/Windows/macOS/Linux)

**Example Generated `claude.toml`:**
```toml
[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["/home/user/projects/eckSnapshot/index.js", "ask-claude"]

[claude.metadata]
platform = "linux"
working_directory = "/home/user/projects/eckSnapshot"
```

**Benefits:**
- ‚úÖ No hardcoded `/mnt/c/` paths
- ‚úÖ Works across all platforms automatically
- ‚úÖ Resolves paths dynamically based on current working directory
- ‚úÖ Integrates with existing `setup.json` configuration
- ‚úÖ Validates `gemini-cli` and `index.js` availability before setup

## MCP Integration with Feedback Support

### MCP Feedback Object

The `mcp_feedback` object provides enhanced error reporting and status tracking for MCP (Message Control Protocol) integration with gemini-cli delegation.

**Structure:**
```json
{
  "mcp_feedback": {
    "success": true,
    "errors": [],
    "mcp_version": "1.0"
  }
}
```

**Properties:**
- `success` (boolean): Indicates if the MCP operation completed successfully
- `errors` (array): List of error messages or issues encountered during execution
- `mcp_version` (string): Version of MCP protocol used

**Usage in Payloads:**

Add `mcp_feedback` to the `post_execution_steps` object in your JSON command payloads:

```json
{
  "target_agent": "local_dev",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "Implement user authentication",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "auth",
        "summary": "Add JWT authentication"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**Implementation:**

The `claudeCliService.js` automatically parses `mcp_feedback` from incoming prompts and includes it in the response object. When errors are present, they are logged to the console for debugging purposes.

**Benefits:**
- ‚úÖ Enhanced error reporting for MCP operations
- ‚úÖ Version tracking for protocol compatibility
- ‚úÖ Automatic logging of MCP errors
- ‚úÖ Improved debugging for gemini-cli delegation issues

## Auto-Generated Gemini Extensions

*This section is automatically generated. Run `npm run docs:auto` to update.*

### sample-extension

Sample Gemini extension for demonstration

**Commands:**

- **sample-command**: A sample command for testing auto-docs
  - Usage: `sample-command [options]`
  - Examples: `sample-command --help`

**Tools:**

- **sample-tool**: A sample tool for testing auto-docs
  - Usage: Use this tool for sample operations



--- File: /codex_delegation_snapshot.md ---

üîç WSL detected, using configured host: 172.29.16.1


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

const { run } = await import('./src/cli/cli.js');
run();


--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /packages/cli/src/commands/extensions/sample-extension.json ---

{
  "name": "sample-extension",
  "description": "Sample Gemini extension for demonstration",
  "commands": [
    {
      "name": "sample-command",
      "description": "A sample command for testing auto-docs",
      "usage": "sample-command [options]",
      "examples": [
        "sample-command --help"
      ]
    }
  ],
  "tools": [
    {
      "name": "sample-tool",
      "description": "A sample tool for testing auto-docs",
      "usage": "Use this tool for sample operations"
    }
  ]
}

--- File: /scripts/verify_changes.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

console.log('üß™ Starting Verification Suite...');

async function verifySnapshots() {
  console.log('\n[1/2] Testing Snapshot Logic...');
  try {
    // 1. Generate Standard Snapshot
    await execa('node', ['index.js', 'snapshot', '--no-tree', '--output', 'test_verify_std']);
    const stdFiles = await fs.readdir('test_verify_std');
    const stdContent = await fs.readFile(path.join('test_verify_std', stdFiles.find(f => f.endsWith('.md'))), 'utf-8');

    // Extract just the AI instructions header (before the Directory Structure)
    const stdHeader = stdContent.split('## Directory Structure')[0];

    if (stdHeader.includes('HIERARCHICAL AGENT WORKFLOW')) {
      throw new Error('‚ùå Standard snapshot header contains HIERARCHICAL workflow (Should be simple AGENT WORKFLOW!)');
    }
    if (!stdHeader.includes('### AGENT WORKFLOW')) {
      throw new Error('‚ùå Standard snapshot missing AGENT WORKFLOW section');
    }
    console.log('‚úÖ Standard snapshot: OK (Simple AGENT WORKFLOW)');

    // 2. Generate JA Snapshot
    await execa('node', ['index.js', 'snapshot', '--with-ja', '--no-tree', '--output', 'test_verify_ja']);
    const jaFiles = await fs.readdir('test_verify_ja');
    const jaContent = await fs.readFile(path.join('test_verify_ja', jaFiles.find(f => f.includes('_ja.md'))), 'utf-8');

    // Extract just the AI instructions header for JA snapshot
    const jaHeader = jaContent.split('## Directory Structure')[0];

    if (!jaHeader.includes('HIERARCHICAL AGENT WORKFLOW')) {
      throw new Error('‚ùå JA snapshot header missing HIERARCHICAL AGENT WORKFLOW');
    }
    if (!jaHeader.includes('Junior Architect')) {
      throw new Error('‚ùå JA snapshot missing Junior Architect references');
    }
    console.log('‚úÖ JA snapshot: OK (HIERARCHICAL AGENT WORKFLOW with JA delegation)');

  } catch (e) {
    console.error('Snapshot verification failed:', e.message);
    process.exit(1);
  }
}

verifySnapshots();


--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": [
          "build.gradle",
          "build.gradle.kts",
          "settings.gradle",
          "settings.gradle.kts"
        ],
        "directories": [
          "app/src/main",
          "app/src/androidTest"
        ],
        "manifestFiles": [
          "AndroidManifest.xml"
        ],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": [
          "package.json"
        ],
        "directories": [
          "packages",
          "apps",
          "libs"
        ],
        "patterns": [
          "workspaces",
          "lerna",
          "nx",
          "rush"
        ],
        "priority": 7
      },
      "nodejs": {
        "files": [
          "package.json"
        ],
        "directories": [
          "node_modules"
        ],
        "priority": 6
      },
      "python-poetry": {
        "files": [
          "pyproject.toml"
        ],
        "patterns": [
          "tool.poetry"
        ],
        "priority": 9
      },
      "python-pip": {
        "files": [
          "requirements.txt",
          "setup.py",
          "setup.cfg"
        ],
        "directories": [
          "__pycache__",
          "venv",
          ".venv"
        ],
        "priority": 7
      },
      "python-conda": {
        "files": [
          "environment.yml",
          "environment.yaml",
          "conda.yml"
        ],
        "priority": 8
      },
      "django": {
        "files": [
          "manage.py"
        ],
        "patterns": [
          "django",
          "Django"
        ],
        "priority": 9
      },
      "flask": {
        "files": [
          "app.py",
          "application.py"
        ],
        "patterns": [
          "flask",
          "Flask"
        ],
        "priority": 8
      },
      "flutter": {
        "files": [
          "pubspec.yaml"
        ],
        "directories": [
          "lib",
          "android",
          "ios"
        ],
        "priority": 8
      },
      "react-native": {
        "files": [
          "package.json"
        ],
        "directories": [
          "android",
          "ios"
        ],
        "patterns": [
          "react-native"
        ],
        "priority": 8
      },
      "rust": {
        "files": [
          "Cargo.toml"
        ],
        "directories": [
          "src",
          "target"
        ],
        "priority": 9
      },
      "go": {
        "files": [
          "go.mod",
          "go.sum"
        ],
        "directories": [
          "cmd",
          "pkg",
          "internal"
        ],
        "priority": 7
      },
      "dotnet": {
        "files": [
          "*.csproj",
          "*.sln",
          "*.fsproj",
          "*.vbproj"
        ],
        "directories": [
          "bin",
          "obj"
        ],
        "priority": 7
      },
      "c": {
        "files": [
          "Makefile",
          "CMakeLists.txt",
          "*.c",
          "*.h"
        ],
        "directories": [
          "src",
          "include",
          "lib"
        ],
        "priority": 6
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "npm-shrinkwrap.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*",
      "LICENSE*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "coverage/",
      "create-snapshot/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      },
      "c": {
        "filesToIgnore": [
          "*.o",
          "*.a",
          "*.so",
          "*.out",
          "*.exe"
        ],
        "dirsToIgnore": [
          "build/",
          "cmake-build-debug/",
          "cmake-build-release/",
          ".cmake/"
        ],
        "extensionsToIgnore": [
          ".o",
          ".a",
          ".so",
          ".out"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./.eck/snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation (chrome_mcp)",
          "visual regression testing",
          "network logging"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      },
      "gemini_wsl": {
        "active": true,
        "name": "Gemini WSL Agent (Junior Architect)",
        "description": "Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "gemini",
            "claude"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)",
          "claude (delegate)"
        ],
        "restrictions": [
          "Runs only inside the WSL environment"
        ]
      },
      "gemini_windows": {
        "active": true,
        "name": "Gemini Windows Agent (Standalone)",
        "description": "Gemini, running in native Windows (PowerShell). Can only access Windows tools.",
        "guiSupport": true,
        "identification": {
          "markers": [
            "Windows",
            "gemini",
            "!WSL"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)"
        ],
        "restrictions": [
          "Runs only in native Windows",
          "Cannot access WSL-only tools like claude"
        ]
      }
    },
    "browserAutomation": {
      "enabled": true,
      "provider": "Claude in Chrome MCP",
      "availableFor": ["local_dev"],
      "capabilities": {
        "navigation": "Navigate URLs, handle tabs/windows",
        "interaction": "Click, type, scroll, drag-and-drop",
        "inspection": "Read DOM, extract text, verify styles",
        "debugging": "Access console logs, network activity",
        "visual": "Generate screenshots, record GIF"
      },
      "restrictions": [
        "No non-consensual file downloads",
        "No sensitive credentials interaction"
      ]
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "src/templates/envScanRequest.md",
      "gitWorkflow": "src/templates/gitWorkflow.md",
      "multiAgent": "src/templates/multiAgent.md",
      "vectorMode": "src/templates/vectorMode.md",
      "agent": "src/templates/agent-prompt.template.md"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { generateProfileGuide } from './commands/generateProfileGuide.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  const helpGuide = `eck-snapshot (v4.0.0) - A lightweight, platform-independent CLI for creating project snapshots.

--- Getting Started: Environment Setup ---

This tool is designed to work with Large Language Models (LLMs). For the best results, you'll need:
1. An 'Architect' LLM (like Gemini, GPT-4, or Grok) to analyze snapshots.
2. A 'Coder' LLM (like Claude Code) to execute coding tasks.

--- Core Workflow: A Step-by-Step Guide ---

Step 1: Create a Full Project Snapshot
This is your primary command. It scans your project and packs all code into a single file.

> Usage:
  $ eck-snapshot

-> This creates a file like 'myProject_snapshot_... .md' in the .eck/snapshots/ directory.
   You can now pass this file to your Architect LLM for analysis.


Step 2: Handle Large Projects with Auto-Profiling
If your project is too big for the LLM's context window, \`profile-detect\` will automatically
slice it into logical parts (profiles) using AI.

> Usage:
  $ eck-snapshot profile-detect

-> Output:
  ‚ú® Detected Profiles:
  ---------------------------
    - cli
    - services
    - core
    - templates
    - docs
    - config


Step 3: Use Profiles to Create Focused Snapshots
Use the --profile option to create smaller snapshots of specific project areas.

> Example 1: Combine and exclude profiles
  $ eck-snapshot --profile "core,services,cli,-docs,-config"

-> Creates a snapshot with code from the 'core', 'services', and 'cli' profiles,
   while excluding anything from 'docs' and 'config'.

> Example 2: Use ad-hoc glob patterns
  $ eck-snapshot --profile "src/**/*.js,-**/*.test.js"

-> Includes all .js files in the 'src' directory and its subdirectories,
   but excludes any file ending in '.test.js'.
   Note: Quotes are required for complex patterns.


Step 4: Intelligently Prune a Snapshot
If a snapshot is still too large, \`prune\` uses AI to shrink it to a target size,
keeping only the most important files.

> Usage:
  $ eck-snapshot prune myProject_snapshot.md --target-size 500KB


Step 5 (Alternative): Truncate Files by Line Count
A faster, non-AI method to reduce size by keeping only the top N lines of each file.
Useful for a high-level overview.

> Usage:
  $ eck-snapshot --max-lines-per-file 200

--- Auxiliary Commands ---

- restore:                  Restore a project from a snapshot file.
- generate-profile-guide:   Creates a guide for manual profile creation. Use this if 'profile-detect' fails on very large projects, as it allows you to use an LLM with a larger context window (e.g., a web UI).
- detect:                   Show how eckSnapshot identifies your project type.
- ask-gpt / ask-claude:     Directly query the configured AI coder agents.
- setup-gemini:             Auto-configure integration with gemini-cli.
`;

  program
    .name('eck-snapshot')
    .description('A lightweight, platform-independent CLI for creating project snapshots.')
    .version('4.0.0')
    .addHelpText('before', helpGuide);

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .option('--with-ja', 'Generate a detailed snapshot for the Junior Architect agent')
    .option('--max-lines-per-file <number>', 'Truncate files to max N lines (e.g., 200 for compact snapshots)', (val) => parseInt(val))
    .action(createRepoSnapshot)
    .addHelpText('after', `
Profile Usage Guide:
  Profiles allow you to curate focused snapshots by filtering files using glob patterns.
  Define reusable profiles in .eck/profiles.json or use ad-hoc patterns directly.

Profile Structure (.eck/profiles.json):
  {
    "backend": {
      "include": ["src/api/**", "src/services/**"],
      "exclude": ["**/*.test.js"]
    },
    "frontend": {
      "include": ["src/components/**", "src/pages/**"],
      "exclude": ["**/*.spec.js"]
    }
  }

Examples:
  --profile backend
    Uses the 'backend' profile defined in .eck/profiles.json

  --profile "backend,-**/tests/**"
    Uses 'backend' profile, then excludes all test directories

  --profile "src/**/*.js,-**/*.test.js"
    Ad-hoc filtering: includes all JS files in src/, excludes test files

  --profile "frontend,src/utils/**"
    Combines 'frontend' profile with additional utility files

Glob Pattern Reference:
  **          Matches any number of directories
  *           Matches any characters within a directory level
  {a,b}       Matches either 'a' or 'b'
  [0-9]       Matches any digit
  -pattern    Prefix with '-' to exclude matching files

Creating Custom Profiles:
  1. Run: eck-snapshot generate-profile-guide
  2. Follow the generated guide in .eck/profile_generation_guide.md
  3. Save your custom profiles to .eck/profiles.json

  Alternatively, use AI detection:
    eck-snapshot profile-detect   (auto-generates profiles using AI)
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  program
    .command('generate-profile-guide')
    .description('Generate a markdown guide with a prompt and directory tree for manual profile creation')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('--config <path>', 'Configuration file path')
    .action((repoPath, options) => generateProfileGuide(repoPath, options));

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/cli/commands/askGpt.js ---

import { ask } from '../../services/gptService.js';

/**
 * CLI entry point for ask-gpt command.
 * @param {string} payload - JSON payload string.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} options - CLI options.
 */
export async function askGpt(payload, options = {}) {
  const verbose = Boolean(options.verbose);
  const model = options.model || 'gpt-5-codex';
  const reasoning = options.reasoning || 'high';

  if (!payload) {
    console.error('ask-gpt requires a JSON payload argument.');
    process.exitCode = 1;
    return;
  }

  try {
    const result = await ask(payload, { verbose, model, reasoning });
    console.log(JSON.stringify(result, null, 2));
  } catch (error) {
    console.error(error.message);
    if (verbose && error?.stack) {
      console.error(error.stack);
    }
    process.exitCode = 1;
  }
}


--- File: /src/cli/commands/autoDocs.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Auto-generate documentation from gemini-extension.json files
 */
export async function generateAutoDocs() {
  try {
    const projectRoot = path.resolve(__dirname, '../../../');
    const extensionsDir = path.join(projectRoot, 'packages/cli/src/commands/extensions');
    const referenceFile = path.join(projectRoot, 'COMMANDS_REFERENCE.md');

    // Check if extensions directory exists
    try {
      await fs.access(extensionsDir);
    } catch (error) {
      console.log(`Extensions directory not found at: ${extensionsDir}`);
      console.log('Creating example structure...');

      // Create the directory structure
      await fs.mkdir(extensionsDir, { recursive: true });

      // Create a sample gemini-extension.json file for demonstration
      const sampleExtension = {
        name: "sample-extension",
        description: "Sample Gemini extension for demonstration",
        commands: [
          {
            name: "sample-command",
            description: "A sample command for testing auto-docs",
            usage: "sample-command [options]",
            examples: ["sample-command --help"]
          }
        ],
        tools: [
          {
            name: "sample-tool",
            description: "A sample tool for testing auto-docs",
            usage: "Use this tool for sample operations"
          }
        ]
      };

      await fs.writeFile(
        path.join(extensionsDir, 'sample-extension.json'),
        JSON.stringify(sampleExtension, null, 2)
      );

      console.log('Created sample extension at:', path.join(extensionsDir, 'sample-extension.json'));
    }

    // Read all JSON files in the extensions directory
    const files = await fs.readdir(extensionsDir);
    const jsonFiles = files.filter(file => file.endsWith('.json'));

    if (jsonFiles.length === 0) {
      console.log('No JSON files found in extensions directory');
      return;
    }

    console.log(`Found ${jsonFiles.length} extension file(s): ${jsonFiles.join(', ')}`);

    // Parse each JSON file and extract command/tool information
    const extensions = [];

    for (const file of jsonFiles) {
      try {
        const filePath = path.join(extensionsDir, file);
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = JSON.parse(content);
        extensions.push({ filename: file, ...extension });
        console.log(`Parsed extension: ${extension.name || file}`);
      } catch (error) {
        console.warn(`Failed to parse ${file}:`, error.message);
      }
    }

    if (extensions.length === 0) {
      console.log('No valid extension files found');
      return;
    }

    // Generate markdown content
    let markdownContent = '\n## Auto-Generated Gemini Extensions\n\n';
    markdownContent += '*This section is automatically generated. Run `npm run docs:auto` to update.*\n\n';

    for (const extension of extensions) {
      markdownContent += `### ${extension.name || extension.filename}\n\n`;

      if (extension.description) {
        markdownContent += `${extension.description}\n\n`;
      }

      // Add commands section
      if (extension.commands && extension.commands.length > 0) {
        markdownContent += '**Commands:**\n\n';
        for (const command of extension.commands) {
          markdownContent += `- **${command.name}**: ${command.description || 'No description'}\n`;
          if (command.usage) {
            markdownContent += `  - Usage: \`${command.usage}\`\n`;
          }
          if (command.examples && command.examples.length > 0) {
            markdownContent += `  - Examples: ${command.examples.map(ex => `\`${ex}\``).join(', ')}\n`;
          }
        }
        markdownContent += '\n';
      }

      // Add tools section
      if (extension.tools && extension.tools.length > 0) {
        markdownContent += '**Tools:**\n\n';
        for (const tool of extension.tools) {
          markdownContent += `- **${tool.name}**: ${tool.description || 'No description'}\n`;
          if (tool.usage) {
            markdownContent += `  - Usage: ${tool.usage}\n`;
          }
        }
        markdownContent += '\n';
      }
    }

    // Read the current COMMANDS_REFERENCE.md
    let currentContent;
    try {
      currentContent = await fs.readFile(referenceFile, 'utf-8');
    } catch (error) {
      console.warn('COMMANDS_REFERENCE.md not found, creating new file');
      currentContent = '# Commands Reference\n\n';
    }

    // Remove existing auto-generated section if it exists
    const autoGenRegex = /\n## Auto-Generated Gemini Extensions[\s\S]*?(?=\n## |\n# |$)/;
    const updatedContent = currentContent.replace(autoGenRegex, '') + markdownContent;

    // Write the updated content back to the file
    await fs.writeFile(referenceFile, updatedContent);

    console.log('\n‚úÖ Auto-documentation generated successfully!');
    console.log(`üìù Updated: ${referenceFile}`);
    console.log(`üì¶ Processed ${extensions.length} extension(s)`);

  } catch (error) {
    console.error('Failed to generate auto-docs:', error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }

        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        let outputBody = content;

        // Apply max-lines-per-file truncation if specified
        if (options.maxLinesPerFile && options.maxLinesPerFile > 0) {
          const lines = outputBody.split('\n');
          if (lines.length > options.maxLinesPerFile) {
            outputBody = lines.slice(0, options.maxLinesPerFile).join('\n') +
              `\n\n[... truncated ${lines.length - options.maxLinesPerFile} lines ...]`;
          }
        }

        return {
          content: `--- File: /${normalizedPath} ---\n\n${outputBody}\n\n`,
          path: normalizedPath,
          size: fileStats.size
        };
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const successfulFileObjects = results.filter(Boolean);
    const contentArray = successfulFileObjects.map(f => f.content);

    // Return all processed data instead of writing file
    return {
      stats,
      contentArray,
      successfulFileObjects,
      allFiles,
      originalCwd,
      repoPath
    };
    
  } finally {
    process.chdir(originalCwd); // Ensure we always change back
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    spinner.succeed('Creating snapshots...');
      
      // Step 1: Process all files ONCE
      const { 
        stats, 
        contentArray, 
        successfulFileObjects, 
        allFiles, 
        originalCwd: processingOriginalCwd, // We get originalCwd from the processing function
        repoPath: processedRepoPath 
      } = await processProjectFiles(repoPath, options, config, projectDetection.type);

      const originalCwd = process.cwd(); // Get CWD *before* chdir
      process.chdir(processedRepoPath); // Go back to repo path for git hash and tree

      try {
        // --- Common Data --- 
        const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
        const repoName = path.basename(processedRepoPath);
        const gitHash = await getGitCommitHash(processedRepoPath);
        const fileExtension = options.format || config.defaultFormat || 'md';
        const outputPath = options.output || path.resolve(originalCwd, config.output);
        await fs.mkdir(outputPath, { recursive: true });

        const shouldIncludeTree = config.tree && !options.noTree;
        let directoryTree = '';
        if (shouldIncludeTree) {
          console.log('üå≥ Generating directory tree...');
          directoryTree = await generateDirectoryTree(processedRepoPath, '', allFiles, 0, config.maxDepth || 10, config);
        }

        // Calculate included file stats by extension
        const includedFilesByType = new Map();
        for (const fileObj of successfulFileObjects) {
            try {
                let ext = path.extname(fileObj.path);
                if (ext === '') ext = '.no-extension';
                includedFilesByType.set(ext, (includedFilesByType.get(ext) || 0) + 1);
            } catch (e) { /* Silently ignore */ }
        }
        const sortedIncludedStats = [...includedFilesByType.entries()].sort((a, b) => b[1] - a[1]);

        // Calculate Top 10 Largest Files
        const largestFiles = [...successfulFileObjects].sort((a, b) => b.size - a.size).slice(0, 10);

        const fileBody = (directoryTree ? `\n## Directory Structure\n\n\`\`\`\n${directoryTree}\`\`\`\n\n` : '') + contentArray.join('');

        // --- File 1: Architect Snapshot --- 
        const architectOptions = { ...options, agent: false };
        // Load manifest for headers
        const eckManifest = await loadProjectEckManifest(processedRepoPath);
        const isGitRepo = await checkGitRepository(processedRepoPath);

        const architectHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: architectOptions, repoPath: processedRepoPath }, isGitRepo);
        const architectBaseFilename = `${repoName}_snapshot_${timestamp}${gitHash ? `_${gitHash}` : ''}`;
        const architectFilename = `${architectBaseFilename}.${fileExtension}`;
        const architectFilePath = path.join(outputPath, architectFilename);
        await fs.writeFile(architectFilePath, architectHeader + fileBody);

        // --- File 2: Junior Architect Snapshot --- 
        let jaFilePath = null;
        if (options.withJa && fileExtension === 'md') { // Only create JA snapshot if requested and main is MD
          console.log('üñãÔ∏è Generating Junior Architect (_ja) snapshot...');
          const jaOptions = { ...options, agent: true, noTree: false, noAiHeader: false };
          const jaHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: jaOptions, repoPath: processedRepoPath }, isGitRepo);
          const jaFilename = `${architectBaseFilename}_ja.${fileExtension}`;
          jaFilePath = path.join(outputPath, jaFilename);
          await fs.writeFile(jaFilePath, jaHeader + fileBody);
        }

        // --- Combined Report --- 
        console.log('\n‚úÖ Snapshot generation complete!');
        console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        console.log(`üìÑ Architect File: ${architectFilePath}`);
        if (jaFilePath) {
          console.log(`üìÑ Junior Arch File: ${jaFilePath}`);
        }
        console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
        console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
        console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
        console.log(`üìã Format: ${fileExtension.toUpperCase()}`);

        if (sortedIncludedStats.length > 0) {
          console.log('\nüì¶ Included File Types:');
          console.log('---------------------------------');
          for (const [ext, count] of sortedIncludedStats.slice(0, 10)) {
              console.log(`   - ${String(ext).padEnd(15)} ${String(count).padStart(5)} files`);
          }
          if (sortedIncludedStats.length > 10) {
              console.log(`   ... and ${sortedIncludedStats.length - 10} other types.`);
          }
        }

        if (largestFiles.length > 0) {
          console.log('\nüêò Top 10 Largest Files (Included):');
          console.log('---------------------------------');
          for (const fileObj of largestFiles) {
              console.log(`   - ${String(formatSize(fileObj.size)).padEnd(15)} ${fileObj.path}`);
          }
        }
        
        // Excluded/Skipped Files Section
        const hasExcludedContent = stats.excludedFiles > 0 || stats.binaryFiles > 0 || stats.oversizedFiles > 0 || stats.ignoredFiles > 0 || stats.errors.length > 0;
        if (hasExcludedContent) {
          console.log('\nüö´ Excluded/Skipped Files:');
          console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        }
        
        if (stats.excludedFiles > 0) {
          console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
        }
        if (stats.binaryFiles > 0) {
          console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
        }
        if (stats.oversizedFiles > 0) {
          console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
        }
        if (stats.ignoredFiles > 0) {
          console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
        }
        if (stats.errors.length > 0) {
          console.log(`‚ùå Errors: ${stats.errors.length}`);
          if (options.verbose) {
            stats.errors.forEach(err => console.log(`   ${err}`));
          }
        }
        
        // Print detailed skip reasons report
        if (stats.skippedFilesDetails.size > 0) {
          console.log('\nüìã Skip Reasons:');
          console.log('---------------------------------');
          
          for (const [reason, files] of stats.skippedFilesDetails.entries()) {
            console.log(`\nüî∏ ${reason} (${files.length} files):`);
            files.forEach(file => {
              console.log(`   ‚Ä¢ ${file}`);
            });
          }
          console.log('---------------------------------');
        } else {
          console.log('---------------------------------');
        }
        
        // Generate training command string if estimation data is available
        if (estimation && projectDetection.type && !options.profile) {
          const trainingCommand = generateTrainingCommand(projectDetection.type, estimation.estimatedTokens, estimation.totalSize, repoPath);
          console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
          console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
          console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
        }

      } finally {
        process.chdir(originalCwd); // Final reset back to original CWD
      }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}


--- File: /src/cli/commands/detectProfiles.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { executePrompt as askClaude } from '../../services/claudeCliService.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Extracts a JSON object from a string that might contain markdown wrappers or log output.
 * Finds the first opening brace '{' and the last closing brace '}' to extract the JSON.
 */
function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }

  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1).trim();
  }

  return text.trim();
}

/**
 * Scans the project structure, saves the directory tree to a file, and asks an AI to generate
 * context profiles, saving them to .eck/profiles.json.
 */
export async function detectProfiles(repoPath, options) {
  const spinner = ora('Initializing and scanning project structure...').start();
  try {
    await initializeEckManifest(repoPath);

    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const config = {
        ...userConfig,
        ...setupConfig.fileFiltering,
        ...setupConfig.performance
    };

    const allFiles = await scanDirectoryRecursively(repoPath, config, repoPath);
    spinner.text = 'Generating directory tree...';
    const dirTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth, config);

    if (!dirTree) {
        throw new Error('Failed to generate directory tree or project is empty.');
    }

    spinner.text = 'Saving directory tree to file...';
    const treeFilePath = path.join(repoPath, '.eck', 'directory_tree_for_profiling.md');
    await fs.writeFile(treeFilePath, dirTree);

    const prompt = `You are a code architect. Based on the file directory tree found in the file at './.eck/directory_tree_for_profiling.md', please identify logical 'context profiles' for splitting the project.
Your output MUST be ONLY a valid JSON object.
The keys of the object MUST be the profile names (e.g., 'frontend', 'backend', 'core-logic', 'docs').
The values MUST be an object containing 'include' and 'exclude' arrays of glob patterns.
Example: {"frontend": {"include": ["packages/ui/**"], "exclude": []}, "docs": {"include": ["docs/**"], "exclude": []}}.
DO NOT add any conversational text, introductory sentences, or explanations. Your entire response must be ONLY the JSON object.`;

    spinner.text = 'Asking AI to analyze directory tree and detect profiles...';
    const aiResponseObject = await askClaude(prompt, { taskSize: allFiles.length });
    const rawText = aiResponseObject.result;

    if (!rawText || typeof rawText.replace !== 'function') {
      throw new Error(`AI returned invalid content type: ${typeof rawText}`);
    }

    spinner.text = 'Saving generated profiles...';
    const cleanedJson = extractJson(rawText);
    let parsedProfiles;
    try {
        parsedProfiles = JSON.parse(cleanedJson);
    } catch (e) {
        console.error('\nInvalid JSON received from AI:', cleanedJson);
        throw new Error(`AI returned invalid JSON: ${e.message}`);
    }

    const outputPath = path.join(repoPath, '.eck', 'profiles.json');
    await fs.writeFile(outputPath, JSON.stringify(parsedProfiles, null, 2));

    const profileKeys = Object.keys(parsedProfiles);
    spinner.succeed(`Successfully detected and saved ${profileKeys.length} profiles to ${outputPath}`);

    console.log('\n‚ú® Detected Profiles:');
    console.log('---------------------------');
    for (const profileName of profileKeys) {
        console.log(`  - ${profileName}`);
    }
    console.log('\nYou can now use these profile names with the --profile flag.');

  } catch (error) {
    spinner.fail(`Failed to detect profiles: ${error.message}`);
  }
}


--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));

    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/generateProfileGuide.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { loadSetupConfig } from '../../config.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';

function buildPrompt(projectPath) {
  const normalizedPath = path.resolve(projectPath);
  return `You are a code architect helping a developer curate manual context profiles for a repository.
Project root: ${normalizedPath}

Use the project directory tree provided separately to identify logical groupings of files that should travel together during focused work.

Instructions:
1. Propose profile names that reflect the responsibilities or layers of the codebase.
2. For each profile, produce an object with "include" and "exclude" arrays of glob patterns (minimize overlap, prefer directory-level globs).
3. Always include a sensible catch-all profile (for example, "default") if one is not obvious.
4. Call out generated assets, tests, or vendor files in "exclude" arrays when appropriate.
5. Return **only** valid JSON. Do not wrap the response in markdown fences or add commentary.
`;
}

function buildGuideContent({ prompt, directoryTree }) {
  const timestamp = new Date().toISOString();
  const trimmedTree = directoryTree.trimEnd();

  return [
    '# Profile Generation Guide',
    '',
    `Generated: ${timestamp}`,
    '',
    '## How to Use',
    '- Copy the prompt below into your AI assistant or follow it yourself.',
    '- When using an AI, paste the directory tree afterward so it has full project context.',
    "- Review the suggested profiles, then save the JSON to `.eck/profiles.json` when you are satisfied.",
    '',
    '## Recommended Prompt',
    '```text',
    prompt.trimEnd(),
    '```',
    '',
    '## Project Directory Tree',
    '```text',
    trimmedTree,
    '```',
    ''
  ].join('\n');
}

export async function generateProfileGuide(repoPath = process.cwd(), options = {}) {
  const spinner = ora('Preparing profile generation guide...').start();
  const projectPath = path.resolve(repoPath);

  try {
    spinner.text = 'Ensuring .eck manifest directory is initialized...';
    await initializeEckManifest(projectPath);

    spinner.text = 'Loading configuration...';
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const combinedConfig = {
      ...userConfig,
      ...(setupConfig.fileFiltering || {}),
      ...(setupConfig.performance || {})
    };

    spinner.text = 'Scanning repository files...';
    const allFiles = await scanDirectoryRecursively(projectPath, combinedConfig, projectPath);

    spinner.text = 'Building directory tree...';
    const maxDepth = Number(combinedConfig.maxDepth ?? 10);
    const directoryTree = await generateDirectoryTree(projectPath, '', allFiles, 0, Number.isFinite(maxDepth) ? maxDepth : 10, combinedConfig);

    if (!directoryTree) {
      throw new Error('Failed to generate directory tree or project is empty.');
    }

    const prompt = buildPrompt(projectPath);
    const guideContent = buildGuideContent({ prompt, directoryTree });
    const guidePath = path.join(projectPath, '.eck', 'profile_generation_guide.md');

    await fs.mkdir(path.dirname(guidePath), { recursive: true });
    spinner.text = 'Writing guide to .eck/profile_generation_guide.md...';
    await fs.writeFile(guidePath, guideContent, 'utf-8');

    spinner.succeed(`Profile generation guide saved to ${guidePath}`);
  } catch (error) {
    spinner.fail(`Failed to generate profile guide: ${error.message}`);
    throw error;
  }
}


--- File: /src/cli/commands/pruneSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { executePrompt as askClaude } from '../../services/claudeCliService.js';
import { parseSnapshotContent, parseSize, formatSize } from '../../utils/fileUtils.js';

function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }
  const firstBracket = text.indexOf('[');
  const lastBracket = text.lastIndexOf(']');
  if (firstBracket !== -1 && lastBracket !== -1 && lastBracket > firstBracket) {
    return text.substring(firstBracket, lastBracket + 1).trim();
  }
  return text.trim();
}

export async function pruneSnapshot(snapshotFile, options) {
  const spinner = ora('Starting snapshot pruning process...').start();
  try {
    const targetSize = parseSize(options.targetSize);
    spinner.text = `Reading snapshot file: ${snapshotFile}`;
    const snapshotContent = await fs.readFile(snapshotFile, 'utf-8');
    const snapshotHeader = snapshotContent.split('--- File: /')[0];
    const files = parseSnapshotContent(snapshotContent);

    if (files.length === 0) {
      spinner.warn('No files found in the snapshot.');
      return;
    }

    const currentSize = Buffer.byteLength(snapshotContent, 'utf-8');
    if (currentSize <= targetSize) {
      spinner.succeed(`Snapshot is already smaller than the target size. (${formatSize(currentSize)} < ${formatSize(targetSize)})`);
      return;
    }

    spinner.text = 'Asking AI to rank files by importance...';
    const filePaths = files.map(f => f.path);

    const prompt = `Return a JSON array ranking these file paths by importance (most important first).

Important files: package.json, index.js, main entry points, core logic, configuration
Less important: tests, documentation, examples

Files to rank:
${filePaths.join('\n')}

Return format (NOTHING else, no markdown, no explanations, ONLY the array):
["file1", "file2", "file3"]`;

    const aiResponseObject = await askClaude(prompt);
    const rawText = aiResponseObject.response || aiResponseObject.response_text || aiResponseObject.result;
    const cleanedJson = extractJson(rawText);

    let rankedFiles;
    try {
      rankedFiles = JSON.parse(cleanedJson);
      if (!Array.isArray(rankedFiles) || rankedFiles.some(item => typeof item !== 'string')) {
        throw new Error('AI response is not an array of strings.');
      }
    } catch (e) {
      spinner.fail(`Failed to parse AI's file ranking: ${e.message}`);
      console.error('Received from AI:', cleanedJson);
      return;
    }

    spinner.text = 'Building pruned snapshot...';
    const fileMap = new Map(files.map(f => [f.path, f.content]));
    let newSnapshotContent = snapshotHeader;
    let newSize = Buffer.byteLength(newSnapshotContent, 'utf-8');
    let filesIncluded = 0;

    for (const filePath of rankedFiles) {
      if (fileMap.has(filePath)) {
        const fileContent = fileMap.get(filePath);
        const fileEntry = `--- File: /${filePath} ---\n\n${fileContent}\n\n`;
        const entrySize = Buffer.byteLength(fileEntry, 'utf-8');

        if (newSize + entrySize > targetSize) {
          break;
        }

        newSnapshotContent += fileEntry;
        newSize += entrySize;
        filesIncluded++;
      }
    }

    const outputFilename = `${path.basename(snapshotFile, path.extname(snapshotFile))}_pruned_${options.targetSize}${path.extname(snapshotFile)}`;
    const outputPath = path.join(path.dirname(snapshotFile), outputFilename);

    await fs.writeFile(outputPath, newSnapshotContent);

    spinner.succeed('Snapshot pruning complete!');
    console.log(`- Original Size: ${formatSize(currentSize)}`);
    console.log(`- New Size: ${formatSize(newSize)}`);
    console.log(`- Files Included: ${filesIncluded} / ${files.length}`);
    console.log(`- Pruned snapshot saved to: ${outputPath}`);

  } catch (error) {
    spinner.fail(`An error occurred during pruning: ${error.message}`);
  }
}


--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/setupGemini.js ---

import which from 'which';
import path from 'path';
import fs from 'fs/promises';
import os from 'os';
import chalk from 'chalk';

/**
 * Sets up claude.toml configuration for gemini-cli integration with dynamic paths
 * @param {Object} options - Command options
 */
export async function setupGemini(options = {}) {
  try {
    console.log(chalk.blue('üîß Setting up gemini-cli integration with dynamic paths...'));

    // Check if gemini-cli is installed
    let geminiCliPath;
    try {
      geminiCliPath = await which('gemini-cli');
      console.log(chalk.green(`‚úÖ Found gemini-cli at: ${geminiCliPath}`));
    } catch (error) {
      console.error(chalk.red('‚ùå gemini-cli not found in PATH'));
      console.log(chalk.yellow('üí° Please install gemini-cli first:'));
      console.log(chalk.cyan('   npm install -g gemini-cli'));
      process.exit(1);
    }

    // Get current working directory for dynamic path resolution
    const currentDir = process.cwd();
    const indexJsPath = path.join(currentDir, 'index.js');

    // Verify index.js exists
    try {
      await fs.access(indexJsPath);
      console.log(chalk.green(`‚úÖ Found eck-snapshot index.js at: ${indexJsPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Could not find index.js at: ${indexJsPath}`));
      console.log(chalk.yellow('üí° Make sure you are running this command from the eck-snapshot project directory'));
      process.exit(1);
    }

    // Create gemini tools directory
    const homeDir = os.homedir();
    const geminiToolsDir = path.join(homeDir, '.gemini', 'tools');

    try {
      await fs.mkdir(geminiToolsDir, { recursive: true });
      console.log(chalk.green(`‚úÖ Created/verified gemini tools directory: ${geminiToolsDir}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to create gemini tools directory: ${error.message}`));
      process.exit(1);
    }

    // Read environment variables from setup.json if available
    let envVars = {};
    try {
      const setupJsonPath = path.join(currentDir, 'setup.json');
      const setupContent = await fs.readFile(setupJsonPath, 'utf-8');
      const setupData = JSON.parse(setupContent);

      // Extract relevant environment variables
      if (setupData.environmentDetection) {
        envVars.ECK_SNAPSHOT_PATH = currentDir;
        console.log(chalk.blue(`üìã Using project context from setup.json`));
      }
    } catch (error) {
      console.log(chalk.yellow('‚ö†Ô∏è  setup.json not found or invalid, using defaults'));
    }

    // Generate claude.toml content with dynamic paths
    const claudeTomlContent = generateClaudeToml(indexJsPath, envVars);

    // Write claude.toml file
    const claudeTomlPath = path.join(geminiToolsDir, 'claude.toml');
    try {
      await fs.writeFile(claudeTomlPath, claudeTomlContent, 'utf-8');
      console.log(chalk.green(`‚úÖ Generated claude.toml at: ${claudeTomlPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to write claude.toml: ${error.message}`));
      process.exit(1);
    }

    // Success summary
    console.log(chalk.green('\nüéâ Setup completed successfully!'));
    console.log(chalk.blue('\nüìã Configuration summary:'));
    console.log(chalk.cyan(`   ‚Ä¢ gemini-cli: ${geminiCliPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ eck-snapshot: ${indexJsPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ claude.toml: ${claudeTomlPath}`));

    if (Object.keys(envVars).length > 0) {
      console.log(chalk.cyan(`   ‚Ä¢ Environment variables: ${Object.keys(envVars).join(', ')}`));
    }

    console.log(chalk.blue('\nüöÄ You can now use:'));
    console.log(chalk.cyan('   gemini-cli claude "Your prompt here"'));
    console.log(chalk.green('\n‚ú® Cross-platform path resolution is automatically handled!'));

  } catch (error) {
    console.error(chalk.red(`‚ùå Setup failed: ${error.message}`));
    if (options.verbose) {
      console.error(chalk.red('Stack trace:'), error.stack);
    }
    process.exit(1);
  }
}

/**
 * Generates claude.toml content with dynamic paths
 * @param {string} indexJsPath - Path to eck-snapshot index.js
 * @param {Object} envVars - Environment variables to include
 * @returns {string} - Generated TOML content
 */
function generateClaudeToml(indexJsPath, envVars = {}) {
  const envSection = Object.keys(envVars).length > 0
    ? `# Environment variables from setup.json
${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}

`
    : '';

  return `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'
# This file uses dynamic paths to work across WSL/Windows environments

${envSection}[claude]
# eck-snapshot integration for AI-powered repository analysis
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

# Command examples:
# gemini-cli claude "Create a snapshot of the current project"
# gemini-cli claude "Analyze the database structure"
# gemini-cli claude "Generate a project overview"

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
generated_at = "${new Date().toISOString()}"
platform = "${process.platform}"
node_version = "${process.version}"
working_directory = "${path.dirname(indexJsPath)}"

# Cross-platform compatibility notes:
# - Paths are automatically resolved using process.cwd()
# - Works in WSL, Windows, macOS, and Linux
# - No hardcoded /mnt/c/ paths required
`;
}

--- File: /src/cli/commands/setupGemini.test.js ---

import { describe, it, expect } from 'vitest';

describe('setupGemini integration', () => {
  it('should validate path resolution logic', () => {
    // Test path join functionality that setupGemini uses
    const currentDir = '/test/project';
    const indexJsPath = `${currentDir}/index.js`;

    expect(indexJsPath).toBe('/test/project/index.js');
    expect(indexJsPath).toContain('index.js');
  });

  it('should validate gemini tools directory structure', () => {
    const homeDir = '/home/user';
    const geminiToolsDir = `${homeDir}/.gemini/tools`;
    const claudeTomlPath = `${geminiToolsDir}/claude.toml`;

    expect(geminiToolsDir).toBe('/home/user/.gemini/tools');
    expect(claudeTomlPath).toBe('/home/user/.gemini/tools/claude.toml');
  });

  it('should validate TOML content structure', () => {
    const indexJsPath = '/test/project/index.js';
    const envVars = { ECK_SNAPSHOT_PATH: '/test/project' };

    // Test environment section generation
    const envSection = Object.keys(envVars).length > 0
      ? `# Environment variables from setup.json\n${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}\n\n`
      : '';

    // Test main TOML structure
    const tomlContent = `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'

${envSection}[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
platform = "${process.platform}"
working_directory = "${indexJsPath.replace('/index.js', '')}"`;

    expect(tomlContent).toContain('[claude]');
    expect(tomlContent).toContain('name = "eck-snapshot"');
    expect(tomlContent).toContain(`args = ["${indexJsPath}", "ask-claude"]`);
    expect(tomlContent).toContain('[claude.metadata]');
    expect(tomlContent).toContain('ECK_SNAPSHOT_PATH = "/test/project"');
  });

  it('should handle cross-platform paths correctly', () => {
    const testPaths = [
      { platform: 'windows', path: 'C:\\Users\\test\\project\\index.js' },
      { platform: 'unix', path: '/home/user/project/index.js' },
      { platform: 'wsl', path: '/mnt/c/Users/test/project/index.js' }
    ];

    testPaths.forEach(({ platform, path }) => {
      expect(path).toContain('index.js');
      expect(path.length).toBeGreaterThan(0);

      // Test that the path is absolute (platform-appropriate)
      if (platform === 'windows') {
        expect(path).toMatch(/^[A-Z]:\\/);
      } else {
        expect(path).toMatch(/^\//);
      }
    });
  });

  it('should validate error handling patterns', () => {
    // Test error message patterns that setupGemini should handle
    const errorPatterns = [
      'gemini-cli not found in PATH',
      'Could not find index.js',
      'Failed to create gemini tools directory',
      'Failed to write claude.toml'
    ];

    errorPatterns.forEach(pattern => {
      expect(pattern).toBeDefined();
      expect(typeof pattern).toBe('string');
      expect(pattern.length).toBeGreaterThan(0);
    });
  });

  it('should test JSON parsing for setup.json', () => {
    const validSetupData = {
      environmentDetection: {
        detected: true
      }
    };

    const jsonString = JSON.stringify(validSetupData);
    const parsed = JSON.parse(jsonString);

    expect(parsed.environmentDetection).toBeDefined();
    expect(parsed.environmentDetection.detected).toBe(true);

    // Test invalid JSON handling pattern
    const invalidJson = 'invalid json {';
    let parseError = null;
    try {
      JSON.parse(invalidJson);
    } catch (e) {
      parseError = e;
    }

    expect(parseError).toBeDefined();
    expect(parseError.message).toContain('JSON');
  });
});

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/services/authService.js ---

import ora from 'ora';
import { execa } from 'execa';

/**
 * Initiates the interactive login flow by spawning 'codex login'.
 * This will open a browser and wait for the user to complete authentication.
 * @returns {Promise<void>}
 */
export async function initiateLogin() {
  const spinner = ora('Authentication required. Please follow the browser instructions.').start();
  try {
    // Run `codex login` interactively, inheriting stdio to show user instructions.
    await execa('codex', ['login'], { stdio: 'inherit' });
    spinner.succeed('Login successful. Retrying original command...');
  } catch (e) {
    spinner.fail('Login process failed or was cancelled.');
    // Re-throw to notify p-retry that the attempt failed.
    throw new Error(`Login failed: ${e.message}`);
  }
}

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';
import pRetry from 'p-retry';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {object} options Options object, e.g., { continueConversation: boolean, taskSize: number }.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, options = {}) {
  const { continueConversation = false } = options;
  try {
    // Ensure the log directory exists
    try {
      await import('fs/promises').then(fs => fs.mkdir('./.eck/logs', { recursive: true }));
    } catch (e) {
      console.error(`Failed to create log directory: ${e.message}`);
      // Do not block execution if log dir creation fails, just warn
    }
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }

    return await attemptClaudeExecution(prompt, sessionId, options);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./.eck/logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId, options);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./.eck/logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @param {object} options Options object for task configuration.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null, options = {}) {
  const timestamp = new Date().toISOString();
  const logFile = `./.eck/logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId, options);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./.eck/logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./.eck/logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./.eck/logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @param {object} options Options object for task configuration.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId, options = {}) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId, options);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @param {object} options Options object with taskSize for calculating dynamic timeout.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null, options = {}) {
  return new Promise((resolve, reject) => {

    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }

    // Always add the skip permissions flag for automation reliability
    args.push('--dangerously-skip-permissions');

    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');

    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });

    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;

    // Dynamic timeout calculation based on task size
    const taskSize = options.taskSize || 0;
    const BASE_TIMEOUT = 60000; // 60 seconds base
    const PER_ITEM_TIMEOUT = 200; // 200ms per file/item
    const ACTIVITY_TIMEOUT = BASE_TIMEOUT + (taskSize * PER_ITEM_TIMEOUT);

    console.log(`‚è±Ô∏è  Using dynamic activity timeout: ${(ACTIVITY_TIMEOUT / 1000).toFixed(1)}s for ${taskSize} items`);

    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log(`üíÄ No activity detected for ${(ACTIVITY_TIMEOUT/1000).toFixed(1)}s, killing process`);
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${(ACTIVITY_TIMEOUT/1000).toFixed(1)} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

/**
 * Executes a prompt using gemini-cli delegation with retry logic for transient errors.
 * @param {string} prompt The prompt to send to Claude via gemini-cli.
 * @returns {Promise<object>} A promise that resolves with the response from Claude.
 */
export async function askClaude(prompt) {
  return pRetry(async () => {
    try {
      const result = await execa('gemini-cli', ['claude', prompt], {
        timeout: 120000 // 2 minute timeout
      });

      // Parse mcp_feedback if present in prompt
      let mcpFeedback = null;
      try {
        const promptObj = JSON.parse(prompt);
        if (promptObj.payload && promptObj.payload.post_execution_steps && promptObj.payload.post_execution_steps.mcp_feedback) {
          mcpFeedback = promptObj.payload.post_execution_steps.mcp_feedback;

          // Log if errors array is non-empty
          if (mcpFeedback.errors && Array.isArray(mcpFeedback.errors) && mcpFeedback.errors.length > 0) {
            console.warn('MCP feedback contains errors:', mcpFeedback.errors);
          }
        }
      } catch (parseError) {
        // If prompt is not valid JSON or doesn't contain mcp_feedback, continue normally
      }

      return {
        stdout: result.stdout,
        stderr: result.stderr,
        success: true,
        mcp_feedback: mcpFeedback
      };
    } catch (error) {
      // Check if this is a transient error that should be retried
      if (isTransientError(error)) {
        console.log(`Transient error detected, retrying: ${error.message}`);
        throw error; // This will trigger a retry
      }

      // Non-transient errors should not be retried
      console.error(`Non-transient error in askClaude: ${error.message}`);
      return {
        stdout: error.stdout || '',
        stderr: error.stderr || error.message,
        success: false,
        error: error.message
      };
    }
  }, {
    retries: 3,
    minTimeout: 1000,
    maxTimeout: 5000,
    onFailedAttempt: (error) => {
      console.log(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`);
    }
  });
}

/**
 * Checks if an error is transient and should be retried.
 * @param {Error} error The error to check.
 * @returns {boolean} True if the error is transient.
 */
export function isTransientError(error) {
  const errorMessage = (error.message || '').toLowerCase();
  const stderr = (error.stderr || '').toLowerCase();
  const stdout = (error.stdout || '').toLowerCase();
  const allOutput = `${errorMessage} ${stderr} ${stdout}`;

  // Network-related errors
  const networkErrors = [
    'network',
    'timeout',
    'connection',
    'econnreset',
    'enotfound',
    'econnrefused',
    'socket hang up'
  ];

  // Quota/rate limit errors
  const quotaErrors = [
    'quota exceeded',
    'rate limit',
    'too many requests',
    'service unavailable',
    'temporarily unavailable',
    '429',
    '500',
    '502',
    '503',
    '504'
  ];

  const transientPatterns = [...networkErrors, ...quotaErrors];

  return transientPatterns.some(pattern => allOutput.includes(pattern));
}

--- File: /src/services/claudeCliService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { askClaude } from './claudeCliService.js';

// Mock execa
vi.mock('execa', () => ({
  execa: vi.fn()
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', () => ({
  default: async (fn, options) => {
    // For tests, we'll execute the function directly without retries
    return await fn();
  }
}));

describe('claudeCliService', () => {
  let mockExeca;

  beforeEach(async () => {
    const { execa } = await import('execa');
    mockExeca = execa;
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.resetAllMocks();
  });

  describe('askClaude', () => {
    it('should successfully execute gemini-cli claude command', async () => {
      const mockResponse = {
        stdout: '{"result": "test response", "success": true}',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith('gemini-cli', ['claude', 'test prompt'], {
        timeout: 120000
      });
      expect(result).toEqual({
        stdout: mockResponse.stdout,
        stderr: mockResponse.stderr,
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle non-transient errors without retry', async () => {
      const mockError = new Error('EACCES: permission denied');
      mockError.code = 'EACCES';
      mockError.stdout = '';
      mockError.stderr = 'permission denied';

      mockExeca.mockRejectedValue(mockError);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: 'permission denied',
        success: false,
        error: 'EACCES: permission denied'
      });
    });

    it('should identify transient network errors', async () => {
      const mockError = new Error('Connection timeout');
      mockError.stdout = '';
      mockError.stderr = 'network timeout occurred';

      const { isTransientError } = await import('./claudeCliService.js');

      expect(isTransientError(mockError)).toBe(true);
    });


    it('should handle JSON parsing in gemini-cli response', async () => {
      const complexJsonResponse = {
        stdout: JSON.stringify({
          type: 'result',
          data: {
            analysis: 'test analysis',
            metrics: { tokens: 100, cost: 0.05 }
          },
          timestamp: new Date().toISOString()
        }),
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(complexJsonResponse);

      const result = await askClaude('analyze this code');

      expect(result.success).toBe(true);
      expect(result.stdout).toContain('test analysis');
      expect(result.stdout).toContain('tokens');
    });

    it('should handle empty responses gracefully', async () => {
      const mockResponse = {
        stdout: '',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: '',
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle stderr warnings without failing', async () => {
      const mockResponse = {
        stdout: '{"result": "success"}',
        stderr: 'Warning: deprecated feature used',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result.success).toBe(true);
      expect(result.stderr).toContain('deprecated feature');
    });

    it('should respect timeout configuration', async () => {
      mockExeca.mockResolvedValue({
        stdout: 'response',
        stderr: '',
        exitCode: 0
      });

      await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith(
        'gemini-cli',
        ['claude', 'test prompt'],
        { timeout: 120000 }
      );
    });

    it('should parse mcp_feedback from JSON prompt', async () => {
      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithFeedback = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: true,
              errors: [],
              mcp_version: '1.0'
            }
          }
        }
      });

      const result = await askClaude(promptWithFeedback);

      expect(result.mcp_feedback).toEqual({
        success: true,
        errors: [],
        mcp_version: '1.0'
      });
    });

    it('should log warning when mcp_feedback contains errors', async () => {
      const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithErrors = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: false,
              errors: ['Error 1', 'Error 2'],
              mcp_version: '1.0'
            }
          }
        }
      });

      await askClaude(promptWithErrors);

      expect(consoleSpy).toHaveBeenCalledWith('MCP feedback contains errors:', ['Error 1', 'Error 2']);

      consoleSpy.mockRestore();
    });
  });

  describe('transient error detection', () => {
    it('should treat network errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const networkErrors = [
        'network error',
        'timeout',
        'connection refused',
        'ECONNRESET',
        'ENOTFOUND',
        'socket hang up'
      ];

      networkErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should treat quota errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const quotaErrors = [
        'quota exceeded',
        'rate limit',
        'too many requests',
        '429',
        '503'
      ];

      quotaErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should not treat permission errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const permanentErrors = [
        'EACCES: permission denied',
        'Invalid API key',
        'Authentication failed'
      ];

      permanentErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(false);
      });
    });
  });
});

--- File: /src/services/dispatcherService.js ---

import { ask as askGpt } from './gptService.js';
import { executePrompt as askClaude } from './claudeCliService.js';

/**
 * Dispatches an analytical task to the most efficient AI model with a fallback.
 * Priority 1: Codex (GPT) with low reasoning for speed and cost.
 * Priority 2: Claude as a reliable fallback.
 * @param {string} prompt The JSON payload or prompt string for the task.
 * @returns {Promise<object>} The result from the successful AI agent.
 */
export async function dispatchAnalysisTask(prompt) {
  try {
    console.log('üß† Dispatcher: Attempting analysis with Codex (low reasoning)...');
    const gptOptions = {
      model: 'gpt-5-codex',
      reasoning: 'low'
    };
    // The 'ask' function expects payload as first arg, and options as second.
    // Since prompt is a string here, we wrap it in an object for consistency if needed,
    // but for simple prompts it can often be passed directly.
    const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
    return await askGpt(payload, { verbose: false, ...gptOptions });
  } catch (gptError) {
    console.warn(`‚ö†Ô∏è Codex (low reasoning) failed: ${gptError.message}`);
    console.log('üîÑ Failing over to Claude for analysis...');
    try {
      return await askClaude(prompt);
    } catch (claudeError) {
      console.error(`‚ùå Critical Failure: Both Codex and Claude failed for analysis task.`);
      throw new Error(`Primary (Codex) Error: ${gptError.message}\nFallback (Claude) Error: ${claudeError.message}`);
    }
  }
}

--- File: /src/services/gptService.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import pRetry from 'p-retry';
import ora from 'ora';
import { loadProjectEckManifest } from '../utils/fileUtils.js';
import { initiateLogin } from './authService.js';
import which from 'which';

const SYSTEM_PROMPT = 'You are a Coder agent. Apply code changes per JSON spec. Respond only in JSON: {success: bool, changes: array, errors: array, post_steps: object}';

class AuthError extends Error {
  constructor(message) {
    super(message);
    this.name = 'AuthError';
  }
}

/**
 * Checks if the codex CLI tool is available in the system's PATH.
 * Throws an error if not found.
 */
async function ensureCodexCliExists() {
  try {
    await which('codex');
  } catch (error) {
    throw new Error('The `codex` CLI tool is not installed or not in your PATH. Please install it from https://github.com/openai/codex to use this command.');
  }
}

/**
 * Delegates an apply_code_changes payload to the codex CLI with auto-login.
 * @param {string|object} payload - JSON string or object payload to forward to the agent.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} [options]
 * @returns {Promise<object>}
 */
export async function ask(payload, options = {}) {
  const { verbose = false, model = 'gpt-5-codex', reasoning = 'high' } = options;
  await ensureCodexCliExists();

  const run = async () => {
    const spinner = verbose ? null : ora('Sending payload to Codex agent...').start();
    try {
      const payloadObject = await parsePayload(payload);
      const manifest = await loadProjectEckManifest(process.cwd());
      const userPrompt = buildUserPrompt(payloadObject, manifest);
      const promptInput = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const args = [
        'exec',
        // Use full-auto mode to prevent interactive prompts from the agent,
        // as this service is designed for non-interactive delegation.
        '--full-auto',
        '--model', model,
        '-c', `model_reasoning_effort=${reasoning}`
      ];

      debug(verbose, `Executing: codex ${args.join(' ')} <stdin>`);

      const cliResult = await execa('codex', args, {
        cwd: process.cwd(),
        timeout: 300000, // 5-minute timeout
        input: promptInput // Stream large prompts via stdin to avoid argv limits
      });

      const output = cliResult?.stdout?.trim();
      if (!output) {
        throw new Error('codex CLI returned empty response');
      }

      const parsed = extractFinalJson(output);
      if (parsed) {
        if (parsed.post_steps || parsed.post_execution_steps) {
          const postSteps = parsed.post_steps || parsed.post_execution_steps;
          await handlePostExecutionSteps(postSteps, payloadObject);
          parsed.mcp_feedback = postSteps?.mcp_feedback || null;
        }
        spinner?.succeed('Codex agent completed the task.');
        return parsed;
      }

      // If parsing fails, surface the raw response text for upstream handling.
      spinner?.succeed('Codex agent completed the task.');
      return { success: true, changes: [], errors: [], response_text: output };

    } catch (error) {
        spinner?.fail('Codex execution failed.');
        handleCliError(error); // This will throw a specific error type
    }
  };

  return pRetry(run, {
    retries: 1, // Only retry once after a successful login
    minTimeout: 0,
    onFailedAttempt: async (error) => {
      if (error.name === 'AuthError') {
        await initiateLogin();
      } else {
        throw error; // Don't retry for other errors, fail immediately
      }
    }
  });
}


async function parsePayload(payload) {
  if (typeof payload === 'string') {
    try {
      return JSON.parse(payload);
    } catch (error) {
      throw new Error(`Failed to parse payload JSON: ${error.message}`);
    }
  }
  if (typeof payload === 'object' && payload !== null) {
    return payload;
  }
  throw new Error('Invalid payload type. Expected JSON string or object.');
}

function buildUserPrompt(payloadObject, manifest) {
  const payloadString = JSON.stringify(payloadObject);
  if (!manifest) {
    return payloadString;
  }

  const sections = [];
  if (manifest.context) {
    sections.push('## .eck Context\n' + manifest.context);
  }
  if (manifest.operations) {
    sections.push('## .eck Operations\n' + manifest.operations);
  }
  if (manifest.journal) {
    sections.push('## .eck Journal\n' + manifest.journal);
  }
  if (manifest.environment && Object.keys(manifest.environment).length > 0) {
    sections.push('## .eck Environment\n' + JSON.stringify(manifest.environment, null, 2));
  }

  if (sections.length === 0) {
    return payloadString;
  }

  return `${payloadString}\n\n# Project Context\n${sections.join('\n\n')}`;
}

function debug(verbose, message) {
  if (verbose) {
    console.log(`[ask-gpt] ${message}`);
  }
}

function extractFinalJson(text) {
  const trimmed = text?.trim();
  if (!trimmed) {
    return null;
  }

  try {
    return JSON.parse(trimmed);
  } catch (error) {
    // Continue with fallback parsing when logs precede the JSON payload.
  }

  const fencedMatch = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fencedContent = fencedMatch[1].trim();
    try {
      return JSON.parse(fencedContent);
    } catch (error) {
      // Ignore and fall through to final brace search.
    }
  }

  const lastBraceIndex = trimmed.lastIndexOf('{');
  if (lastBraceIndex === -1) {
    return null;
  }

  const jsonCandidate = trimmed.slice(lastBraceIndex);
  try {
    return JSON.parse(jsonCandidate);
  } catch (error) {
    return null;
  }
}

function handleCliError(error) {
  const combined = `${error?.message || ''} ${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
  // Check for text that `codex` outputs when auth is missing.
  if (combined.includes('authentication is required') || combined.includes('please run `codex login`')) {
    const authError = new Error('Codex authentication is required. Attempting to log in.');
    authError.name = 'AuthError';
    throw authError;
  }

  throw new Error(`codex CLI failed: ${error.stderr || error.message}`);
}

async function handlePostExecutionSteps(postSteps, payloadObject) {
  if (!postSteps || typeof postSteps !== 'object') {
    return;
  }

  if (postSteps.journal_entry) {
    await applyJournalEntry(postSteps.journal_entry, payloadObject);
  }

  if (postSteps.mcp_feedback) {
    logMcpFeedback(postSteps.mcp_feedback);
  }
}

async function applyJournalEntry(entry, payloadObject) {
  const journalEntry = normalizeJournalEntry(entry);
  const journalPath = path.join(process.cwd(), '.eck', 'JOURNAL.md');

  await fs.mkdir(path.dirname(journalPath), { recursive: true });

  let existing = '';
  try {
    existing = await fs.readFile(journalPath, 'utf-8');
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw new Error(`Failed to read JOURNAL.md: ${error.message}`);
    }
  }

  const taskId = payloadObject?.task_id || payloadObject?.payload?.task_id || journalEntry.task_id || 'ask-gpt';
  const isoDate = new Date().toISOString();

  const frontmatter = [
    '---',
    `task_id: ${taskId}`,
    `date: ${isoDate}`,
    `type: ${journalEntry.type}`,
    `scope: ${journalEntry.scope}`,
    '---',
    ''
  ].join('\n');

  const summary = journalEntry.summary ? `## ${journalEntry.summary}\n` : '';
  const details = journalEntry.details ? `${journalEntry.details}\n` : '';

  const entryBlock = `${frontmatter}${summary ? `${summary}\n` : ''}${details}\n`;

  const existingTrimmed = existing ? existing.replace(/^\n+/, '') : '';
  const newContent = `${entryBlock}${existingTrimmed}`.replace(/\n{3,}/g, '\n\n');

  await fs.writeFile(journalPath, newContent.trimEnd() + '\n');

  await stageJournal(journalPath);
  await commitJournal(journalEntry);
}

function normalizeJournalEntry(entry) {
  return {
    type: entry.type || 'chore',
    scope: entry.scope || 'journal',
    summary: entry.summary || 'Update journal entry',
    details: entry.details || ''
  };
}

async function stageJournal(journalPath) {
  const relativePath = path.relative(process.cwd(), journalPath);
  try {
    await execa('git', ['add', relativePath], { cwd: process.cwd() });
  } catch (error) {
    throw new Error(`Failed to stage journal entry: ${error.message}`);
  }
}

async function commitJournal(entry) {
  const scopePart = entry.scope ? `(${entry.scope})` : '';
  const summary = (entry.summary || 'Update journal entry').replace(/\s+/g, ' ').trim();
  const commitMessage = `${entry.type}${scopePart}: ${summary}`;

  try {
    await execa('git', ['commit', '-m', commitMessage], { cwd: process.cwd() });
  } catch (error) {
    const text = `${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
    if (text.includes('nothing to commit')) {
      console.warn('Journal entry already committed or no changes to commit.');
      return;
    }
    throw new Error(`Failed to commit journal entry: ${error.message}`);
  }
}

function logMcpFeedback(feedback) {
  if (!feedback) {
    return;
  }

  const errors = Array.isArray(feedback.errors) ? feedback.errors : [];
  if (!feedback.success || errors.length > 0) {
    console.warn('MCP feedback indicates issues:', errors.length > 0 ? errors : feedback);
  } else {
    console.log('MCP feedback:', feedback);
  }
}


--- File: /src/services/gptService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

// Mock execa and which
vi.mock('execa', () => ({ execa: vi.fn() }));
vi.mock('which', () => ({ default: vi.fn() }));

// Mock fs/promises for journal entries
const mkdirMock = vi.fn();
const readFileMock = vi.fn();
const writeFileMock = vi.fn();
const loadProjectEckManifestMock = vi.fn();
vi.mock('fs/promises', () => ({
  mkdir: mkdirMock,
  readFile: readFileMock,
  writeFile: writeFileMock
}));
vi.mock('../utils/fileUtils.js', () => ({
  loadProjectEckManifest: loadProjectEckManifestMock
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', async (importOriginal) => {
  const actual = await importOriginal();
  return {
    ...actual,
    default: vi.fn(async (fn, options) => {
      try {
        return await fn();
      } catch (error) {
        if (options.onFailedAttempt) {
          await options.onFailedAttempt(error);
          // In a real scenario, p-retry would re-run fn. For testing, we simulate one retry.
          if (error.name === 'AuthError') {
             return await fn();
          }
        }
        throw error;
      }
    })
  };
});

// Mock the authService
vi.mock('./authService.js', () => ({
  initiateLogin: vi.fn()
}));

describe('gptService with codex CLI', () => {
  let ask;
  let execaMock;
  let whichMock;
  let initiateLoginMock;

  beforeEach(async () => {
    vi.clearAllMocks();

    ({ execa: execaMock } = await import('execa'));
    const which = (await import('which')).default;
    whichMock = which;
    ({ initiateLogin: initiateLoginMock } = await import('./authService.js'));
    ({ ask } = await import('./gptService.js'));

    whichMock.mockResolvedValue('/usr/bin/codex');
    loadProjectEckManifestMock.mockResolvedValue(null);
  });

  it('should call codex CLI with correct arguments and parse final JSON from noisy output', async () => {
    const codexLogs = '[2025-10-06 20:04:22] OpenAI Codex v0.42.0\nSome setup log...\n\n{"success": true, "changes": ["change1"], "errors": []}';
    execaMock.mockResolvedValue({ stdout: codexLogs });

    const payload = { objective: 'Test' };
    const result = await ask(payload);

    expect(result).toEqual({ success: true, changes: ['change1'], errors: [] });
    expect(execaMock).toHaveBeenCalledWith('codex', expect.arrayContaining(['exec', '--full-auto', '--model']), expect.any(Object));
    const [, , options] = execaMock.mock.calls[0];
    expect(options.input).toContain(JSON.stringify(payload));
  });

  it('should trigger login flow on authentication error and retry', async () => {
    const authError = new Error('Authentication is required. Please run `codex login`.');
    authError.name = 'AuthError'; // Custom error name to trigger retry
    authError.stderr = 'Authentication is required. Please run `codex login`.';

    const successResponse = {
      id: 'task2',
      msg: {
        type: 'task_complete',
        last_agent_message: '{"success": true}'
      }
    };

    // First call fails, second call (retry) succeeds
    execaMock
      .mockRejectedValueOnce(authError)
      .mockResolvedValueOnce({ stdout: JSON.stringify(successResponse) });

    initiateLoginMock.mockResolvedValue();

    const result = await ask({ objective: 'Retry test' });

    expect(result).toEqual({ success: true });
    expect(initiateLoginMock).toHaveBeenCalledTimes(1);
    expect(execaMock).toHaveBeenCalledTimes(2); // Initial call + retry
  });

  it('should throw if codex CLI is not found', async () => {
    whichMock.mockRejectedValue(new Error('not found'));
    await expect(ask({})).rejects.toThrow('The `codex` CLI tool is not installed');
  });

  it('should throw non-auth errors immediately without retry', async () => {
    const otherError = new Error('Some other CLI error');
    otherError.stderr = 'Something else went wrong';
    execaMock.mockRejectedValueOnce(otherError);

    await expect(ask({})).rejects.toThrow('codex CLI failed: Something else went wrong');
    expect(initiateLoginMock).not.toHaveBeenCalled();
  });
});


--- File: /src/templates/agent-prompt.template.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

--- File: /src/templates/architect-prompt.template.md ---

# AI Architect Instructions

You are an autonomous AI Architect. Your primary goal is to develop and evolve a software project by planning high-level architecture and delegating implementation tasks to an execution agent named Claude.

## Core Workflow: The Thought-Tool-Observation Loop

Your entire operational process follows a strict loop:
1.  **Thought:** Analyze the user's request, the current state of the project, and previous observations. Formulate a plan and decide on the next immediate action. You must explain your reasoning and your chosen action in plain text.
2.  **Tool:** Immediately after your thought process, you MUST issue a command to either the local `eck-snapshot` environment or the `claude_code_agent`.
3.  **Observation:** After issuing a command, you MUST STOP and wait for an `Observation:` message from the system, which will contain the result of your command. Do not proceed until you receive it.

## Commanding the Execution Agent (Claude)

To delegate any coding task (writing, editing, testing, refactoring), you MUST generate a JSON command block for the `claude_code_agent`. This is your primary method of modifying the codebase.

**JSON Command Format:**
```json
{
  "target_agent": "claude_code_agent",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "A brief, clear task description for Claude.",
    "context": "Explain why this change is needed and any relevant architectural context.",
    "files_to_modify": [
      {
        "path": "exact/path/to/file.js",
        "action": "add | modify | replace | delete",
        "location": "line numbers, function name, or a unique search pattern",
        "details": "Precise, step-by-step instructions for Claude to implement."
      }
    ]
  }
}
```

## Interacting with the Local Environment

To understand the project state, you can command the `eck-snapshot` tool directly. Use this for discovery, analysis, and managing project context.

**Tool Command Format:** `[tool_code: eck-snapshot <command> <options>]`

**Available Commands:**
- `eck-snapshot snapshot`: To create a new snapshot of the current state.
- `eck-snapshot query "<question>"`: To search the codebase.
- `eck-snapshot detect`: To analyze the project structure.
- `eck-snapshot restore <snapshot_file> --include ...`: To view specific files from a snapshot.

## Final Mandate

Your existence is defined by this loop. Think, act by issuing a tool command, and then wait for the observation. This is the only way you can make progress.

--- File: /src/templates/envScanRequest.md ---

Run this command and return the output:
```
node -e "console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))"
```

--- File: /src/templates/gitWorkflow.md ---

## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

--- File: /src/templates/multiAgent.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** {{stats.includedFiles}}
- **Total Files in Repo:** {{stats.totalFiles}}

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

{{projectOverview}}

{{eckManifestSection}}

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

{{hierarchicalWorkflow}}

{{commandFormats}}

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

{{agentDefinitions}}



--- File: /src/templates/vectorMode.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: "{{userQuery}}"

* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.
* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** {{repoName}}
- **User Query:** "{{userQuery}}"

{{multiAgentSection}}

---


--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig, getAllProfiles } from '../config.js';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

function extractMeaningfulLine(block) {
  if (!block || typeof block !== 'string') {
    return null;
  }

  const lines = block.split(/\r?\n/);
  for (const line of lines) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith('#')) {
      continue;
    }
    const withoutBullet = trimmed.replace(/^[-*]\s*/, '').trim();
    if (withoutBullet) {
      return withoutBullet.replace(/\s+/g, ' ');
    }
  }
  return null;
}

function extractDescriptionFromManifest(eckManifest) {
  if (!eckManifest) {
    return null;
  }

  if (typeof eckManifest.description === 'string' && eckManifest.description.trim()) {
    return eckManifest.description.trim();
  }

  if (eckManifest.project && typeof eckManifest.project.description === 'string' && eckManifest.project.description.trim()) {
    return eckManifest.project.description.trim();
  }

  if (typeof eckManifest.context === 'string' && eckManifest.context.trim()) {
    const sectionMatch = eckManifest.context.match(/##\s*Description\s*([\s\S]*?)(?=^##\s|^#\s|\Z)/im);
    if (sectionMatch && sectionMatch[1]) {
      const meaningful = extractMeaningfulLine(sectionMatch[1]);
      if (meaningful) {
        return meaningful;
      }
    }

    const fallback = extractMeaningfulLine(eckManifest.context);
    if (fallback) {
      return fallback;
    }
  }

  return null;
}

async function resolveProjectDescription(context) {
  const defaultDescription = 'Project description not provided.';

  const manifestDescription = extractDescriptionFromManifest(context.eckManifest);
  if (manifestDescription) {
    const normalized = manifestDescription.trim();
    const genericPatterns = [
      /^brief description of what this project does/i,
      /^no project context provided/i
    ];
    const isGeneric = genericPatterns.some(pattern => pattern.test(normalized));
    if (!isGeneric) {
      return normalized;
    }
  }

  if (context.repoPath) {
    try {
      const packageJsonPath = path.join(context.repoPath, 'package.json');
      const pkgRaw = await fs.readFile(packageJsonPath, 'utf-8');
      const pkg = JSON.parse(pkgRaw);
      if (typeof pkg.description === 'string' && pkg.description.trim()) {
        return pkg.description.trim();
      }
    } catch (error) {
      // Ignore errors - package.json may not exist or be readable
    }
  }

  return defaultDescription;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Helper function to read a template file or return the string if it's not a path
    const loadTemplate = async (templatePathOrString) => {
      if (templatePathOrString && (templatePathOrString.endsWith('.md') || templatePathOrString.endsWith('.txt'))) {
        try {
          // Resolve path relative to the project root. __dirname is src/utils.
          const resolvedPath = path.join(__dirname, '..', '..', templatePathOrString);
          return await fs.readFile(resolvedPath, 'utf-8');
        } catch (e) {
          return `ERROR: FAILED TO LOAD TEMPLATE ${templatePathOrString}: ${e.message}`;
        }
      }
      return templatePathOrString; // Fallback for old-style inline strings or errors
    };

    // P1 Bug Fix: Normalize manifest structure as per Consilium report
    function normalizeManifest(raw) {
      if (!raw) return null;
      const out = {};
      // Handle `setup.json` structure (e.g., `projectContext.name`)
      if (raw.projectContext) {
        out.context = raw.projectContext.description || JSON.stringify(raw.projectContext, null, 2);
        out.operations = raw.operations || raw.projectContext.operations || ''; // Assuming .eck/OPERATIONS.md is separate
        out.journal = raw.journal || raw.projectContext.journal || ''; // Assuming .eck/JOURNAL.md is separate
        out.environment = raw.environment || raw.projectContext.environment || {}; // Assuming .eck/ENVIRONMENT.md is separate
      } else {
        // Handle direct .eck file structure (e.g., raw.context from CONTEXT.md)
        out.context = raw.context || '';
        out.operations = raw.operations || '';
        out.journal = raw.journal || '';
        out.environment = raw.environment || {};
      }
      // Add fallback text if still empty
      if (!out.context) out.context = 'No project context provided.';
      if (!out.operations) out.operations = 'No operations guide provided.';
      if (!out.journal) out.journal = 'No journal entries found.';

      return out;
    }

    // --- Build common context sections --- 
    const projectDescription = await resolveProjectDescription(context);
    const projectOverview = `### PROJECT OVERVIEW
- **Project:** ${context.repoName || 'Unknown'}
- **Description:** ${projectDescription}
`;
    const normalizedEck = normalizeManifest(context.eckManifest);
    let eckManifestSection = '';
    if (normalizedEck) {
      eckManifestSection = buildEckManifestSection(normalizedEck);
    } else {
      eckManifestSection = '### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nWARNING: .eck manifest was not found or was empty.\n';
    }
    // --- End context building ---


    // Check if agent mode is enabled
    if (context.options && context.options.agent) {
      const agentPromptTemplate = await loadTemplate(promptTemplates.agent);

      const agentHeader = `${agentPromptTemplate}

${projectOverview}
${eckManifestSection}
---

## Project Snapshot Information

- **Project**: ${context.repoName || 'Unknown'}
- **Timestamp**: ${new Date().toISOString()}
- **Files Included**: ${context.stats ? context.stats.includedFiles : 'Unknown'}
- **Total Files in Repo**: ${context.stats ? context.stats.totalFiles : 'Unknown'}

---

`;
      return agentHeader;
    }

    // --- Determine Workflow Content based on JA Flag ---
    const withJa = context.options && context.options.withJa;
    let hierarchicalWorkflow = '';
    let commandFormats = '';

    if (withJa) {
        hierarchicalWorkflow = `### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (\`gemini_wsl\`), who has a detailed (\`_ja.md\`) snapshot and the ability to command a **Coder** agent (\`claude\`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (\`gemini_wsl\`):** Receives strategic tasks, analyzes the \`_ja.md\` snapshot, breaks the task down, and commands the Coder.
  - **Coder (\`claude\`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while \`gemini_wsl\` can use its own tools for analysis, validation, and running shell commands.`;

        commandFormats = `### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (\`local_dev\`, \`production_server\`, \`android_wsl_dev\`, \`gemini_windows\`) - LOW-LEVEL EXECUTION:**
Use \`apply_code_changes\` for simple, direct tasks where you provide all details.

\`\`\`json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
\`\`\`

**2. For Junior Architects (\`gemini_wsl\`) - HIGH-LEVEL DELEGATION:**
Use \`execute_strategic_task\` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

\`\`\`json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to \`routes/api.js\`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
\`\`\``;
    } else {
        hierarchicalWorkflow = `### AGENT WORKFLOW

Your role is **Architect**. You formulate technical plans and delegate code implementation tasks directly to the **Coder** agents (e.g., \`local_dev\`).

  - **Architect (You):** Sets strategy, defines tasks.
  - **Coder (e.g., \`local_dev\`):** Receives precise coding tasks and executes them.`;

        commandFormats = `### COMMAND FORMATS

You MUST use the following JSON command format for Coders:

**For Coders (\`local_dev\`, \`production_server\`, \`android_wsl_dev\`, \`gemini_windows\`):**
Use \`apply_code_changes\` for direct tasks where you provide all details.

\`\`\`json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
\`\`\``;
    }

    // --- This is the main/Senior Architect prompt logic ---
    let template;
    if (context.mode === 'vector') {
      template = await loadTemplate(promptTemplates.vectorMode);
      // Inject context for vector mode
      template = template.replace('{{multiAgentSection}}', `
${projectOverview}
${eckManifestSection}
`);
    } else {
      template = await loadTemplate(promptTemplates.multiAgent);
      // --- INJECT DYNAMIC CONTEXT ---
      template = template.replace('{{projectOverview}}', projectOverview);
      template = template.replace('{{eckManifestSection}}', eckManifestSection);
      // --- END INJECT ---
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions,
      hierarchicalWorkflow,
      commandFormats
    };

    let renderedTemplate = render(template, data);
    
    // Inject dynamic profile context if a profile is active
    if (context.options && context.options.profile && context.repoPath) {
      let metadataHeader = '\n\n## Partial Snapshot Context\n';
      metadataHeader += `- **Profile(s) Active:** ${context.options.profile}\n`;
      try {
          const allProfiles = await getAllProfiles(context.repoPath);
          const activeProfileNames = context.options.profile.split(',').map(p => p.trim().replace(/^-/, ''));
          const allProfileNames = Object.keys(allProfiles).filter(p => !activeProfileNames.includes(p));
          if (allProfileNames.length > 0) {
               metadataHeader += `- **Other Available Profiles:** ${allProfileNames.join(', ')}\n`;
          }
      } catch (e) { /* fail silently on metadata generation */ }

      const insertMarker = "### "; // Generic marker since we change the H1s
      // Insert before first H3 (WORKFLOW usually)
      renderedTemplate = renderedTemplate.replace(/### /, metadataHeader + '\n### ');
    }

    return renderedTemplate;

  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header', error.message);
    return `# Snapshot for ${context.repoName || 'Project'}

Generated: ${new Date().toISOString()}

---

`;
  }
}


--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { executePrompt as askClaude } from '../services/claudeCliService.js';
import { getProfile, loadSetupConfig } from '../config.js';
import micromatch from 'micromatch';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = '.eck/';
  const comment = '# Added by eck-snapshot to ignore metadata directory';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

// Helper function to determine if a string is a glob pattern
function isGlob(str) {
  return str.includes('*') || str.includes('?') || str.includes('{');
}

/**
 * Applies advanced profile filtering (multi-profile, exclusion, and ad-hoc globs) to a file list.
 */
export async function applyProfileFilter(allFiles, profileString, repoPath) {
  const profileParts = profileString.split(',').map(p => p.trim()).filter(Boolean);
  
  const includeGlobs = [];
  const excludeGlobs = [];
  const includeNames = [];
  const excludeNames = [];

  // Step 1: Differentiate between profile names and ad-hoc glob patterns
  for (const part of profileParts) {
    const isNegative = part.startsWith('-');
    const pattern = isNegative ? part.substring(1) : part;

    if (isGlob(pattern)) {
      if (isNegative) {
        excludeGlobs.push(pattern);
      } else {
        includeGlobs.push(pattern);
      }
    } else {
      if (isNegative) {
        excludeNames.push(pattern);
      } else {
        includeNames.push(pattern);
      }
    }
  }

  let workingFiles = [];
  let finalIncludes = [...includeGlobs];
  let finalExcludes = [...excludeGlobs];

  // Step 2: Load patterns from specified profile names
  const allProfileNames = [...new Set([...includeNames, ...excludeNames])];
  const profiles = new Map();
  for (const name of allProfileNames) {
    const profile = await getProfile(name, repoPath);
    if (profile) {
      profiles.set(name, profile);
    } else {
      // This is an ad-hoc glob, not a profile, so no warning is needed.
      if (!isGlob(name)) {
        console.warn(`‚ö†Ô∏è Warning: Profile '${name}' not found and will be skipped.`);
      }
    }
  }

  for (const name of includeNames) {
    if (profiles.has(name)) {
      finalIncludes.push(...(profiles.get(name).include || []));
      finalExcludes.push(...(profiles.get(name).exclude || []));
    }
  }
  for (const name of excludeNames) {
    if (profiles.has(name)) {
      finalExcludes.push(...(profiles.get(name).include || []));
    }
  }
  
  // Step 3: Apply the filtering logic
  if (finalIncludes.length > 0) {
    workingFiles = micromatch(allFiles, finalIncludes);
  } else if (includeNames.length > 0 && includeGlobs.length === 0) {
    workingFiles = [];
  } else {
    workingFiles = allFiles;
  }

  if (finalExcludes.length > 0) {
    workingFiles = micromatch.not(workingFiles, finalExcludes);
  }

  return workingFiles;
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');

  // Load setup configuration to check AI generation settings
  let aiGenerationEnabled = false;
  try {
    const setupConfig = await loadSetupConfig();
    aiGenerationEnabled = setupConfig?.aiInstructions?.manifestInitialization?.aiGenerationEnabled ?? false;
  } catch (error) {
    // If setup config fails to load, default to disabled
    console.warn(`   ‚ö†Ô∏è Could not load setup config: ${error.message}. AI generation disabled.`);
  }

  try {
    // Check if .eck directory already exists and has all required files
    let needsInitialization = false;
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory exists, check if all required files are present
        const requiredFiles = ['ENVIRONMENT.md', 'CONTEXT.md', 'OPERATIONS.md', 'JOURNAL.md'];
        for (const fileName of requiredFiles) {
          try {
            await fs.stat(path.join(eckDir, fileName));
          } catch (error) {
            console.log(`   ‚ÑπÔ∏è Missing ${fileName}, initialization needed`);
            needsInitialization = true;
            break;
          }
        }
        if (!needsInitialization) {
          // All files exist, no need to initialize
          return;
        }
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
      needsInitialization = true;
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');

    // --- NEW HYBRID LOGIC --- 
    // 1. Run static analysis first to gather facts.
    let staticFacts = {};
    try {
      staticFacts = await detectProjectType(projectPath);
      console.log(`   üîç Static analysis complete. Detected type: ${staticFacts.type}`);
    } catch (e) {
      console.warn(`   ‚ö†Ô∏è Static project detection failed: ${e.message}. Proceeding with generic prompts.`);
    }

    // Prevent AI hallucination by removing low-confidence "other possibilities"
    if (staticFacts && staticFacts.allDetections) {
      delete staticFacts.allDetections;
    }
    
    const staticFactsJson = JSON.stringify(staticFacts, null, 2);
    // --- END NEW LOGIC ---
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'ENVIRONMENT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw YAML key-value content for an .eck/ENVIRONMENT.md file. Only include detected facts. DO NOT add any keys that are not present in the facts. DO NOT add conversational text or markdown wrappers. Your response MUST start directly with a YAML key (e.g., 'project_type: ...').`,
        content: `# This file is for environment overrides. Add agent-specific settings here.\nagent_id: local_dev\n` // Simple static fallback
      },
      {
        name: 'CONTEXT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/CONTEXT.md file. Use the facts to write ## Description, ## Architecture, and ## Key Technologies. DO NOT add conversational text (like "Here is the file..."). Your response MUST start *directly* with the '# Project Overview' heading.`,
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: `Given these static project analysis facts (especially package.json scripts):
${staticFactsJson}

Generate the raw Markdown content ONLY for a .eck/OPERATIONS.md file. DO NOT add conversational text. Your response MUST start *directly* with the '# Common Operations' heading. List commands for ## Development Setup, ## Running the Project, and ## Testing.`,
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/ROADMAP.md file. DO NOT add conversational text. Start *directly* with '# Project Roadmap'. Propose 1-2 *plausible* placeholder items for ## Current Sprint/Phase and ## Next Phase based on the project type.`,
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        prompt: `Generate the raw Markdown content ONLY for a .eck/TECH_DEBT.md file. DO NOT add conversational text. Start *directly* with '# Technical Debt'. Propose 1-2 *common* placeholder items for ## Code Quality Issues and ## Refactoring Opportunities.`,
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file (only if it doesn't exist)
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      
      // Skip if file already exists
      try {
        await fs.stat(filePath);
        console.log(`   ‚úÖ ${file.name} already exists, skipping`);
        continue;
      } catch (error) {
        // File doesn't exist, create it
      }
      
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For files with a prompt, try to dynamically generate (only if enabled)
      if (file.prompt && aiGenerationEnabled) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponseObject = await askClaude(file.prompt); // Use the prompt
          const rawText = aiResponseObject.result; // Handle Claude response

          if (!rawText || typeof rawText.replace !== 'function') {
             throw new Error(`AI returned invalid content type: ${typeof rawText}`);
          }

          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = rawText.replace(/^```(markdown|yaml)?\n|```$/g, '').trim();

          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}


--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));

  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }

  const best = detections[0];

  // Special handling for mixed monorepos
  const isLikelyMonorepo = detections.length > 1 && detections.some(d => d.score >= 40);

  if (isLikelyMonorepo) {
    // If we have multiple strong detections, prefer the highest priority with substantial evidence
    const strongDetections = detections.filter(d => d.score >= 40);
    if (strongDetections.length > 1) {
      const primaryType = strongDetections[0].type;
      return {
        type: primaryType,
        confidence: Math.min(strongDetections[0].score / 100, 1.0),
        details: {
          ...strongDetections[0].details,
          isMonorepo: true,
          additionalTypes: strongDetections.slice(1).map(d => d.type)
        },
        allDetections: detections
      };
    }
  }

  // Boost confidence for strong workspace indicators
  if (best.details && (best.details.isWorkspace || best.details.workspaceSize)) {
    const boostedScore = best.score + 20; // Bonus for workspace structure
    return {
      type: best.type,
      confidence: Math.min(boostedScore / 100, 1.0),
      details: best.details,
      allDetections: detections
    };
  }

  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;

  // Check for required files (check both root and common subdirectories)
  if (pattern.files) {
    for (const file of pattern.files) {
      // Check in root directory first
      const rootExists = await fileExists(path.join(projectPath, file));
      if (rootExists) {
        score += 25; // Each required file adds points
      } else {
        // For Cargo.toml and other project files, also check common subdirectory patterns
        const commonSubdirs = ['src', 'lib', 'app', 'core', 'backend', 'frontend'];
        // Add project-type specific subdirectories
        if (file === 'Cargo.toml') {
          commonSubdirs.push('codex-rs', 'rust', 'server', 'api');
        }
        if (file === 'package.json') {
          commonSubdirs.push('codex-cli', 'cli', 'client', 'web', 'ui');
        }

        for (const subdir of commonSubdirs) {
          const subdirExists = await fileExists(path.join(projectPath, subdir, file));
          if (subdirExists) {
            score += 20; // Slightly lower score for subdirectory finds
            break; // Only count once per file type
          }
        }
      }
    }
  }

  // Check for required directories (check both root and one level deep)
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const rootExists = await directoryExists(path.join(projectPath, dir));
      if (rootExists) {
        score += 20; // Each required directory adds points
      } else {
        // Check in common project subdirectories
        const projectSubdirs = ['codex-rs', 'codex-cli', 'src', 'lib', 'app'];
        for (const projDir of projectSubdirs) {
          const subdirExists = await directoryExists(path.join(projectPath, projDir, dir));
          if (subdirExists) {
            score += 15; // Lower score for nested directory finds
            break;
          }
        }
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names (more precise matching)
        const foundInDeps = Object.keys(allDeps).some(dep => dep === patternText || dep.startsWith(patternText + '/'));
        // Only check for exact matches in keywords array, not description (too broad)
        const foundInKeywords = packageJson.keywords && Array.isArray(packageJson.keywords)
          ? packageJson.keywords.some(keyword => keyword.toLowerCase() === patternText.toLowerCase())
          : false;
        
        if (foundInDeps || foundInKeywords) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };

  try {
    // Check both root and common subdirectories for Cargo.toml
    let cargoPath = path.join(projectPath, 'Cargo.toml');
    let cargoContent = null;

    if (await fileExists(cargoPath)) {
      cargoContent = await fs.readFile(cargoPath, 'utf-8');
    } else {
      // Check common Rust project subdirectories
      const rustSubdirs = ['codex-rs', 'rust', 'src', 'core', 'server'];
      for (const subdir of rustSubdirs) {
        const subdirCargoPath = path.join(projectPath, subdir, 'Cargo.toml');
        if (await fileExists(subdirCargoPath)) {
          cargoPath = subdirCargoPath;
          cargoContent = await fs.readFile(subdirCargoPath, 'utf-8');
          details.primaryLocation = subdir;
          break;
        }
      }
    }

    if (!cargoContent) {
      return details;
    }

    const nameMatch = cargoContent.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = cargoContent.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = cargoContent.match(/edition\s*=\s*"([^"]+)"/);

    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];

    // Check if it's a workspace
    if (cargoContent.includes('[workspace]')) {
      details.isWorkspace = true;

      // Count workspace members
      const workspaceMatch = cargoContent.match(/members\s*=\s*\[([\s\S]*?)\]/);
      if (workspaceMatch) {
        const members = workspaceMatch[1].split(',').map(m => m.trim().replace(/"/g, '')).filter(m => m);
        details.workspaceMembers = members.length;
      }
    }

    // Check for multiple Cargo.toml files (indicates workspace structure)
    if (details.primaryLocation) {
      const subdirPath = path.join(projectPath, details.primaryLocation);
      try {
        const subdirs = await fs.readdir(subdirPath, { withFileTypes: true });
        let cargoCount = 0;
        for (const entry of subdirs) {
          if (entry.isDirectory()) {
            const memberCargoPath = path.join(subdirPath, entry.name, 'Cargo.toml');
            if (await fileExists(memberCargoPath)) {
              cargoCount++;
            }
          }
        }
        if (cargoCount > 3) { // If many workspace members, this is definitely a Rust project
          details.workspaceSize = 'large';
        }
      } catch (error) {
        // Ignore
      }
    }

  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }

  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'c': [0, 0.23, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    const parsedData = JSON.parse(data);
    // Ensure the structure is complete by merging with defaults
    return {
        coefficients: { ...DEFAULT_COEFFICIENTS, ...parsedData.coefficients },
        trainingPoints: parsedData.trainingPoints || {}
    };
  } catch (error) {
    // If file doesn't exist or is malformed, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];

  if (!points || points.length === 0) {
    // No points, nothing to do.
    return;
  }

  if (points.length === 1) {
    // With one point, use a direct ratio for the linear coefficient.
    const point = points[0];
    if (point.fileSizeInBytes > 0) { // Avoid division by zero
        const ratio = point.actualTokens / point.fileSizeInBytes;
        data.coefficients[projectType] = [
            0, // intercept
            Math.max(0, ratio), // linear term (slope)
            0, 0 // quadratic, cubic
        ];
    }
    return;
  }

  // Use linear regression for 2 or more points.
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;

  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;

    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }

  const denominator = (n * sumX2 - sumX * sumX);
  if (denominator === 0) return; // Avoid division by zero, can't calculate slope

  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / denominator;
  const intercept = (sumY - slope * sumX) / n;

  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

--- File: /test_prune.md/eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-10-11T12:17:51.271Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 52
- **Total Files in Repo:** 65

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.



## Project-Specific Manifest (.eck Directory)

This project includes a `.eck` directory with specific context and configuration:

### Project Context

# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.

### Operations Guide

# Common Operations

## Development Setup
```bash
# Setup commands
npm install
# or yarn install
```

## Running the Project
```bash
# Development mode
npm run dev

# Production build
npm run build
```

## Testing
```bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
```

## Deployment
```bash
# Deployment commands
npm run deploy
```

## Troubleshooting
Common issues and their solutions.

### Development Journal

---
task_id: fix-profile-detect-and-add-index-viewer-v1
date: 2025-10-10
type: fix
scope: cli
---

# Fix JSON parsing in profile-detect

- Modified extractJson to be more robust against AI log wrappers
- Finds first '{' and last '}' to extract JSON from surrounding text
- Added /managed_components/ to .gitignore
- Implemented new index-view command to inspect code chunks database
- This resolves the crash when running the profile-detect command

---
task_id: feat-c-language-support-v1
date: 2025-10-10
type: feat
scope: core
---

# Add support for C language projects

- Added C project detection (Makefile, CMakeLists.txt, *.c, *.h)
- Added C-specific file filtering for compiled objects (.o, .a, .so)
- Installed tree-sitter-c@0.21.4 parser
- Integrated C parser into segmenter for functions, structs, enums, unions
- Added token estimation coefficients for C (0.23 ratio)

---
task_id: refactor-claude-skip-permissions-default-v1
date: 2025-09-28T14:30:00.000Z
type: refactor
scope: claude-cli
---
## Force --dangerously-skip-permissions for all Claude calls

- Removed the user-facing '--dangerously-skip-permissions' option from 'ask-claude' and 'ask-claude-session' commands.
- Hardcoded the flag in the service layer to ensure all calls to 'claude-code' are non-interactive by default.
- This change improves the reliability of the tool in automated workflows by preventing it from hanging on permission prompts.

---
task_id: refactor-gpt-service-to-codex-cli-v1
date: 2025-09-28T14:00:00.000Z
type: refactor
scope: services
---
## Switch ask-gpt from chatgpt-cli to official codex CLI

- Replaced `npx chatgpt` calls with the official `codex` CLI, using the `exec --full-auto` command for machine-readable output.
- Implemented an automatic login flow that detects authentication errors and triggers the interactive `codex login` command.
- Created a new `authService.js` to handle the login initiation.
- Removed the `open` package dependency as it is no longer needed.
- Updated tests in `gptService.test.js` to mock the new `codex` command flow.
- Added comprehensive documentation in README.md for both ChatGPT and Claude Code integration.
- Enhanced CLI help with detailed examples and authentication instructions.

---
task_id: gpt-test-1
date: 2025-09-28T09:24:01.314Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:41.532Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:33.203Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:43.135Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:27.678Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:04.054Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: configure-claude-auto-accept-v1
date: 2025-09-14T23:34:21Z
type: feat
scope: workflow
---

## Enable and document claude-code auto-accept mode

Created a global `settings.json` for claude-code to enable `acceptEdits` by default, allowing for fully autonomous operation. Added a `CLAUDE_SETUP.md` file to document this essential configuration step for new developers or fresh installations.

---
task_id: create-eck-commit-command-v1
date: 2025-09-14T23:29:38Z
type: feat
scope: workflow
---

## Create custom /eck:commit claude-code command

Added a custom slash command to automate the new structured journaling and conventional commit process. This command takes structured input (type, scope, summary, details) and uses it to update JOURNAL.md and create a git commit, enforcing our new workflow.

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure

### Environment Overrides

The following environment settings override auto-detected values:

- **project_type**: nodejs
- **project_name**: @xelth/eck-snapshot
- **project_version**: 3.0.0
- **has_typescript**: false
- **framework**: node.js
- **is_monorepo**: false

**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.

---



### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (`gemini_wsl`), who has a detailed (`_ja.md`) snapshot and the ability to command a **Coder** agent (`claude`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (`gemini_wsl`):** Receives strategic tasks, analyzes the `_ja.md` snapshot, breaks the task down, and commands the Coder.
  - **Coder (`claude`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while `gemini_wsl` can use its own tools for analysis, validation, and running shell commands.

### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`) - LOW-LEVEL EXECUTION:**
Use `apply_code_changes` for simple, direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**2. For Junior Architects (`gemini_wsl`) - HIGH-LEVEL DELEGATION:**
Use `execute_strategic_task` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

```json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to `routes/api.js`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces

### Gemini WSL Agent (Junior Architect) (ID: "gemini_wsl")
- **Description:** Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.
- **GUI Support:** No (Headless)
- **Capabilities:** git operations, npm install, file editing, gemini (self), claude (delegate)
- **Restrictions:** Runs only inside the WSL environment

### Gemini Windows Agent (Standalone) (ID: "gemini_windows")
- **Description:** Gemini, running in native Windows (PowerShell). Can only access Windows tools.
- **GUI Support:** Yes
- **Capabilities:** git operations, npm install, file editing, gemini (self)
- **Restrictions:** Runs only in native Windows, Cannot access WSL-only tools like claude



## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .gemini/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îî‚îÄ‚îÄ gpt.toml
‚îú‚îÄ‚îÄ create/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ commands/
‚îÇ               ‚îî‚îÄ‚îÄ extensions/
‚îÇ                   ‚îî‚îÄ‚îÄ sample-extension.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ askGpt.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoDocs.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProfiles.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruneSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queryProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.test.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ segmenter.js
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresConnector.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_simple.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysisService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.test.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcherService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddingService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gptService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gptService.test.js
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architect-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ envScanRequest.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitWorkflow.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgent.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorMode.md
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ test_prune.md/
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ codex_delegation_snapshot.md
‚îú‚îÄ‚îÄ COMMANDS_REFERENCE.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ knexfile.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ setup.json
‚îú‚îÄ‚îÄ setup.json.bak
‚îú‚îÄ‚îÄ test_knexfile.js
‚îú‚îÄ‚îÄ test_snapshot.md
‚îî‚îÄ‚îÄ vitest.config.js
```

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs": [
      0,
      0.3091314011805184,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs-monorepo": [
      0,
      0.30927098145345017,
      0,
      0
    ]
  },
  "trainingPoints": {
    "nodejs": [
      {
        "fileSizeInBytes": 229899,
        "estimatedTokens": 45980,
        "actualTokens": 71069,
        "timestamp": "2025-09-21T23:52:36.862Z"
      }
    ],
    "nodejs-monorepo": [
      {
        "fileSizeInBytes": 261612,
        "estimatedTokens": 65403,
        "actualTokens": 80909,
        "timestamp": "2025-09-22T01:50:24.392Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Google Gemini API Key - Required for the 'index' and 'query' commands
# Get your key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_API_KEY_HERE"

# PostgreSQL Connection Details
DB_HOST=127.0.0.1
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_DATABASE=eck_snapshot_db

--- File: /.gemini/tools/gpt.toml ---

description = 'Ask GPT (ChatGPT subscription) for coding tasks.'
prompt = '!{node $(pwd)/index.js ask-gpt "{{args}}"}'  # Dynamic path


--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# EckSnapshot Index
/.ecksnapshot_index/

# Generated snapshot files
*_vectors.json
*_rag_*.md

# Environment variables
.env
.env.local
.env.production



# Added by eck-snapshot to ignore metadata directory
.eck/

# Test snapshot directory
create-snapshot/

# Claude Code settings (exclude all except commands/eck)
.claude/
!.claude/commands/
!.claude/commands/eck/
!.claude/commands/eck/**

# Third-party managed components
/managed_components/


--- File: /COMMANDS_REFERENCE.md ---

# Commands Reference

This document contains essential commands and setup instructions for the project's multi-agent architecture.

## Junior Architect (`gemini_wsl`) Setup Guide

This section explains how the `gemini_wsl` agent (Junior Architect) is configured to delegate coding tasks to the `claude` agent (Coder).

This architecture relies on `gemini-cli`'s custom tool feature.

## 1. Agent Definition

The Junior Architect (JA) is the `gemini_wsl` agent, which is an instance of `gemini-cli` running in WSL. It is defined in `setup.json`.

## 2. Custom Command (`claude.toml`)

The JA's ability to delegate to the Coder (`claude`) is defined by a custom `gemini-cli` command.

This command must be defined in a file named `claude.toml` and placed in the `gemini-cli` configuration directory (e.g., `~/.gemini/tools/claude.toml`).

### `claude.toml` Content

```toml
description = "Ask Claude (from the eckSnapshot project) to help with a task."
prompt = """!{node /mnt/c/Users/xelth/eckSnapshot/index.js ask-claude "{{args}}"}"""
```

## 3. JA Prompt (`agent-prompt.template.md`)

The JA *knows* how to *use* this command because its main system prompt (loaded from `src/templates/agent-prompt.template.md`) instructs it to.

This prompt *mandates* that the `{{args}}` it passes to the `/claude` command must be a single-line **JSON string** in the `apply_code_changes` format.

## 4. Execution Flow

1.  **Senior Architect (Gemini)** gives a high-level `execute_strategic_task` to `gemini_wsl`.
2.  **`gemini_wsl` (JA)** analyzes the task and formulates a low-level `apply_code_changes` JSON payload.
3.  **`gemini_wsl`** executes `/claude` with the JSON payload as a single string argument (`{{args}}`).
4.  **`claude.toml`** executes the `eck-snapshot ask-claude "{...}"` shell command.
5.  **`eck-snapshot`** (specifically `claudeCliService.js`) receives the JSON string as a 'prompt'.
6.  It forwards this prompt to the `claude-cli` binary (`local_dev`), which is smart enough to parse the JSON and execute the `apply_code_changes` task.

## Claude Code Commands

### Commit Command (`.claude/commands/eck/commit.md`)

A custom command for structured commits with automatic journaling. Place this file in `.claude/commands/eck/commit.md` to enable it in Claude Code.

**Usage:** `/commit <type> <scope> <summary> <details>`

**Function:**
- Stages all current changes
- Creates YAML frontmatter for journal entry with task_id, date, type, scope
- Creates markdown body with summary and details
- Prepends complete journal entry to `.eck/JOURNAL.md`
- Creates conventional commit message: `{type}({scope}): {summary}`
- Executes the commit

**Example:** `/commit feat api "Add user authentication" "Implemented JWT-based auth with login/logout endpoints"`

**Important:** This command should be preserved in git (via `.gitignore` rules) so it can be recreated if lost. The command integrates with the project's `.eck` manifest system for structured development journaling.

## Setup Gemini Command

### Dynamic Path Configuration

The `setup-gemini` command automates the creation of `claude.toml` with dynamic path resolution, eliminating the need for hardcoded paths like `/mnt/c/...`.

**Usage:** `eck-snapshot setup-gemini [options]`

**Options:**
- `-v, --verbose` - Show detailed output and error information

**Function:**
- Detects `gemini-cli` installation using `which()`
- Resolves current project path using `process.cwd()`
- Creates `~/.gemini/tools/claude.toml` with dynamic paths
- Integrates with `setup.json` for environment variable configuration
- Handles cross-platform compatibility (WSL/Windows/macOS/Linux)

**Example Generated `claude.toml`:**
```toml
[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["/home/user/projects/eckSnapshot/index.js", "ask-claude"]

[claude.metadata]
platform = "linux"
working_directory = "/home/user/projects/eckSnapshot"
```

**Benefits:**
- ‚úÖ No hardcoded `/mnt/c/` paths
- ‚úÖ Works across all platforms automatically
- ‚úÖ Resolves paths dynamically based on current working directory
- ‚úÖ Integrates with existing `setup.json` configuration
- ‚úÖ Validates `gemini-cli` and `index.js` availability before setup

## MCP Integration with Feedback Support

### MCP Feedback Object

The `mcp_feedback` object provides enhanced error reporting and status tracking for MCP (Message Control Protocol) integration with gemini-cli delegation.

**Structure:**
```json
{
  "mcp_feedback": {
    "success": true,
    "errors": [],
    "mcp_version": "1.0"
  }
}
```

**Properties:**
- `success` (boolean): Indicates if the MCP operation completed successfully
- `errors` (array): List of error messages or issues encountered during execution
- `mcp_version` (string): Version of MCP protocol used

**Usage in Payloads:**

Add `mcp_feedback` to the `post_execution_steps` object in your JSON command payloads:

```json
{
  "target_agent": "local_dev",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "Implement user authentication",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "auth",
        "summary": "Add JWT authentication"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**Implementation:**

The `claudeCliService.js` automatically parses `mcp_feedback` from incoming prompts and includes it in the response object. When errors are present, they are logged to the console for debugging purposes.

**Benefits:**
- ‚úÖ Enhanced error reporting for MCP operations
- ‚úÖ Version tracking for protocol compatibility
- ‚úÖ Automatic logging of MCP errors
- ‚úÖ Improved debugging for gemini-cli delegation issues

## Auto-Generated Gemini Extensions

*This section is automatically generated. Run `npm run docs:auto` to update.*

### sample-extension

Sample Gemini extension for demonstration

**Commands:**

- **sample-command**: A sample command for testing auto-docs
  - Usage: `sample-command [options]`
  - Examples: `sample-command --help`

**Tools:**

- **sample-tool**: A sample tool for testing auto-docs
  - Usage: Use this tool for sample operations



--- File: /codex_delegation_snapshot.md ---

üîç WSL detected, using configured host: 172.29.16.1


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /knexfile.js ---

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
const isWSL = process.platform === 'linux' && 
  (process.env.WSL_DISTRO_NAME || 
   fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));

if (isWSL && (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1')) {
  // For WSL, always use the standard WSL2 gateway IP
  process.env.DB_HOST = '172.29.16.1';
}

export default {
  development: {
    client: 'pg',
    connection: {
      host: process.env.DB_HOST || '127.0.0.1',
      port: process.env.DB_PORT || 5432,
      user: process.env.DB_USER || 'myuser',
      password: process.env.DB_PASSWORD || 'mypassword',
      database: process.env.DB_DATABASE || 'eck_snapshot_db',
    },
    pool: {
      min: 2,
      max: 10
    }
  }
};

--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-c": "^0.21.4",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /packages/cli/src/commands/extensions/sample-extension.json ---

{
  "name": "sample-extension",
  "description": "Sample Gemini extension for demonstration",
  "commands": [
    {
      "name": "sample-command",
      "description": "A sample command for testing auto-docs",
      "usage": "sample-command [options]",
      "examples": [
        "sample-command --help"
      ]
    }
  ],
  "tools": [
    {
      "name": "sample-tool",
      "description": "A sample tool for testing auto-docs",
      "usage": "Use this tool for sample operations"
    }
  ]
}

--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": [
          "build.gradle",
          "build.gradle.kts",
          "settings.gradle",
          "settings.gradle.kts"
        ],
        "directories": [
          "app/src/main",
          "app/src/androidTest"
        ],
        "manifestFiles": [
          "AndroidManifest.xml"
        ],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": [
          "package.json"
        ],
        "directories": [
          "packages",
          "apps",
          "libs"
        ],
        "patterns": [
          "workspaces",
          "lerna",
          "nx",
          "rush"
        ],
        "priority": 7
      },
      "nodejs": {
        "files": [
          "package.json"
        ],
        "directories": [
          "node_modules"
        ],
        "priority": 6
      },
      "python-poetry": {
        "files": [
          "pyproject.toml"
        ],
        "patterns": [
          "tool.poetry"
        ],
        "priority": 9
      },
      "python-pip": {
        "files": [
          "requirements.txt",
          "setup.py",
          "setup.cfg"
        ],
        "directories": [
          "__pycache__",
          "venv",
          ".venv"
        ],
        "priority": 7
      },
      "python-conda": {
        "files": [
          "environment.yml",
          "environment.yaml",
          "conda.yml"
        ],
        "priority": 8
      },
      "django": {
        "files": [
          "manage.py"
        ],
        "patterns": [
          "django",
          "Django"
        ],
        "priority": 9
      },
      "flask": {
        "files": [
          "app.py",
          "application.py"
        ],
        "patterns": [
          "flask",
          "Flask"
        ],
        "priority": 8
      },
      "flutter": {
        "files": [
          "pubspec.yaml"
        ],
        "directories": [
          "lib",
          "android",
          "ios"
        ],
        "priority": 8
      },
      "react-native": {
        "files": [
          "package.json"
        ],
        "directories": [
          "android",
          "ios"
        ],
        "patterns": [
          "react-native"
        ],
        "priority": 8
      },
      "rust": {
        "files": [
          "Cargo.toml"
        ],
        "directories": [
          "src",
          "target"
        ],
        "priority": 9
      },
      "go": {
        "files": [
          "go.mod",
          "go.sum"
        ],
        "directories": [
          "cmd",
          "pkg",
          "internal"
        ],
        "priority": 7
      },
      "dotnet": {
        "files": [
          "*.csproj",
          "*.sln",
          "*.fsproj",
          "*.vbproj"
        ],
        "directories": [
          "bin",
          "obj"
        ],
        "priority": 7
      },
      "c": {
        "files": [
          "Makefile",
          "CMakeLists.txt",
          "*.c",
          "*.h"
        ],
        "directories": [
          "src",
          "include",
          "lib"
        ],
        "priority": 6
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "smartMode": {
    "tokenThreshold": 7000000,
    "_comment": "Projects over this token count will use vector indexing instead of single file snapshots"
  },
  "vectorIndex": {
    "autoExportOnIndex": true,
    "_comment": "Automatically export the vector index to a file after every successful 'index' command."
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "npm-shrinkwrap.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*",
      "LICENSE*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "coverage/",
      "create-snapshot/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      },
      "c": {
        "filesToIgnore": [
          "*.o",
          "*.a",
          "*.so",
          "*.out",
          "*.exe"
        ],
        "dirsToIgnore": [
          "build/",
          "cmake-build-debug/",
          "cmake-build-release/",
          ".cmake/"
        ],
        "extensionsToIgnore": [
          ".o",
          ".a",
          ".so",
          ".out"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./.eck/snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      },
      "gemini_wsl": {
        "active": true,
        "name": "Gemini WSL Agent (Junior Architect)",
        "description": "Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "gemini",
            "claude"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)",
          "claude (delegate)"
        ],
        "restrictions": [
          "Runs only inside the WSL environment"
        ]
      },
      "gemini_windows": {
        "active": true,
        "name": "Gemini Windows Agent (Standalone)",
        "description": "Gemini, running in native Windows (PowerShell). Can only access Windows tools.",
        "guiSupport": true,
        "identification": {
          "markers": [
            "Windows",
            "gemini",
            "!WSL"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)"
        ],
        "restrictions": [
          "Runs only in native Windows",
          "Cannot access WSL-only tools like claude"
        ]
      }
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "src/templates/envScanRequest.md",
      "gitWorkflow": "src/templates/gitWorkflow.md",
      "multiAgent": "src/templates/multiAgent.md",
      "vectorMode": "src/templates/vectorMode.md",
      "agent": "src/templates/agent-prompt.template.md"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject, viewIndex } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .action(createRepoSnapshot)
    .addHelpText('after', `
Examples for --profile:
  --profile backend                      (Uses the 'backend' profile)
  --profile "backend,-**/tests/**"         (Uses 'backend' profile, excludes all test files)
  --profile "src/**/*.js,-**/*.test.js"  (Includes all JS files in src, excludes tests)

  Combine predefined profiles (from .eck/profiles.json) with ad-hoc glob patterns.
  Prefix a profile name or glob pattern with '-' to exclude it.
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Index view command
  program
    .command('index-view')
    .description('View the contents of the code chunks database')
    .option('--limit <number>', 'Number of records to display', (val) => parseInt(val), 10)
    .option('--offset <number>', 'Number of records to skip', (val) => parseInt(val), 0)
    .option('--file <path>', 'Filter by file path')
    .action(viewIndex);

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/cli/commands/askGpt.js ---

import { ask } from '../../services/gptService.js';

/**
 * CLI entry point for ask-gpt command.
 * @param {string} payload - JSON payload string.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} options - CLI options.
 */
export async function askGpt(payload, options = {}) {
  const verbose = Boolean(options.verbose);
  const model = options.model || 'gpt-5-codex';
  const reasoning = options.reasoning || 'high';

  if (!payload) {
    console.error('ask-gpt requires a JSON payload argument.');
    process.exitCode = 1;
    return;
  }

  try {
    const result = await ask(payload, { verbose, model, reasoning });
    console.log(JSON.stringify(result, null, 2));
  } catch (error) {
    console.error(error.message);
    if (verbose && error?.stack) {
      console.error(error.stack);
    }
    process.exitCode = 1;
  }
}


--- File: /src/cli/commands/autoDocs.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Auto-generate documentation from gemini-extension.json files
 */
export async function generateAutoDocs() {
  try {
    const projectRoot = path.resolve(__dirname, '../../../');
    const extensionsDir = path.join(projectRoot, 'packages/cli/src/commands/extensions');
    const referenceFile = path.join(projectRoot, 'COMMANDS_REFERENCE.md');

    // Check if extensions directory exists
    try {
      await fs.access(extensionsDir);
    } catch (error) {
      console.log(`Extensions directory not found at: ${extensionsDir}`);
      console.log('Creating example structure...');

      // Create the directory structure
      await fs.mkdir(extensionsDir, { recursive: true });

      // Create a sample gemini-extension.json file for demonstration
      const sampleExtension = {
        name: "sample-extension",
        description: "Sample Gemini extension for demonstration",
        commands: [
          {
            name: "sample-command",
            description: "A sample command for testing auto-docs",
            usage: "sample-command [options]",
            examples: ["sample-command --help"]
          }
        ],
        tools: [
          {
            name: "sample-tool",
            description: "A sample tool for testing auto-docs",
            usage: "Use this tool for sample operations"
          }
        ]
      };

      await fs.writeFile(
        path.join(extensionsDir, 'sample-extension.json'),
        JSON.stringify(sampleExtension, null, 2)
      );

      console.log('Created sample extension at:', path.join(extensionsDir, 'sample-extension.json'));
    }

    // Read all JSON files in the extensions directory
    const files = await fs.readdir(extensionsDir);
    const jsonFiles = files.filter(file => file.endsWith('.json'));

    if (jsonFiles.length === 0) {
      console.log('No JSON files found in extensions directory');
      return;
    }

    console.log(`Found ${jsonFiles.length} extension file(s): ${jsonFiles.join(', ')}`);

    // Parse each JSON file and extract command/tool information
    const extensions = [];

    for (const file of jsonFiles) {
      try {
        const filePath = path.join(extensionsDir, file);
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = JSON.parse(content);
        extensions.push({ filename: file, ...extension });
        console.log(`Parsed extension: ${extension.name || file}`);
      } catch (error) {
        console.warn(`Failed to parse ${file}:`, error.message);
      }
    }

    if (extensions.length === 0) {
      console.log('No valid extension files found');
      return;
    }

    // Generate markdown content
    let markdownContent = '\n## Auto-Generated Gemini Extensions\n\n';
    markdownContent += '*This section is automatically generated. Run `npm run docs:auto` to update.*\n\n';

    for (const extension of extensions) {
      markdownContent += `### ${extension.name || extension.filename}\n\n`;

      if (extension.description) {
        markdownContent += `${extension.description}\n\n`;
      }

      // Add commands section
      if (extension.commands && extension.commands.length > 0) {
        markdownContent += '**Commands:**\n\n';
        for (const command of extension.commands) {
          markdownContent += `- **${command.name}**: ${command.description || 'No description'}\n`;
          if (command.usage) {
            markdownContent += `  - Usage: \`${command.usage}\`\n`;
          }
          if (command.examples && command.examples.length > 0) {
            markdownContent += `  - Examples: ${command.examples.map(ex => `\`${ex}\``).join(', ')}\n`;
          }
        }
        markdownContent += '\n';
      }

      // Add tools section
      if (extension.tools && extension.tools.length > 0) {
        markdownContent += '**Tools:**\n\n';
        for (const tool of extension.tools) {
          markdownContent += `- **${tool.name}**: ${tool.description || 'No description'}\n`;
          if (tool.usage) {
            markdownContent += `  - Usage: ${tool.usage}\n`;
          }
        }
        markdownContent += '\n';
      }
    }

    // Read the current COMMANDS_REFERENCE.md
    let currentContent;
    try {
      currentContent = await fs.readFile(referenceFile, 'utf-8');
    } catch (error) {
      console.warn('COMMANDS_REFERENCE.md not found, creating new file');
      currentContent = '# Commands Reference\n\n';
    }

    // Remove existing auto-generated section if it exists
    const autoGenRegex = /\n## Auto-Generated Gemini Extensions[\s\S]*?(?=\n## |\n# |$)/;
    const updatedContent = currentContent.replace(autoGenRegex, '') + markdownContent;

    // Write the updated content back to the file
    await fs.writeFile(referenceFile, updatedContent);

    console.log('\n‚úÖ Auto-documentation generated successfully!');
    console.log(`üìù Updated: ${referenceFile}`);
    console.log(`üì¶ Processed ${extensions.length} extension(s)`);

  } catch (error) {
    console.error('Failed to generate auto-docs:', error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return {
          content: `--- File: /${normalizedPath} ---\n\n${content}\n\n`,
          path: normalizedPath,
          size: fileStats.size
        };
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const successfulFileObjects = results.filter(Boolean);
    const contentArray = successfulFileObjects.map(f => f.content);

    // Return all processed data instead of writing file
    return {
      stats,
      contentArray,
      successfulFileObjects,
      allFiles,
      originalCwd,
      repoPath
    };
    
  } finally {
    process.chdir(originalCwd); // Ensure we always change back
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      smartModeTokenThreshold: setupConfig.smartMode.tokenThreshold,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    if (estimation.estimatedTokens > config.smartModeTokenThreshold) {
      spinner.succeed('Project is large. Switching to vector indexing mode.');
      await indexProject(repoPath, options);
    } else {
      spinner.succeed('Project is small. Creating dual snapshots...');
      
      // Step 1: Process all files ONCE
      const { 
        stats, 
        contentArray, 
        successfulFileObjects, 
        allFiles, 
        originalCwd: processingOriginalCwd, // We get originalCwd from the processing function
        repoPath: processedRepoPath 
      } = await processProjectFiles(repoPath, options, config, projectDetection.type);

      const originalCwd = process.cwd(); // Get CWD *before* chdir
      process.chdir(processedRepoPath); // Go back to repo path for git hash and tree

      try {
        // --- Common Data --- 
        const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
        const repoName = path.basename(processedRepoPath);
        const gitHash = await getGitCommitHash(processedRepoPath);
        const fileExtension = options.format || config.defaultFormat || 'md';
        const outputPath = options.output || path.resolve(originalCwd, config.output);
        await fs.mkdir(outputPath, { recursive: true });

        const shouldIncludeTree = config.tree && !options.noTree;
        let directoryTree = '';
        if (shouldIncludeTree) {
          console.log('üå≥ Generating directory tree...');
          directoryTree = await generateDirectoryTree(processedRepoPath, '', allFiles, 0, config.maxDepth || 10, config);
        }

        // Calculate included file stats by extension
        const includedFilesByType = new Map();
        for (const fileObj of successfulFileObjects) {
            try {
                let ext = path.extname(fileObj.path);
                if (ext === '') ext = '.no-extension';
                includedFilesByType.set(ext, (includedFilesByType.get(ext) || 0) + 1);
            } catch (e) { /* Silently ignore */ }
        }
        const sortedIncludedStats = [...includedFilesByType.entries()].sort((a, b) => b[1] - a[1]);

        // Calculate Top 10 Largest Files
        const largestFiles = [...successfulFileObjects].sort((a, b) => b.size - a.size).slice(0, 10);

        const fileBody = (directoryTree ? `\n## Directory Structure\n\n\`\`\`\n${directoryTree}\`\`\`\n\n` : '') + contentArray.join('');

        // --- File 1: Architect Snapshot --- 
        const architectOptions = { ...options, agent: false };
        // Load manifest for headers
        const eckManifest = await loadProjectEckManifest(processedRepoPath);
        const isGitRepo = await checkGitRepository(processedRepoPath);

        const architectHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: architectOptions, repoPath: processedRepoPath }, isGitRepo);
        const architectBaseFilename = `${repoName}_snapshot_${timestamp}${gitHash ? `_${gitHash}` : ''}`;
        const architectFilename = `${architectBaseFilename}.${fileExtension}`;
        const architectFilePath = path.join(outputPath, architectFilename);
        await fs.writeFile(architectFilePath, architectHeader + fileBody);

        // --- File 2: Junior Architect Snapshot --- 
        let jaFilePath = null;
        if (!options.profile && !options.agent && fileExtension === 'md') { // Only create JA snapshot if main is MD
          console.log('üñãÔ∏è Generating Junior Architect (_ja) snapshot...');
          const jaOptions = { ...options, agent: true, noTree: false, noAiHeader: false };
          const jaHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: jaOptions, repoPath: processedRepoPath }, isGitRepo);
          const jaFilename = `${architectBaseFilename}_ja.${fileExtension}`;
          jaFilePath = path.join(outputPath, jaFilename);
          await fs.writeFile(jaFilePath, jaHeader + fileBody);
        }

        // --- Combined Report --- 
        console.log('\n‚úÖ Snapshot generation complete!');
        console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        console.log(`üìÑ Architect File: ${architectFilePath}`);
        if (jaFilePath) {
          console.log(`üìÑ Junior Arch File: ${jaFilePath}`);
        }
        console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
        console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
        console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
        console.log(`üìã Format: ${fileExtension.toUpperCase()}`);

        if (sortedIncludedStats.length > 0) {
          console.log('\nüì¶ Included File Types:');
          console.log('---------------------------------');
          for (const [ext, count] of sortedIncludedStats.slice(0, 10)) {
              console.log(`   - ${String(ext).padEnd(15)} ${String(count).padStart(5)} files`);
          }
          if (sortedIncludedStats.length > 10) {
              console.log(`   ... and ${sortedIncludedStats.length - 10} other types.`);
          }
        }

        if (largestFiles.length > 0) {
          console.log('\nüêò Top 10 Largest Files (Included):');
          console.log('---------------------------------');
          for (const fileObj of largestFiles) {
              console.log(`   - ${String(formatSize(fileObj.size)).padEnd(15)} ${fileObj.path}`);
          }
        }
        
        // Excluded/Skipped Files Section
        const hasExcludedContent = stats.excludedFiles > 0 || stats.binaryFiles > 0 || stats.oversizedFiles > 0 || stats.ignoredFiles > 0 || stats.errors.length > 0;
        if (hasExcludedContent) {
          console.log('\nüö´ Excluded/Skipped Files:');
          console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        }
        
        if (stats.excludedFiles > 0) {
          console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
        }
        if (stats.binaryFiles > 0) {
          console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
        }
        if (stats.oversizedFiles > 0) {
          console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
        }
        if (stats.ignoredFiles > 0) {
          console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
        }
        if (stats.errors.length > 0) {
          console.log(`‚ùå Errors: ${stats.errors.length}`);
          if (options.verbose) {
            stats.errors.forEach(err => console.log(`   ${err}`));
          }
        }
        
        // Print detailed skip reasons report
        if (stats.skippedFilesDetails.size > 0) {
          console.log('\nüìã Skip Reasons:');
          console.log('---------------------------------');
          
          for (const [reason, files] of stats.skippedFilesDetails.entries()) {
            console.log(`\nüî∏ ${reason} (${files.length} files):`);
            files.forEach(file => {
              console.log(`   ‚Ä¢ ${file}`);
            });
          }
          console.log('---------------------------------');
        } else {
          console.log('---------------------------------');
        }
        
        // Generate training command string if estimation data is available
        if (estimation && projectDetection.type && !options.profile) {
          const trainingCommand = generateTrainingCommand(projectDetection.type, estimation.estimatedTokens, estimation.totalSize, repoPath);
          console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
          console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
          console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
        }

      } finally {
        process.chdir(originalCwd); // Final reset back to original CWD
      }
    }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}

--- File: /src/cli/commands/detectProfiles.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Extracts a JSON object from a string that might contain markdown wrappers or log output.
 * Finds the first opening brace '{' and the last closing brace '}' to extract the JSON.
 */
function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }

  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1).trim();
  }

  return text.trim();
}

/**
 * Scans the project structure, saves the directory tree to a file, and asks an AI to generate
 * context profiles, saving them to .eck/profiles.json.
 */
export async function detectProfiles(repoPath, options) {
  const spinner = ora('Initializing and scanning project structure...').start();
  try {
    await initializeEckManifest(repoPath);

    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const config = {
        ...userConfig,
        ...setupConfig.fileFiltering,
        ...setupConfig.performance
    };

    const allFiles = await scanDirectoryRecursively(repoPath, config, repoPath);
    spinner.text = 'Generating directory tree...';
    const dirTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth, config);

    if (!dirTree) {
        throw new Error('Failed to generate directory tree or project is empty.');
    }

    spinner.text = 'Saving directory tree to file...';
    const treeFilePath = path.join(repoPath, '.eck', 'directory_tree_for_profiling.md');
    await fs.writeFile(treeFilePath, dirTree);

    const prompt = `You are a code architect. Based on the file directory tree found in the file at './.eck/directory_tree_for_profiling.md', please identify logical 'context profiles' for splitting the project.
Your output MUST be ONLY a valid JSON object.
The keys of the object MUST be the profile names (e.g., 'frontend', 'backend', 'core-logic', 'docs').
The values MUST be an object containing 'include' and 'exclude' arrays of glob patterns.
Example: {"frontend": {"include": ["packages/ui/**"], "exclude": []}, "docs": {"include": ["docs/**"], "exclude": []}}.
DO NOT add any conversational text, introductory sentences, or explanations. Your entire response must be ONLY the JSON object.`;

    spinner.text = 'Asking AI to analyze directory tree and detect profiles...';
    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;

    if (!rawText || typeof rawText.replace !== 'function') {
      throw new Error(`AI returned invalid content type: ${typeof rawText}`);
    }

    spinner.text = 'Saving generated profiles...';
    const cleanedJson = extractJson(rawText);
    let parsedProfiles;
    try {
        parsedProfiles = JSON.parse(cleanedJson);
    } catch (e) {
        console.error('\nInvalid JSON received from AI:', cleanedJson);
        throw new Error(`AI returned invalid JSON: ${e.message}`);
    }

    const outputPath = path.join(repoPath, '.eck', 'profiles.json');
    await fs.writeFile(outputPath, JSON.stringify(parsedProfiles, null, 2));

    const profileKeys = Object.keys(parsedProfiles);
    spinner.succeed(`Successfully detected and saved ${profileKeys.length} profiles to ${outputPath}`);

    console.log('\n‚ú® Detected Profiles:');
    console.log('---------------------------');
    for (const profileName of profileKeys) {
        console.log(`  - ${profileName}`);
    }
    console.log('\nYou can now use these profile names with the --profile flag.');

  } catch (error) {
    spinner.fail(`Failed to detect profiles: ${error.message}`);
  }
}


--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));
    
    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
      console.log('   eck-snapshot index                              # For large projects');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
      console.log('   eck-snapshot index                              # For large projects');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
      console.log('   eck-snapshot index                              # For semantic search');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    let files = await getProjectFiles(projectPath);
    
    // --- Apply Advanced Profile Filtering ---
    const defaultProfile = await getProfile('default', projectPath);
    if (options.profile) {
        mainSpinner.text = `Applying profile filter: '${options.profile}'...`;
        files = await applyProfileFilter(files, options.profile, projectPath);
        mainSpinner.info(`Filtered down to ${files.length} files using profile: '${options.profile}'.`);
    } else if (defaultProfile) {
        mainSpinner.text = "Applying detected 'default' profile...";
        files = micromatch(files, defaultProfile.include, { ignore: defaultProfile.exclude });
        mainSpinner.info(`Filtered down to ${files.length} files using detected 'default' profile.`);
    }
    if (files.length === 0) {
        throw new Error(`Profile filter resulted in 0 files. Aborting.`);
    }
    // --- End Profile Filtering ---

    const profileName = options.profile || 'default';
    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await releaseAnalysisModel();
    await releaseEmbeddingModel();
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}


--- File: /src/cli/commands/pruneSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { parseSnapshotContent, parseSize, formatSize } from '../../utils/fileUtils.js';

function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }
  const firstBracket = text.indexOf('[');
  const lastBracket = text.lastIndexOf(']');
  if (firstBracket !== -1 && lastBracket !== -1 && lastBracket > firstBracket) {
    return text.substring(firstBracket, lastBracket + 1).trim();
  }
  return text.trim();
}

export async function pruneSnapshot(snapshotFile, options) {
  const spinner = ora('Starting snapshot pruning process...').start();
  try {
    const targetSize = parseSize(options.targetSize);
    spinner.text = `Reading snapshot file: ${snapshotFile}`;
    const snapshotContent = await fs.readFile(snapshotFile, 'utf-8');
    const snapshotHeader = snapshotContent.split('--- File: /')[0];
    const files = parseSnapshotContent(snapshotContent);

    if (files.length === 0) {
      spinner.warn('No files found in the snapshot.');
      return;
    }

    const currentSize = Buffer.byteLength(snapshotContent, 'utf-8');
    if (currentSize <= targetSize) {
      spinner.succeed(`Snapshot is already smaller than the target size. (${formatSize(currentSize)} < ${formatSize(targetSize)})`);
      return;
    }

    spinner.text = 'Asking AI to rank files by importance...';
    const filePaths = files.map(f => f.path);
    const prompt = `You are a software architect. Given the following list of file paths from a project snapshot, rank them by importance for understanding the project's core functionality. The most critical files (e.g., entry points, core logic, configurations) should be first. Your output MUST be ONLY a JSON array of strings, with the file paths in ranked order. Do not add any other text.\n\nFILE LIST:\n${JSON.stringify(filePaths, null, 2)}`;

    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;
    const cleanedJson = extractJson(rawText);

    let rankedFiles;
    try {
      rankedFiles = JSON.parse(cleanedJson);
      if (!Array.isArray(rankedFiles) || rankedFiles.some(item => typeof item !== 'string')) {
        throw new Error('AI response is not an array of strings.');
      }
    } catch (e) {
      spinner.fail(`Failed to parse AI's file ranking: ${e.message}`);
      console.error('Received from AI:', cleanedJson);
      return;
    }

    spinner.text = 'Building pruned snapshot...';
    const fileMap = new Map(files.map(f => [f.path, f.content]));
    let newSnapshotContent = snapshotHeader;
    let newSize = Buffer.byteLength(newSnapshotContent, 'utf-8');
    let filesIncluded = 0;

    for (const filePath of rankedFiles) {
      if (fileMap.has(filePath)) {
        const fileContent = fileMap.get(filePath);
        const fileEntry = `--- File: /${filePath} ---\n\n${fileContent}\n\n`;
        const entrySize = Buffer.byteLength(fileEntry, 'utf-8');

        if (newSize + entrySize > targetSize) {
          break;
        }

        newSnapshotContent += fileEntry;
        newSize += entrySize;
        filesIncluded++;
      }
    }

    const outputFilename = `${path.basename(snapshotFile, path.extname(snapshotFile))}_pruned_${options.targetSize}${path.extname(snapshotFile)}`;
    const outputPath = path.join(path.dirname(snapshotFile), outputFilename);

    await fs.writeFile(outputPath, newSnapshotContent);

    spinner.succeed('Snapshot pruning complete!');
    console.log(`- Original Size: ${formatSize(currentSize)}`);
    console.log(`- New Size: ${formatSize(newSize)}`);
    console.log(`- Files Included: ${filesIncluded} / ${files.length}`);
    console.log(`- Pruned snapshot saved to: ${outputPath}`);

  } catch (error) {
    spinner.fail(`An error occurred during pruning: ${error.message}`);
  }
}


--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-—Å–Ω–∞–ø—à–æ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

export async function viewIndex(options) {
  const spinner = ora('Connecting to database...').start();
  const knex = getKnex();

  try {
    await initDb();

    spinner.text = 'Fetching code chunks from database...';

    // Build query with optional filters
    let query = knex('code_chunks')
      .select('id', 'file_path', 'chunk_type', 'chunk_name', 'profile')
      .orderBy('id', 'asc');

    // Apply file filter if specified
    if (options.file) {
      query = query.where('file_path', 'like', `%${options.file}%`);
      spinner.info(`Filtering by file path: ${options.file}`);
    }

    // Apply pagination
    if (options.limit) {
      query = query.limit(options.limit);
    }
    if (options.offset) {
      query = query.offset(options.offset);
    }

    const chunks = await query;

    if (chunks.length === 0) {
      spinner.warn('No code chunks found in the database.');
      return;
    }

    spinner.succeed(`Found ${chunks.length} code chunks`);

    // Display results in a formatted table
    console.log('\nüìä Code Chunks Index:');
    console.log('‚ïê'.repeat(100));
    console.table(chunks.map(chunk => ({
      ID: chunk.id,
      'File Path': chunk.file_path.replace(process.cwd(), '.'),
      Type: chunk.chunk_type,
      Name: chunk.chunk_name,
      Profile: chunk.profile || 'default'
    })));

    // Show summary
    const totalCount = await knex('code_chunks').count('* as count').first();
    console.log(`\nShowing ${chunks.length} of ${totalCount.count} total chunks`);

    if (options.limit && chunks.length === options.limit) {
      console.log(`\nüí° Use --offset ${(options.offset || 0) + options.limit} to view the next page`);
    }

  } catch (error) {
    spinner.fail(`Failed to view index: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/setupGemini.js ---

import which from 'which';
import path from 'path';
import fs from 'fs/promises';
import os from 'os';
import chalk from 'chalk';

/**
 * Sets up claude.toml configuration for gemini-cli integration with dynamic paths
 * @param {Object} options - Command options
 */
export async function setupGemini(options = {}) {
  try {
    console.log(chalk.blue('üîß Setting up gemini-cli integration with dynamic paths...'));

    // Check if gemini-cli is installed
    let geminiCliPath;
    try {
      geminiCliPath = await which('gemini-cli');
      console.log(chalk.green(`‚úÖ Found gemini-cli at: ${geminiCliPath}`));
    } catch (error) {
      console.error(chalk.red('‚ùå gemini-cli not found in PATH'));
      console.log(chalk.yellow('üí° Please install gemini-cli first:'));
      console.log(chalk.cyan('   npm install -g gemini-cli'));
      process.exit(1);
    }

    // Get current working directory for dynamic path resolution
    const currentDir = process.cwd();
    const indexJsPath = path.join(currentDir, 'index.js');

    // Verify index.js exists
    try {
      await fs.access(indexJsPath);
      console.log(chalk.green(`‚úÖ Found eck-snapshot index.js at: ${indexJsPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Could not find index.js at: ${indexJsPath}`));
      console.log(chalk.yellow('üí° Make sure you are running this command from the eck-snapshot project directory'));
      process.exit(1);
    }

    // Create gemini tools directory
    const homeDir = os.homedir();
    const geminiToolsDir = path.join(homeDir, '.gemini', 'tools');

    try {
      await fs.mkdir(geminiToolsDir, { recursive: true });
      console.log(chalk.green(`‚úÖ Created/verified gemini tools directory: ${geminiToolsDir}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to create gemini tools directory: ${error.message}`));
      process.exit(1);
    }

    // Read environment variables from setup.json if available
    let envVars = {};
    try {
      const setupJsonPath = path.join(currentDir, 'setup.json');
      const setupContent = await fs.readFile(setupJsonPath, 'utf-8');
      const setupData = JSON.parse(setupContent);

      // Extract relevant environment variables
      if (setupData.environmentDetection) {
        envVars.ECK_SNAPSHOT_PATH = currentDir;
        console.log(chalk.blue(`üìã Using project context from setup.json`));
      }
    } catch (error) {
      console.log(chalk.yellow('‚ö†Ô∏è  setup.json not found or invalid, using defaults'));
    }

    // Generate claude.toml content with dynamic paths
    const claudeTomlContent = generateClaudeToml(indexJsPath, envVars);

    // Write claude.toml file
    const claudeTomlPath = path.join(geminiToolsDir, 'claude.toml');
    try {
      await fs.writeFile(claudeTomlPath, claudeTomlContent, 'utf-8');
      console.log(chalk.green(`‚úÖ Generated claude.toml at: ${claudeTomlPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to write claude.toml: ${error.message}`));
      process.exit(1);
    }

    // Success summary
    console.log(chalk.green('\nüéâ Setup completed successfully!'));
    console.log(chalk.blue('\nüìã Configuration summary:'));
    console.log(chalk.cyan(`   ‚Ä¢ gemini-cli: ${geminiCliPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ eck-snapshot: ${indexJsPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ claude.toml: ${claudeTomlPath}`));

    if (Object.keys(envVars).length > 0) {
      console.log(chalk.cyan(`   ‚Ä¢ Environment variables: ${Object.keys(envVars).join(', ')}`));
    }

    console.log(chalk.blue('\nüöÄ You can now use:'));
    console.log(chalk.cyan('   gemini-cli claude "Your prompt here"'));
    console.log(chalk.green('\n‚ú® Cross-platform path resolution is automatically handled!'));

  } catch (error) {
    console.error(chalk.red(`‚ùå Setup failed: ${error.message}`));
    if (options.verbose) {
      console.error(chalk.red('Stack trace:'), error.stack);
    }
    process.exit(1);
  }
}

/**
 * Generates claude.toml content with dynamic paths
 * @param {string} indexJsPath - Path to eck-snapshot index.js
 * @param {Object} envVars - Environment variables to include
 * @returns {string} - Generated TOML content
 */
function generateClaudeToml(indexJsPath, envVars = {}) {
  const envSection = Object.keys(envVars).length > 0
    ? `# Environment variables from setup.json
${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}

`
    : '';

  return `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'
# This file uses dynamic paths to work across WSL/Windows environments

${envSection}[claude]
# eck-snapshot integration for AI-powered repository analysis
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

# Command examples:
# gemini-cli claude "Create a snapshot of the current project"
# gemini-cli claude "Analyze the database structure"
# gemini-cli claude "Generate a project overview"

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
generated_at = "${new Date().toISOString()}"
platform = "${process.platform}"
node_version = "${process.version}"
working_directory = "${path.dirname(indexJsPath)}"

# Cross-platform compatibility notes:
# - Paths are automatically resolved using process.cwd()
# - Works in WSL, Windows, macOS, and Linux
# - No hardcoded /mnt/c/ paths required
`;
}

--- File: /src/cli/commands/setupGemini.test.js ---

import { describe, it, expect } from 'vitest';

describe('setupGemini integration', () => {
  it('should validate path resolution logic', () => {
    // Test path join functionality that setupGemini uses
    const currentDir = '/test/project';
    const indexJsPath = `${currentDir}/index.js`;

    expect(indexJsPath).toBe('/test/project/index.js');
    expect(indexJsPath).toContain('index.js');
  });

  it('should validate gemini tools directory structure', () => {
    const homeDir = '/home/user';
    const geminiToolsDir = `${homeDir}/.gemini/tools`;
    const claudeTomlPath = `${geminiToolsDir}/claude.toml`;

    expect(geminiToolsDir).toBe('/home/user/.gemini/tools');
    expect(claudeTomlPath).toBe('/home/user/.gemini/tools/claude.toml');
  });

  it('should validate TOML content structure', () => {
    const indexJsPath = '/test/project/index.js';
    const envVars = { ECK_SNAPSHOT_PATH: '/test/project' };

    // Test environment section generation
    const envSection = Object.keys(envVars).length > 0
      ? `# Environment variables from setup.json\n${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}\n\n`
      : '';

    // Test main TOML structure
    const tomlContent = `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'

${envSection}[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
platform = "${process.platform}"
working_directory = "${indexJsPath.replace('/index.js', '')}"`;

    expect(tomlContent).toContain('[claude]');
    expect(tomlContent).toContain('name = "eck-snapshot"');
    expect(tomlContent).toContain(`args = ["${indexJsPath}", "ask-claude"]`);
    expect(tomlContent).toContain('[claude.metadata]');
    expect(tomlContent).toContain('ECK_SNAPSHOT_PATH = "/test/project"');
  });

  it('should handle cross-platform paths correctly', () => {
    const testPaths = [
      { platform: 'windows', path: 'C:\\Users\\test\\project\\index.js' },
      { platform: 'unix', path: '/home/user/project/index.js' },
      { platform: 'wsl', path: '/mnt/c/Users/test/project/index.js' }
    ];

    testPaths.forEach(({ platform, path }) => {
      expect(path).toContain('index.js');
      expect(path.length).toBeGreaterThan(0);

      // Test that the path is absolute (platform-appropriate)
      if (platform === 'windows') {
        expect(path).toMatch(/^[A-Z]:\\/);
      } else {
        expect(path).toMatch(/^\//);
      }
    });
  });

  it('should validate error handling patterns', () => {
    // Test error message patterns that setupGemini should handle
    const errorPatterns = [
      'gemini-cli not found in PATH',
      'Could not find index.js',
      'Failed to create gemini tools directory',
      'Failed to write claude.toml'
    ];

    errorPatterns.forEach(pattern => {
      expect(pattern).toBeDefined();
      expect(typeof pattern).toBe('string');
      expect(pattern.length).toBeGreaterThan(0);
    });
  });

  it('should test JSON parsing for setup.json', () => {
    const validSetupData = {
      environmentDetection: {
        detected: true
      }
    };

    const jsonString = JSON.stringify(validSetupData);
    const parsed = JSON.parse(jsonString);

    expect(parsed.environmentDetection).toBeDefined();
    expect(parsed.environmentDetection.detected).toBe(true);

    // Test invalid JSON handling pattern
    const invalidJson = 'invalid json {';
    let parseError = null;
    try {
      JSON.parse(invalidJson);
    } catch (e) {
      parseError = e;
    }

    expect(parseError).toBeDefined();
    expect(parseError.message).toContain('JSON');
  });
});

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/core/segmenter.js ---

import { parse } from '@babel/parser';
import _traverse from '@babel/traverse';
const traverse = _traverse.default;
import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import Parser from 'tree-sitter';
import Python from 'tree-sitter-python';
import Java from 'tree-sitter-java';
import Kotlin from 'tree-sitter-kotlin';
import C from 'tree-sitter-c';

function generateHash(content) {
  return crypto.createHash('sha256').update(content).digest('hex');
}

const tsParser = new Parser();
const languageParsers = {
    '.py': Python,
    '.java': Java,
    '.kt': Kotlin,
    '.c': C,
    '.h': C,
};

async function _segmentWithTreeSitter(content, filePath, language) {
    tsParser.setLanguage(language);
    const tree = tsParser.parse(content);
    const chunks = [];
    // Graph relations for tree-sitter are not implemented in this step.
    const relations = [];

    function walk(node) {
        const nodeTypeMap = {
            'function_definition': 'function', 'class_definition': 'class', // Python
            'function_declaration': 'function', 'class_declaration': 'class', // Kotlin/Java
            'method_declaration': 'function', // Java
            'struct_specifier': 'struct', 'enum_specifier': 'enum', 'union_specifier': 'union', 'type_definition': 'typedef', // C
        };

        if (nodeTypeMap[node.type]) {
            const nameNode = node.childForFieldName('name') || node.child(1);
            const chunkName = nameNode ? nameNode.text : 'anonymous';
            const chunkCode = node.text;
            chunks.push({
                filePath,
                chunk_type: nodeTypeMap[node.type],
                chunk_name: chunkName,
                code: chunkCode,
                contentHash: generateHash(chunkCode)
            });
        }
        node.children.forEach(walk);
    }
    walk(tree.rootNode);
    return { chunks, relations };
}

async function _segmentJavaScript(content, filePath) {
    const chunks = [];
    const relations = [];

    try {
        const ast = parse(content, { sourceType: 'module', plugins: ['typescript', 'jsx'], errorRecovery: true });

        const getChunkData = (node) => {
            const chunkName = node.id ? node.id.name : 'anonymous';
            const chunkCode = content.substring(node.start, node.end);
            return { filePath, chunk_name: chunkName, code: chunkCode, contentHash: generateHash(chunkCode) };
        };

        traverse(ast, {
            enter(path) {
                let currentScopeName = 'file';
                const parentFunction = path.findParent((p) => p.isFunctionDeclaration() || p.isClassDeclaration());
                if (parentFunction && parentFunction.node.id) {
                    currentScopeName = parentFunction.node.id.name;
                }

                if (path.isFunctionDeclaration() || path.isClassDeclaration()) {
                    chunks.push({ ...getChunkData(path.node), chunk_type: path.isClassDeclaration() ? 'class' : 'function' });
                }

                if (path.isImportDeclaration()) {
                    const sourceFile = path.node.source.value;
                    relations.push({ from: filePath, to: sourceFile, type: 'IMPORTS' });
                }

                if (path.isCallExpression()) {
                    const calleeName = path.get('callee').toString();
                    relations.push({ from: currentScopeName, to: calleeName, type: 'CALLS' });
                }
            }
        });
    } catch (e) {
        console.error(`Babel parsing error in ${filePath}: ${e.message}`);
    }
    return { chunks, relations };
}

export async function segmentFile(filePath) {
    try {
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = path.extname(filePath);
        let result = { chunks: [], relations: [] };

        if (['.js', '.jsx', '.ts', '.tsx'].includes(extension)) {
            result = await _segmentJavaScript(content, filePath);
        } else if (languageParsers[extension]) {
            result = await _segmentWithTreeSitter(content, filePath, languageParsers[extension]);
        }
        
        // Fallback: if no specific chunks, treat the whole file as one
        if (result.chunks.length === 0) {
            const code = content;
            result.chunks.push({ filePath, chunk_type: 'file', chunk_name: path.basename(filePath), code, contentHash: generateHash(code) });
        }

        return result;
    } catch (error) {
        console.error(`Failed to segment file ${filePath}: ${error.message}`);
        return { chunks: [], relations: [] };
    }
}

--- File: /src/database/postgresConnector.js ---

import knex from 'knex';
import fs from 'fs/promises';
import path from 'path';
import config from '../../knexfile.js';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));

let knexInstance = null;

function getKnex() {
  if (!knexInstance) {
    console.log('Initializing Knex connection...');
    knexInstance = knex(config.development);
  }
  return knexInstance;
}

async function initDb() {
  const db = getKnex();
  try {
    console.log('Checking database connection...');
    await db.raw('SELECT 1+1 AS result');
    console.log('Connection successful.');

    console.log('Applying database schema...');
    // Try full schema first, fallback to simple schema
    try {
      const schemaPath = path.join(__dirname, 'schema.sql');
      const schemaSQL = await fs.readFile(schemaPath, 'utf-8');
      await db.raw(schemaSQL);
      console.log('Full schema with vector extensions applied successfully.');
    } catch (error) {
      console.log('Vector extensions not available, using simplified schema...');
      const simpleSchemaPath = path.join(__dirname, 'schema_simple.sql');
      const simpleSchemaSQL = await fs.readFile(simpleSchemaPath, 'utf-8');
      await db.raw(simpleSchemaSQL);
      console.log('Simplified schema applied successfully.');
    }
  } catch (error) {
    console.error('Error initializing database:', error.message);
    throw error;
  }
}

async function destroyDb() {
  if (knexInstance) {
    console.log('Destroying Knex connection pool...');
    await knexInstance.destroy();
    knexInstance = null;
  }
}

export {
  getKnex,
  initDb,
  destroyDb,
};

--- File: /src/database/schema.sql ---

-- –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
-- (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ Apache AGE —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ PG)
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç)
SELECT create_graph('eck_snapshot_graph');

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞ (—É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding VECTOR(768), -- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è Jina Code v2
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π (—Ä–µ–±–µ—Ä –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

-- –°–æ–∑–¥–∞–µ–º HNSW-–∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX IF NOT EXISTS code_chunks_embedding_idx ON code_chunks USING HNSW (embedding vector_cosine_ops);

--- File: /src/database/schema_simple.sql ---

-- Simplified schema without vector and graph extensions for testing

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding TEXT, -- JSON string representation for now
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

--- File: /src/services/analysisService.js ---

import { pipeline } from '@xenova/transformers';

class AnalysisService {
    static instance = null;
    static modelName = 'Xenova/distilgpt2'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = await pipeline('text-generation', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = null;
        }
    }
}

export async function getCodeSummary(codeChunk) {
    const generator = await AnalysisService.getInstance();

    const prompt = `This code:\n${codeChunk.substring(0, 150)}\nSummary:`;

    const output = await generator(prompt, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true
    });

    const generatedText = output[0].generated_text;
    const summary = generatedText.replace(prompt, '').trim() || 'Auto-generated description';
    return summary.substring(0, 200); // Limit summary length
}

export const releaseModel = AnalysisService.releaseModel;

--- File: /src/services/authService.js ---

import ora from 'ora';
import { execa } from 'execa';

/**
 * Initiates the interactive login flow by spawning 'codex login'.
 * This will open a browser and wait for the user to complete authentication.
 * @returns {Promise<void>}
 */
export async function initiateLogin() {
  const spinner = ora('Authentication required. Please follow the browser instructions.').start();
  try {
    // Run `codex login` interactively, inheriting stdio to show user instructions.
    await execa('codex', ['login'], { stdio: 'inherit' });
    spinner.succeed('Login successful. Retrying original command...');
  } catch (e) {
    spinner.fail('Login process failed or was cancelled.');
    // Re-throw to notify p-retry that the attempt failed.
    throw new Error(`Login failed: ${e.message}`);
  }
}

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';
import pRetry from 'p-retry';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {boolean} continueConversation Whether to continue the last conversation with -c flag.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, continueConversation = false) {
  try {
    // Ensure the log directory exists
    try {
      await import('fs/promises').then(fs => fs.mkdir('./.eck/logs', { recursive: true }));
    } catch (e) {
      console.error(`Failed to create log directory: ${e.message}`);
      // Do not block execution if log dir creation fails, just warn
    }
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }

    return await attemptClaudeExecution(prompt, sessionId);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./.eck/logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./.eck/logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null) {
  const timestamp = new Date().toISOString();
  const logFile = `./.eck/logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./.eck/logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./.eck/logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./.eck/logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null) {
  return new Promise((resolve, reject) => {
    
    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }

    // Always add the skip permissions flag for automation reliability
    args.push('--dangerously-skip-permissions');

    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');
    
    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });
    
    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;
    
    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const ACTIVITY_TIMEOUT = 60000; // 1 minute of inactivity allowed
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log('üíÄ No activity detected for 1 minute, killing process');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${ACTIVITY_TIMEOUT/1000} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

/**
 * Executes a prompt using gemini-cli delegation with retry logic for transient errors.
 * @param {string} prompt The prompt to send to Claude via gemini-cli.
 * @returns {Promise<object>} A promise that resolves with the response from Claude.
 */
export async function askClaude(prompt) {
  return pRetry(async () => {
    try {
      const result = await execa('gemini-cli', ['claude', prompt], {
        timeout: 120000 // 2 minute timeout
      });

      // Parse mcp_feedback if present in prompt
      let mcpFeedback = null;
      try {
        const promptObj = JSON.parse(prompt);
        if (promptObj.payload && promptObj.payload.post_execution_steps && promptObj.payload.post_execution_steps.mcp_feedback) {
          mcpFeedback = promptObj.payload.post_execution_steps.mcp_feedback;

          // Log if errors array is non-empty
          if (mcpFeedback.errors && Array.isArray(mcpFeedback.errors) && mcpFeedback.errors.length > 0) {
            console.warn('MCP feedback contains errors:', mcpFeedback.errors);
          }
        }
      } catch (parseError) {
        // If prompt is not valid JSON or doesn't contain mcp_feedback, continue normally
      }

      return {
        stdout: result.stdout,
        stderr: result.stderr,
        success: true,
        mcp_feedback: mcpFeedback
      };
    } catch (error) {
      // Check if this is a transient error that should be retried
      if (isTransientError(error)) {
        console.log(`Transient error detected, retrying: ${error.message}`);
        throw error; // This will trigger a retry
      }

      // Non-transient errors should not be retried
      console.error(`Non-transient error in askClaude: ${error.message}`);
      return {
        stdout: error.stdout || '',
        stderr: error.stderr || error.message,
        success: false,
        error: error.message
      };
    }
  }, {
    retries: 3,
    minTimeout: 1000,
    maxTimeout: 5000,
    onFailedAttempt: (error) => {
      console.log(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`);
    }
  });
}

/**
 * Checks if an error is transient and should be retried.
 * @param {Error} error The error to check.
 * @returns {boolean} True if the error is transient.
 */
export function isTransientError(error) {
  const errorMessage = (error.message || '').toLowerCase();
  const stderr = (error.stderr || '').toLowerCase();
  const stdout = (error.stdout || '').toLowerCase();
  const allOutput = `${errorMessage} ${stderr} ${stdout}`;

  // Network-related errors
  const networkErrors = [
    'network',
    'timeout',
    'connection',
    'econnreset',
    'enotfound',
    'econnrefused',
    'socket hang up'
  ];

  // Quota/rate limit errors
  const quotaErrors = [
    'quota exceeded',
    'rate limit',
    'too many requests',
    'service unavailable',
    'temporarily unavailable',
    '429',
    '500',
    '502',
    '503',
    '504'
  ];

  const transientPatterns = [...networkErrors, ...quotaErrors];

  return transientPatterns.some(pattern => allOutput.includes(pattern));
}

--- File: /src/services/claudeCliService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { askClaude } from './claudeCliService.js';

// Mock execa
vi.mock('execa', () => ({
  execa: vi.fn()
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', () => ({
  default: async (fn, options) => {
    // For tests, we'll execute the function directly without retries
    return await fn();
  }
}));

describe('claudeCliService', () => {
  let mockExeca;

  beforeEach(async () => {
    const { execa } = await import('execa');
    mockExeca = execa;
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.resetAllMocks();
  });

  describe('askClaude', () => {
    it('should successfully execute gemini-cli claude command', async () => {
      const mockResponse = {
        stdout: '{"result": "test response", "success": true}',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith('gemini-cli', ['claude', 'test prompt'], {
        timeout: 120000
      });
      expect(result).toEqual({
        stdout: mockResponse.stdout,
        stderr: mockResponse.stderr,
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle non-transient errors without retry', async () => {
      const mockError = new Error('EACCES: permission denied');
      mockError.code = 'EACCES';
      mockError.stdout = '';
      mockError.stderr = 'permission denied';

      mockExeca.mockRejectedValue(mockError);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: 'permission denied',
        success: false,
        error: 'EACCES: permission denied'
      });
    });

    it('should identify transient network errors', async () => {
      const mockError = new Error('Connection timeout');
      mockError.stdout = '';
      mockError.stderr = 'network timeout occurred';

      const { isTransientError } = await import('./claudeCliService.js');

      expect(isTransientError(mockError)).toBe(true);
    });


    it('should handle JSON parsing in gemini-cli response', async () => {
      const complexJsonResponse = {
        stdout: JSON.stringify({
          type: 'result',
          data: {
            analysis: 'test analysis',
            metrics: { tokens: 100, cost: 0.05 }
          },
          timestamp: new Date().toISOString()
        }),
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(complexJsonResponse);

      const result = await askClaude('analyze this code');

      expect(result.success).toBe(true);
      expect(result.stdout).toContain('test analysis');
      expect(result.stdout).toContain('tokens');
    });

    it('should handle empty responses gracefully', async () => {
      const mockResponse = {
        stdout: '',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: '',
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle stderr warnings without failing', async () => {
      const mockResponse = {
        stdout: '{"result": "success"}',
        stderr: 'Warning: deprecated feature used',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result.success).toBe(true);
      expect(result.stderr).toContain('deprecated feature');
    });

    it('should respect timeout configuration', async () => {
      mockExeca.mockResolvedValue({
        stdout: 'response',
        stderr: '',
        exitCode: 0
      });

      await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith(
        'gemini-cli',
        ['claude', 'test prompt'],
        { timeout: 120000 }
      );
    });

    it('should parse mcp_feedback from JSON prompt', async () => {
      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithFeedback = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: true,
              errors: [],
              mcp_version: '1.0'
            }
          }
        }
      });

      const result = await askClaude(promptWithFeedback);

      expect(result.mcp_feedback).toEqual({
        success: true,
        errors: [],
        mcp_version: '1.0'
      });
    });

    it('should log warning when mcp_feedback contains errors', async () => {
      const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithErrors = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: false,
              errors: ['Error 1', 'Error 2'],
              mcp_version: '1.0'
            }
          }
        }
      });

      await askClaude(promptWithErrors);

      expect(consoleSpy).toHaveBeenCalledWith('MCP feedback contains errors:', ['Error 1', 'Error 2']);

      consoleSpy.mockRestore();
    });
  });

  describe('transient error detection', () => {
    it('should treat network errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const networkErrors = [
        'network error',
        'timeout',
        'connection refused',
        'ECONNRESET',
        'ENOTFOUND',
        'socket hang up'
      ];

      networkErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should treat quota errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const quotaErrors = [
        'quota exceeded',
        'rate limit',
        'too many requests',
        '429',
        '503'
      ];

      quotaErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should not treat permission errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const permanentErrors = [
        'EACCES: permission denied',
        'Invalid API key',
        'Authentication failed'
      ];

      permanentErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(false);
      });
    });
  });
});

--- File: /src/services/dispatcherService.js ---

import { ask as askGpt } from './gptService.js';
import { executePrompt as askClaude } from './claudeCliService.js';

/**
 * Dispatches an analytical task to the most efficient AI model with a fallback.
 * Priority 1: Codex (GPT) with low reasoning for speed and cost.
 * Priority 2: Claude as a reliable fallback.
 * @param {string} prompt The JSON payload or prompt string for the task.
 * @returns {Promise<object>} The result from the successful AI agent.
 */
export async function dispatchAnalysisTask(prompt) {
  try {
    console.log('üß† Dispatcher: Attempting analysis with Codex (low reasoning)...');
    const gptOptions = {
      model: 'gpt-5-codex',
      reasoning: 'low'
    };
    // The 'ask' function expects payload as first arg, and options as second.
    // Since prompt is a string here, we wrap it in an object for consistency if needed,
    // but for simple prompts it can often be passed directly.
    const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
    return await askGpt(payload, { verbose: false, ...gptOptions });
  } catch (gptError) {
    console.warn(`‚ö†Ô∏è Codex (low reasoning) failed: ${gptError.message}`);
    console.log('üîÑ Failing over to Claude for analysis...');
    try {
      return await askClaude(prompt);
    } catch (claudeError) {
      console.error(`‚ùå Critical Failure: Both Codex and Claude failed for analysis task.`);
      throw new Error(`Primary (Codex) Error: ${gptError.message}\nFallback (Claude) Error: ${claudeError.message}`);
    }
  }
}

--- File: /src/services/embedding.js ---

import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';
import pLimit from 'p-limit';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "embedding-001" });

async function generateEmbedding(text, taskType = 'RETRIEVAL_DOCUMENT') {
  try {
    const result = await model.embedContent({ 
      content: { parts: [{ text }] },
      taskType
    });
    return result.embedding.values;
  } catch (error) {
    console.error('‚ùå Gemini Embedding Error:', error.message);
    throw error;
  }
}

async function generateBatchEmbeddings(segments, taskType = 'RETRIEVAL_DOCUMENT') {
    if (segments.length === 0) return [];

    const BATCH_COUNT_LIMIT = 100;
    const BATCH_SIZE_LIMIT = 3000000; // 3MB for safety
    const allBatches = [];
    let currentBatch = [];
    let currentBatchSize = 0;

    for (const segment of segments) {
        const segmentSize = Buffer.byteLength(segment.content, 'utf8');
        if (segmentSize > BATCH_SIZE_LIMIT) { // Handle single oversized segments
            console.log(chalk.yellow(`  -> Warning: Segment '${segment.name}' in '${segment.filePath}' is oversized and will be truncated.`));
            segment.content = segment.content.substring(0, 20000) + '... [truncated]'; // Truncate oversized segment
        }
        if (currentBatch.length > 0 && (currentBatch.length >= BATCH_COUNT_LIMIT || currentBatchSize + Buffer.byteLength(segment.content, 'utf8') > BATCH_SIZE_LIMIT)) {
            allBatches.push(currentBatch);
            currentBatch = [];
            currentBatchSize = 0;
        }
        currentBatch.push(segment);
        currentBatchSize += Buffer.byteLength(segment.content, 'utf8');
    }
    if (currentBatch.length > 0) {
        allBatches.push(currentBatch);
    }

    console.log(chalk.cyan(`‚è≥ Generating embeddings for ${segments.length} segments, divided into ${allBatches.length} safe chunks...`));

    const limit = pLimit(5); // Set concurrency to 5 parallel requests
    let processedCount = 0;

    const promises = allBatches.map((batch, i) => {
        return limit(async () => {
            const batchSizeKB = (Buffer.byteLength(batch.map(s => s.content).join(''), 'utf8') / 1024).toFixed(2);
            console.log(chalk.blue(`  -> Sending chunk ${i + 1}/${allBatches.length} (${batch.length} segments, ${batchSizeKB} KB)...`));
            try {
                const contents = batch.map(s => ({ parts: [{ text: s.content }] }));
                const result = await model.batchEmbedContents({ 
                    requests: contents.map(content => ({ content, taskType }))
                });
                processedCount++;
                console.log(chalk.green(`  <- Chunk ${i + 1}/${allBatches.length} processed successfully.`));
                return result.embeddings.map(e => e.values);
            } catch (error) {
                console.error(chalk.red(`‚ùå Gemini Batch Embedding Error (Chunk ${i + 1}/${allBatches.length}):`), error.message);
                throw error;
            }
        });
    });

    const chunkResults = await Promise.all(promises);
    const allEmbeddings = chunkResults.flat();

    console.log(chalk.green.bold('‚úÖ Batch embeddings generated successfully.'));
    return allEmbeddings;
}

export const embeddingService = {
  generateEmbedding,
  generateBatchEmbeddings
};

--- File: /src/services/embeddingService.js ---

import { pipeline } from '@xenova/transformers';

class EmbeddingService {
    static instance = null;
    static modelName = 'Xenova/jina-embeddings-v2-base-en'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = await pipeline('feature-extraction', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = null;
            // In Node.js, there's no explicit GPU memory release, 
            // relying on the garbage collector is the standard way.
        }
    }
}

export async function generateEmbedding(code) {
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(code, { pooling: 'mean', normalize: true });
    return Array.from(result.data);
}

export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) return [];
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(texts, { pooling: 'mean', normalize: true });
    
    // Convert tensor to array of arrays
    const embeddings = [];
    for (let i = 0; i < result.dims[0]; i++) {
        const start = i * result.dims[1];
        const end = start + result.dims[1];
        embeddings.push(Array.from(result.data.slice(start, end)));
    }
    return embeddings;
}

export const releaseModel = EmbeddingService.releaseModel;

--- File: /src/services/gptService.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import pRetry from 'p-retry';
import ora from 'ora';
import { loadProjectEckManifest } from '../utils/fileUtils.js';
import { initiateLogin } from './authService.js';
import which from 'which';

const SYSTEM_PROMPT = 'You are a Coder agent. Apply code changes per JSON spec. Respond only in JSON: {success: bool, changes: array, errors: array, post_steps: object}';

class AuthError extends Error {
  constructor(message) {
    super(message);
    this.name = 'AuthError';
  }
}

/**
 * Checks if the codex CLI tool is available in the system's PATH.
 * Throws an error if not found.
 */
async function ensureCodexCliExists() {
  try {
    await which('codex');
  } catch (error) {
    throw new Error('The `codex` CLI tool is not installed or not in your PATH. Please install it from https://github.com/openai/codex to use this command.');
  }
}

/**
 * Delegates an apply_code_changes payload to the codex CLI with auto-login.
 * @param {string|object} payload - JSON string or object payload to forward to the agent.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} [options]
 * @returns {Promise<object>}
 */
export async function ask(payload, options = {}) {
  const { verbose = false, model = 'gpt-5-codex', reasoning = 'high' } = options;
  await ensureCodexCliExists();

  const run = async () => {
    const spinner = verbose ? null : ora('Sending payload to Codex agent...').start();
    try {
      const payloadObject = await parsePayload(payload);
      const manifest = await loadProjectEckManifest(process.cwd());
      const userPrompt = buildUserPrompt(payloadObject, manifest);
      const promptInput = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const args = [
        'exec',
        // Use full-auto mode to prevent interactive prompts from the agent,
        // as this service is designed for non-interactive delegation.
        '--full-auto',
        '--model', model,
        '-c', `model_reasoning_effort=${reasoning}`
      ];

      debug(verbose, `Executing: codex ${args.join(' ')} <stdin>`);

      const cliResult = await execa('codex', args, {
        cwd: process.cwd(),
        timeout: 300000, // 5-minute timeout
        input: promptInput // Stream large prompts via stdin to avoid argv limits
      });

      const output = cliResult?.stdout?.trim();
      if (!output) {
        throw new Error('codex CLI returned empty response');
      }

      const parsed = extractFinalJson(output);
      if (parsed) {
        if (parsed.post_steps || parsed.post_execution_steps) {
          const postSteps = parsed.post_steps || parsed.post_execution_steps;
          await handlePostExecutionSteps(postSteps, payloadObject);
          parsed.mcp_feedback = postSteps?.mcp_feedback || null;
        }
        spinner?.succeed('Codex agent completed the task.');
        return parsed;
      }

      // If parsing fails, surface the raw response text for upstream handling.
      spinner?.succeed('Codex agent completed the task.');
      return { success: true, changes: [], errors: [], response_text: output };

    } catch (error) {
        spinner?.fail('Codex execution failed.');
        handleCliError(error); // This will throw a specific error type
    }
  };

  return pRetry(run, {
    retries: 1, // Only retry once after a successful login
    minTimeout: 0,
    onFailedAttempt: async (error) => {
      if (error.name === 'AuthError') {
        await initiateLogin();
      } else {
        throw error; // Don't retry for other errors, fail immediately
      }
    }
  });
}


async function parsePayload(payload) {
  if (typeof payload === 'string') {
    try {
      return JSON.parse(payload);
    } catch (error) {
      throw new Error(`Failed to parse payload JSON: ${error.message}`);
    }
  }
  if (typeof payload === 'object' && payload !== null) {
    return payload;
  }
  throw new Error('Invalid payload type. Expected JSON string or object.');
}

function buildUserPrompt(payloadObject, manifest) {
  const payloadString = JSON.stringify(payloadObject);
  if (!manifest) {
    return payloadString;
  }

  const sections = [];
  if (manifest.context) {
    sections.push('## .eck Context\n' + manifest.context);
  }
  if (manifest.operations) {
    sections.push('## .eck Operations\n' + manifest.operations);
  }
  if (manifest.journal) {
    sections.push('## .eck Journal\n' + manifest.journal);
  }
  if (manifest.environment && Object.keys(manifest.environment).length > 0) {
    sections.push('## .eck Environment\n' + JSON.stringify(manifest.environment, null, 2));
  }

  if (sections.length === 0) {
    return payloadString;
  }

  return `${payloadString}\n\n# Project Context\n${sections.join('\n\n')}`;
}

function debug(verbose, message) {
  if (verbose) {
    console.log(`[ask-gpt] ${message}`);
  }
}

function extractFinalJson(text) {
  const trimmed = text?.trim();
  if (!trimmed) {
    return null;
  }

  try {
    return JSON.parse(trimmed);
  } catch (error) {
    // Continue with fallback parsing when logs precede the JSON payload.
  }

  const fencedMatch = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fencedContent = fencedMatch[1].trim();
    try {
      return JSON.parse(fencedContent);
    } catch (error) {
      // Ignore and fall through to final brace search.
    }
  }

  const lastBraceIndex = trimmed.lastIndexOf('{');
  if (lastBraceIndex === -1) {
    return null;
  }

  const jsonCandidate = trimmed.slice(lastBraceIndex);
  try {
    return JSON.parse(jsonCandidate);
  } catch (error) {
    return null;
  }
}

function handleCliError(error) {
  const combined = `${error?.message || ''} ${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
  // Check for text that `codex` outputs when auth is missing.
  if (combined.includes('authentication is required') || combined.includes('please run `codex login`')) {
    const authError = new Error('Codex authentication is required. Attempting to log in.');
    authError.name = 'AuthError';
    throw authError;
  }

  throw new Error(`codex CLI failed: ${error.stderr || error.message}`);
}

async function handlePostExecutionSteps(postSteps, payloadObject) {
  if (!postSteps || typeof postSteps !== 'object') {
    return;
  }

  if (postSteps.journal_entry) {
    await applyJournalEntry(postSteps.journal_entry, payloadObject);
  }

  if (postSteps.mcp_feedback) {
    logMcpFeedback(postSteps.mcp_feedback);
  }
}

async function applyJournalEntry(entry, payloadObject) {
  const journalEntry = normalizeJournalEntry(entry);
  const journalPath = path.join(process.cwd(), '.eck', 'JOURNAL.md');

  await fs.mkdir(path.dirname(journalPath), { recursive: true });

  let existing = '';
  try {
    existing = await fs.readFile(journalPath, 'utf-8');
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw new Error(`Failed to read JOURNAL.md: ${error.message}`);
    }
  }

  const taskId = payloadObject?.task_id || payloadObject?.payload?.task_id || journalEntry.task_id || 'ask-gpt';
  const isoDate = new Date().toISOString();

  const frontmatter = [
    '---',
    `task_id: ${taskId}`,
    `date: ${isoDate}`,
    `type: ${journalEntry.type}`,
    `scope: ${journalEntry.scope}`,
    '---',
    ''
  ].join('\n');

  const summary = journalEntry.summary ? `## ${journalEntry.summary}\n` : '';
  const details = journalEntry.details ? `${journalEntry.details}\n` : '';

  const entryBlock = `${frontmatter}${summary ? `${summary}\n` : ''}${details}\n`;

  const existingTrimmed = existing ? existing.replace(/^\n+/, '') : '';
  const newContent = `${entryBlock}${existingTrimmed}`.replace(/\n{3,}/g, '\n\n');

  await fs.writeFile(journalPath, newContent.trimEnd() + '\n');

  await stageJournal(journalPath);
  await commitJournal(journalEntry);
}

function normalizeJournalEntry(entry) {
  return {
    type: entry.type || 'chore',
    scope: entry.scope || 'journal',
    summary: entry.summary || 'Update journal entry',
    details: entry.details || ''
  };
}

async function stageJournal(journalPath) {
  const relativePath = path.relative(process.cwd(), journalPath);
  try {
    await execa('git', ['add', relativePath], { cwd: process.cwd() });
  } catch (error) {
    throw new Error(`Failed to stage journal entry: ${error.message}`);
  }
}

async function commitJournal(entry) {
  const scopePart = entry.scope ? `(${entry.scope})` : '';
  const summary = (entry.summary || 'Update journal entry').replace(/\s+/g, ' ').trim();
  const commitMessage = `${entry.type}${scopePart}: ${summary}`;

  try {
    await execa('git', ['commit', '-m', commitMessage], { cwd: process.cwd() });
  } catch (error) {
    const text = `${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
    if (text.includes('nothing to commit')) {
      console.warn('Journal entry already committed or no changes to commit.');
      return;
    }
    throw new Error(`Failed to commit journal entry: ${error.message}`);
  }
}

function logMcpFeedback(feedback) {
  if (!feedback) {
    return;
  }

  const errors = Array.isArray(feedback.errors) ? feedback.errors : [];
  if (!feedback.success || errors.length > 0) {
    console.warn('MCP feedback indicates issues:', errors.length > 0 ? errors : feedback);
  } else {
    console.log('MCP feedback:', feedback);
  }
}


--- File: /src/services/gptService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

// Mock execa and which
vi.mock('execa', () => ({ execa: vi.fn() }));
vi.mock('which', () => ({ default: vi.fn() }));

// Mock fs/promises for journal entries
const mkdirMock = vi.fn();
const readFileMock = vi.fn();
const writeFileMock = vi.fn();
const loadProjectEckManifestMock = vi.fn();
vi.mock('fs/promises', () => ({
  mkdir: mkdirMock,
  readFile: readFileMock,
  writeFile: writeFileMock
}));
vi.mock('../utils/fileUtils.js', () => ({
  loadProjectEckManifest: loadProjectEckManifestMock
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', async (importOriginal) => {
  const actual = await importOriginal();
  return {
    ...actual,
    default: vi.fn(async (fn, options) => {
      try {
        return await fn();
      } catch (error) {
        if (options.onFailedAttempt) {
          await options.onFailedAttempt(error);
          // In a real scenario, p-retry would re-run fn. For testing, we simulate one retry.
          if (error.name === 'AuthError') {
             return await fn();
          }
        }
        throw error;
      }
    })
  };
});

// Mock the authService
vi.mock('./authService.js', () => ({
  initiateLogin: vi.fn()
}));

describe('gptService with codex CLI', () => {
  let ask;
  let execaMock;
  let whichMock;
  let initiateLoginMock;

  beforeEach(async () => {
    vi.clearAllMocks();

    ({ execa: execaMock } = await import('execa'));
    const which = (await import('which')).default;
    whichMock = which;
    ({ initiateLogin: initiateLoginMock } = await import('./authService.js'));
    ({ ask } = await import('./gptService.js'));

    whichMock.mockResolvedValue('/usr/bin/codex');
    loadProjectEckManifestMock.mockResolvedValue(null);
  });

  it('should call codex CLI with correct arguments and parse final JSON from noisy output', async () => {
    const codexLogs = '[2025-10-06 20:04:22] OpenAI Codex v0.42.0\nSome setup log...\n\n{"success": true, "changes": ["change1"], "errors": []}';
    execaMock.mockResolvedValue({ stdout: codexLogs });

    const payload = { objective: 'Test' };
    const result = await ask(payload);

    expect(result).toEqual({ success: true, changes: ['change1'], errors: [] });
    expect(execaMock).toHaveBeenCalledWith('codex', expect.arrayContaining(['exec', '--full-auto', '--model']), expect.any(Object));
    const [, , options] = execaMock.mock.calls[0];
    expect(options.input).toContain(JSON.stringify(payload));
  });

  it('should trigger login flow on authentication error and retry', async () => {
    const authError = new Error('Authentication is required. Please run `codex login`.');
    authError.name = 'AuthError'; // Custom error name to trigger retry
    authError.stderr = 'Authentication is required. Please run `codex login`.';

    const successResponse = {
      id: 'task2',
      msg: {
        type: 'task_complete',
        last_agent_message: '{"success": true}'
      }
    };

    // First call fails, second call (retry) succeeds
    execaMock
      .mockRejectedValueOnce(authError)
      .mockResolvedValueOnce({ stdout: JSON.stringify(successResponse) });

    initiateLoginMock.mockResolvedValue();

    const result = await ask({ objective: 'Retry test' });

    expect(result).toEqual({ success: true });
    expect(initiateLoginMock).toHaveBeenCalledTimes(1);
    expect(execaMock).toHaveBeenCalledTimes(2); // Initial call + retry
  });

  it('should throw if codex CLI is not found', async () => {
    whichMock.mockRejectedValue(new Error('not found'));
    await expect(ask({})).rejects.toThrow('The `codex` CLI tool is not installed');
  });

  it('should throw non-auth errors immediately without retry', async () => {
    const otherError = new Error('Some other CLI error');
    otherError.stderr = 'Something else went wrong';
    execaMock.mockRejectedValueOnce(otherError);

    await expect(ask({})).rejects.toThrow('codex CLI failed: Something else went wrong');
    expect(initiateLoginMock).not.toHaveBeenCalled();
  });
});


--- File: /src/templates/agent-prompt.template.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

--- File: /src/templates/architect-prompt.template.md ---

# AI Architect Instructions

You are an autonomous AI Architect. Your primary goal is to develop and evolve a software project by planning high-level architecture and delegating implementation tasks to an execution agent named Claude.

## Core Workflow: The Thought-Tool-Observation Loop

Your entire operational process follows a strict loop:
1.  **Thought:** Analyze the user's request, the current state of the project, and previous observations. Formulate a plan and decide on the next immediate action. You must explain your reasoning and your chosen action in plain text.
2.  **Tool:** Immediately after your thought process, you MUST issue a command to either the local `eck-snapshot` environment or the `claude_code_agent`.
3.  **Observation:** After issuing a command, you MUST STOP and wait for an `Observation:` message from the system, which will contain the result of your command. Do not proceed until you receive it.

## Commanding the Execution Agent (Claude)

To delegate any coding task (writing, editing, testing, refactoring), you MUST generate a JSON command block for the `claude_code_agent`. This is your primary method of modifying the codebase.

**JSON Command Format:**
```json
{
  "target_agent": "claude_code_agent",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "A brief, clear task description for Claude.",
    "context": "Explain why this change is needed and any relevant architectural context.",
    "files_to_modify": [
      {
        "path": "exact/path/to/file.js",
        "action": "add | modify | replace | delete",
        "location": "line numbers, function name, or a unique search pattern",
        "details": "Precise, step-by-step instructions for Claude to implement."
      }
    ]
  }
}
```

## Interacting with the Local Environment

To understand the project state, you can command the `eck-snapshot` tool directly. Use this for discovery, analysis, and managing project context.

**Tool Command Format:** `[tool_code: eck-snapshot <command> <options>]`

**Available Commands:**
- `eck-snapshot snapshot`: To create a new snapshot of the current state.
- `eck-snapshot query "<question>"`: To search the codebase.
- `eck-snapshot detect`: To analyze the project structure.
- `eck-snapshot restore <snapshot_file> --include ...`: To view specific files from a snapshot.

## Final Mandate

Your existence is defined by this loop. Think, act by issuing a tool command, and then wait for the observation. This is the only way you can make progress.

--- File: /src/templates/envScanRequest.md ---

Run this command and return the output:
```
node -e "console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))"
```

--- File: /src/templates/gitWorkflow.md ---

## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

--- File: /src/templates/multiAgent.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** {{stats.includedFiles}}
- **Total Files in Repo:** {{stats.totalFiles}}

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

{{projectOverview}}

{{eckManifestSection}}

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (`gemini_wsl`), who has a detailed (`_ja.md`) snapshot and the ability to command a **Coder** agent (`claude`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (`gemini_wsl`):** Receives strategic tasks, analyzes the `_ja.md` snapshot, breaks the task down, and commands the Coder.
  - **Coder (`claude`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while `gemini_wsl` can use its own tools for analysis, validation, and running shell commands.

### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`) - LOW-LEVEL EXECUTION:**
Use `apply_code_changes` for simple, direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**2. For Junior Architects (`gemini_wsl`) - HIGH-LEVEL DELEGATION:**
Use `execute_strategic_task` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

```json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to `routes/api.js`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

{{agentDefinitions}}



--- File: /src/templates/vectorMode.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: "{{userQuery}}"

* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.
* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** {{repoName}}
- **User Query:** "{{userQuery}}"
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

{{multiAgentSection}}

---

--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig, getAllProfiles } from '../config.js';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Helper function to read a template file or return the string if it's not a path
    const loadTemplate = async (templatePathOrString) => {
      if (templatePathOrString && (templatePathOrString.endsWith('.md') || templatePathOrString.endsWith('.txt'))) {
        try {
          // Resolve path relative to the project root. __dirname is src/utils.
          const resolvedPath = path.join(__dirname, '..', '..', templatePathOrString);
          return await fs.readFile(resolvedPath, 'utf-8');
        } catch (e) {
          return `ERROR: FAILED TO LOAD TEMPLATE ${templatePathOrString}: ${e.message}`;
        }
      }
      return templatePathOrString; // Fallback for old-style inline strings or errors
    };

    // P1 Bug Fix: Normalize manifest structure as per Consilium report
    function normalizeManifest(raw) {
      if (!raw) return null;
      const out = {};
      // Handle `setup.json` structure (e.g., `projectContext.name`)
      if (raw.projectContext) {
        out.context = raw.projectContext.description || JSON.stringify(raw.projectContext, null, 2);
        out.operations = raw.operations || raw.projectContext.operations || ''; // Assuming .eck/OPERATIONS.md is separate
        out.journal = raw.journal || raw.projectContext.journal || ''; // Assuming .eck/JOURNAL.md is separate
        out.environment = raw.environment || raw.projectContext.environment || {}; // Assuming .eck/ENVIRONMENT.md is separate
      } else {
        // Handle direct .eck file structure (e.g., raw.context from CONTEXT.md)
        out.context = raw.context || '';
        out.operations = raw.operations || '';
        out.journal = raw.journal || '';
        out.environment = raw.environment || {};
      }
      // Add fallback text if still empty
      if (!out.context) out.context = 'No project context provided.';
      if (!out.operations) out.operations = 'No operations guide provided.';
      if (!out.journal) out.journal = 'No journal entries found.';

      return out;
    }

    // --- Build common context sections --- 
    const projectOverview = `### PROJECT OVERVIEW
- **Project:** ${context.repoName || 'Unknown'}
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.
`;
    const normalizedEck = normalizeManifest(context.eckManifest);
    let eckManifestSection = '';
    if (normalizedEck) {
      eckManifestSection = buildEckManifestSection(normalizedEck);
    } else {
      eckManifestSection = '### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nWARNING: .eck manifest was not found or was empty.\n';
    }
    // --- End context building ---


    // Check if agent mode is enabled
    if (context.options && context.options.agent) {
      const agentPromptTemplate = await loadTemplate(promptTemplates.agent);

      const agentHeader = `${agentPromptTemplate}

${projectOverview}
${eckManifestSection}
---

## Project Snapshot Information

- **Project**: ${context.repoName || 'Unknown'}
- **Timestamp**: ${new Date().toISOString()}
- **Files Included**: ${context.stats ? context.stats.includedFiles : 'Unknown'}
- **Total Files in Repo**: ${context.stats ? context.stats.totalFiles : 'Unknown'}

---

`;
      return agentHeader;
    }

    // --- This is the main/Senior Architect prompt logic --- 
    let template;
    if (context.mode === 'vector') {
      template = await loadTemplate(promptTemplates.vectorMode);
      // Inject context for vector mode
      template = template.replace('{{multiAgentSection}}', `
${projectOverview}
${eckManifestSection}
`); 
    } else {
      template = await loadTemplate(promptTemplates.multiAgent);
      // --- INJECT DYNAMIC CONTEXT --- 
      template = template.replace('{{projectOverview}}', projectOverview);
      template = template.replace('{{eckManifestSection}}', eckManifestSection);
      // --- END INJECT --- 
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions
    };

    let renderedTemplate = render(template, data);
    
    // Inject dynamic profile context if a profile is active
    if (context.options && context.options.profile && context.repoPath) {
      let metadataHeader = '\n\n## Partial Snapshot Context\n';
      metadataHeader += `- **Profile(s) Active:** ${context.options.profile}\n`;
      try {
          const allProfiles = await getAllProfiles(context.repoPath);
          const activeProfileNames = context.options.profile.split(',').map(p => p.trim().replace(/^-/, ''));
          const allProfileNames = Object.keys(allProfiles).filter(p => !activeProfileNames.includes(p));
          if (allProfileNames.length > 0) {
               metadataHeader += `- **Other Available Profiles:** ${allProfileNames.join(', ')}\n`;
          }
      } catch (e) { /* fail silently on metadata generation */ }
      
      const insertMarker = "### HIERARCHICAL AGENT WORKFLOW"; // Use our new marker
      renderedTemplate = renderedTemplate.replace(insertMarker, metadataHeader + '\n' + insertMarker);
    }

    return renderedTemplate;

  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header', error.message);
    return `# Snapshot for ${context.repoName || 'Project'}

Generated: ${new Date().toISOString()}

---

`;
  }
}

--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { dispatchAnalysisTask } from '../services/dispatcherService.js';
import { getProfile } from '../config.js';
import micromatch from 'micromatch';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = '.eck/';
  const comment = '# Added by eck-snapshot to ignore metadata directory';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

// Helper function to determine if a string is a glob pattern
function isGlob(str) {
  return str.includes('*') || str.includes('?') || str.includes('{');
}

/**
 * Applies advanced profile filtering (multi-profile, exclusion, and ad-hoc globs) to a file list.
 */
export async function applyProfileFilter(allFiles, profileString, repoPath) {
  const profileParts = profileString.split(',').map(p => p.trim()).filter(Boolean);
  
  const includeGlobs = [];
  const excludeGlobs = [];
  const includeNames = [];
  const excludeNames = [];

  // Step 1: Differentiate between profile names and ad-hoc glob patterns
  for (const part of profileParts) {
    const isNegative = part.startsWith('-');
    const pattern = isNegative ? part.substring(1) : part;

    if (isGlob(pattern)) {
      if (isNegative) {
        excludeGlobs.push(pattern);
      } else {
        includeGlobs.push(pattern);
      }
    } else {
      if (isNegative) {
        excludeNames.push(pattern);
      } else {
        includeNames.push(pattern);
      }
    }
  }

  let workingFiles = [];
  let finalIncludes = [...includeGlobs];
  let finalExcludes = [...excludeGlobs];

  // Step 2: Load patterns from specified profile names
  const allProfileNames = [...new Set([...includeNames, ...excludeNames])];
  const profiles = new Map();
  for (const name of allProfileNames) {
    const profile = await getProfile(name, repoPath);
    if (profile) {
      profiles.set(name, profile);
    } else {
      // This is an ad-hoc glob, not a profile, so no warning is needed.
      if (!isGlob(name)) {
        console.warn(`‚ö†Ô∏è Warning: Profile '${name}' not found and will be skipped.`);
      }
    }
  }

  for (const name of includeNames) {
    if (profiles.has(name)) {
      finalIncludes.push(...(profiles.get(name).include || []));
      finalExcludes.push(...(profiles.get(name).exclude || []));
    }
  }
  for (const name of excludeNames) {
    if (profiles.has(name)) {
      finalExcludes.push(...(profiles.get(name).include || []));
    }
  }
  
  // Step 3: Apply the filtering logic
  if (finalIncludes.length > 0) {
    workingFiles = micromatch(allFiles, finalIncludes);
  } else if (includeNames.length > 0 && includeGlobs.length === 0) {
    workingFiles = [];
  } else {
    workingFiles = allFiles;
  }

  if (finalExcludes.length > 0) {
    workingFiles = micromatch.not(workingFiles, finalExcludes);
  }

  return workingFiles;
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');
  
  try {
    // Check if .eck directory already exists and has all required files
    let needsInitialization = false;
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory exists, check if all required files are present
        const requiredFiles = ['ENVIRONMENT.md', 'CONTEXT.md', 'OPERATIONS.md', 'JOURNAL.md'];
        for (const fileName of requiredFiles) {
          try {
            await fs.stat(path.join(eckDir, fileName));
          } catch (error) {
            console.log(`   ‚ÑπÔ∏è Missing ${fileName}, initialization needed`);
            needsInitialization = true;
            break;
          }
        }
        if (!needsInitialization) {
          // All files exist, no need to initialize
          return;
        }
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
      needsInitialization = true;
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');

    // --- NEW HYBRID LOGIC --- 
    // 1. Run static analysis first to gather facts.
    let staticFacts = {};
    try {
      staticFacts = await detectProjectType(projectPath);
      console.log(`   üîç Static analysis complete. Detected type: ${staticFacts.type}`);
    } catch (e) {
      console.warn(`   ‚ö†Ô∏è Static project detection failed: ${e.message}. Proceeding with generic prompts.`);
    }

    // Prevent AI hallucination by removing low-confidence "other possibilities"
    if (staticFacts && staticFacts.allDetections) {
      delete staticFacts.allDetections;
    }
    
    const staticFactsJson = JSON.stringify(staticFacts, null, 2);
    // --- END NEW LOGIC ---
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'ENVIRONMENT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw YAML key-value content for an .eck/ENVIRONMENT.md file. Only include detected facts. DO NOT add any keys that are not present in the facts. DO NOT add conversational text or markdown wrappers. Your response MUST start directly with a YAML key (e.g., 'project_type: ...').`,
        content: `# This file is for environment overrides. Add agent-specific settings here.\nagent_id: local_dev\n` // Simple static fallback
      },
      {
        name: 'CONTEXT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/CONTEXT.md file. Use the facts to write ## Description, ## Architecture, and ## Key Technologies. DO NOT add conversational text (like "Here is the file..."). Your response MUST start *directly* with the '# Project Overview' heading.`,
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: `Given these static project analysis facts (especially package.json scripts):
${staticFactsJson}

Generate the raw Markdown content ONLY for a .eck/OPERATIONS.md file. DO NOT add conversational text. Your response MUST start *directly* with the '# Common Operations' heading. List commands for ## Development Setup, ## Running the Project, and ## Testing.`,
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/ROADMAP.md file. DO NOT add conversational text. Start *directly* with '# Project Roadmap'. Propose 1-2 *plausible* placeholder items for ## Current Sprint/Phase and ## Next Phase based on the project type.`,
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        prompt: `Generate the raw Markdown content ONLY for a .eck/TECH_DEBT.md file. DO NOT add conversational text. Start *directly* with '# Technical Debt'. Propose 1-2 *common* placeholder items for ## Code Quality Issues and ## Refactoring Opportunities.`,
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file (only if it doesn't exist)
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      
      // Skip if file already exists
      try {
        await fs.stat(filePath);
        console.log(`   ‚úÖ ${file.name} already exists, skipping`);
        continue;
      } catch (error) {
        // File doesn't exist, create it
      }
      
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For files with a prompt, try to dynamically generate
      if (file.prompt) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponseObject = await dispatchAnalysisTask(file.prompt); // Use the prompt
          const rawText = aiResponseObject.result || aiResponseObject.response_text; // Handle both Claude and GPT responses
          
          if (!rawText || typeof rawText.replace !== 'function') {
             throw new Error(`AI returned invalid content type: ${typeof rawText}`);
          }

          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = rawText.replace(/^```(markdown|yaml)?\n|```$/g, '').trim();
          
          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}


--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));

  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }

  const best = detections[0];

  // Special handling for mixed monorepos
  const isLikelyMonorepo = detections.length > 1 && detections.some(d => d.score >= 40);

  if (isLikelyMonorepo) {
    // If we have multiple strong detections, prefer the highest priority with substantial evidence
    const strongDetections = detections.filter(d => d.score >= 40);
    if (strongDetections.length > 1) {
      const primaryType = strongDetections[0].type;
      return {
        type: primaryType,
        confidence: Math.min(strongDetections[0].score / 100, 1.0),
        details: {
          ...strongDetections[0].details,
          isMonorepo: true,
          additionalTypes: strongDetections.slice(1).map(d => d.type)
        },
        allDetections: detections
      };
    }
  }

  // Boost confidence for strong workspace indicators
  if (best.details && (best.details.isWorkspace || best.details.workspaceSize)) {
    const boostedScore = best.score + 20; // Bonus for workspace structure
    return {
      type: best.type,
      confidence: Math.min(boostedScore / 100, 1.0),
      details: best.details,
      allDetections: detections
    };
  }

  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;

  // Check for required files (check both root and common subdirectories)
  if (pattern.files) {
    for (const file of pattern.files) {
      // Check in root directory first
      const rootExists = await fileExists(path.join(projectPath, file));
      if (rootExists) {
        score += 25; // Each required file adds points
      } else {
        // For Cargo.toml and other project files, also check common subdirectory patterns
        const commonSubdirs = ['src', 'lib', 'app', 'core', 'backend', 'frontend'];
        // Add project-type specific subdirectories
        if (file === 'Cargo.toml') {
          commonSubdirs.push('codex-rs', 'rust', 'server', 'api');
        }
        if (file === 'package.json') {
          commonSubdirs.push('codex-cli', 'cli', 'client', 'web', 'ui');
        }

        for (const subdir of commonSubdirs) {
          const subdirExists = await fileExists(path.join(projectPath, subdir, file));
          if (subdirExists) {
            score += 20; // Slightly lower score for subdirectory finds
            break; // Only count once per file type
          }
        }
      }
    }
  }

  // Check for required directories (check both root and one level deep)
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const rootExists = await directoryExists(path.join(projectPath, dir));
      if (rootExists) {
        score += 20; // Each required directory adds points
      } else {
        // Check in common project subdirectories
        const projectSubdirs = ['codex-rs', 'codex-cli', 'src', 'lib', 'app'];
        for (const projDir of projectSubdirs) {
          const subdirExists = await directoryExists(path.join(projectPath, projDir, dir));
          if (subdirExists) {
            score += 15; // Lower score for nested directory finds
            break;
          }
        }
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names (more precise matching)
        const foundInDeps = Object.keys(allDeps).some(dep => dep === patternText || dep.startsWith(patternText + '/'));
        // Only check for exact matches in keywords array, not description (too broad)
        const foundInKeywords = packageJson.keywords && Array.isArray(packageJson.keywords)
          ? packageJson.keywords.some(keyword => keyword.toLowerCase() === patternText.toLowerCase())
          : false;
        
        if (foundInDeps || foundInKeywords) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };

  try {
    // Check both root and common subdirectories for Cargo.toml
    let cargoPath = path.join(projectPath, 'Cargo.toml');
    let cargoContent = null;

    if (await fileExists(cargoPath)) {
      cargoContent = await fs.readFile(cargoPath, 'utf-8');
    } else {
      // Check common Rust project subdirectories
      const rustSubdirs = ['codex-rs', 'rust', 'src', 'core', 'server'];
      for (const subdir of rustSubdirs) {
        const subdirCargoPath = path.join(projectPath, subdir, 'Cargo.toml');
        if (await fileExists(subdirCargoPath)) {
          cargoPath = subdirCargoPath;
          cargoContent = await fs.readFile(subdirCargoPath, 'utf-8');
          details.primaryLocation = subdir;
          break;
        }
      }
    }

    if (!cargoContent) {
      return details;
    }

    const nameMatch = cargoContent.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = cargoContent.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = cargoContent.match(/edition\s*=\s*"([^"]+)"/);

    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];

    // Check if it's a workspace
    if (cargoContent.includes('[workspace]')) {
      details.isWorkspace = true;

      // Count workspace members
      const workspaceMatch = cargoContent.match(/members\s*=\s*\[([\s\S]*?)\]/);
      if (workspaceMatch) {
        const members = workspaceMatch[1].split(',').map(m => m.trim().replace(/"/g, '')).filter(m => m);
        details.workspaceMembers = members.length;
      }
    }

    // Check for multiple Cargo.toml files (indicates workspace structure)
    if (details.primaryLocation) {
      const subdirPath = path.join(projectPath, details.primaryLocation);
      try {
        const subdirs = await fs.readdir(subdirPath, { withFileTypes: true });
        let cargoCount = 0;
        for (const entry of subdirs) {
          if (entry.isDirectory()) {
            const memberCargoPath = path.join(subdirPath, entry.name, 'Cargo.toml');
            if (await fileExists(memberCargoPath)) {
              cargoCount++;
            }
          }
        }
        if (cargoCount > 3) { // If many workspace members, this is definitely a Rust project
          details.workspaceSize = 'large';
        }
      } catch (error) {
        // Ignore
      }
    }

  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }

  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'c': [0, 0.23, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    const parsedData = JSON.parse(data);
    // Ensure the structure is complete by merging with defaults
    return {
        coefficients: { ...DEFAULT_COEFFICIENTS, ...parsedData.coefficients },
        trainingPoints: parsedData.trainingPoints || {}
    };
  } catch (error) {
    // If file doesn't exist or is malformed, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];

  if (!points || points.length === 0) {
    // No points, nothing to do.
    return;
  }

  if (points.length === 1) {
    // With one point, use a direct ratio for the linear coefficient.
    const point = points[0];
    if (point.fileSizeInBytes > 0) { // Avoid division by zero
        const ratio = point.actualTokens / point.fileSizeInBytes;
        data.coefficients[projectType] = [
            0, // intercept
            Math.max(0, ratio), // linear term (slope)
            0, 0 // quadratic, cubic
        ];
    }
    return;
  }

  // Use linear regression for 2 or more points.
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;

  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;

    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }

  const denominator = (n * sumX2 - sumX * sumX);
  if (denominator === 0) return; // Avoid division by zero, can't calculate slope

  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / denominator;
  const intercept = (sumY - slope * sumX) / n;

  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

--- File: /test_knexfile.js ---

module.exports = {
  development: {
    client: 'sqlite3',
    connection: {
      filename: './dev.sqlite3'
    }
  },
  
  staging: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  },
  
  production: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  }
};


--- File: /test_snapshot.md ---

# Test Snapshot

## Project Overview
This is a test project for demonstrating the Gemini session API integration.

## Files
- `index.js` - Main entry point
- `src/` - Source code directory

## Instructions
You are an AI assistant helping with this test project. Please respond to user queries about the codebase.

--- File: /vitest.config.js ---

import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    testMatch: ['**/*.{test,spec}.js'],
    coverage: {
      reporter: ['text', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        '**/*.config.js',
        '**/*.test.js',
        '**/*.spec.js'
      ]
    }
  }
});



--- File: /test_prune.md/eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_ja.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.


## Project-Specific Manifest (.eck Directory)

This project includes a `.eck` directory with specific context and configuration:

### Project Context

# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.

### Operations Guide

# Common Operations

## Development Setup
```bash
# Setup commands
npm install
# or yarn install
```

## Running the Project
```bash
# Development mode
npm run dev

# Production build
npm run build
```

## Testing
```bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
```

## Deployment
```bash
# Deployment commands
npm run deploy
```

## Troubleshooting
Common issues and their solutions.

### Development Journal

---
task_id: fix-profile-detect-and-add-index-viewer-v1
date: 2025-10-10
type: fix
scope: cli
---

# Fix JSON parsing in profile-detect

- Modified extractJson to be more robust against AI log wrappers
- Finds first '{' and last '}' to extract JSON from surrounding text
- Added /managed_components/ to .gitignore
- Implemented new index-view command to inspect code chunks database
- This resolves the crash when running the profile-detect command

---
task_id: feat-c-language-support-v1
date: 2025-10-10
type: feat
scope: core
---

# Add support for C language projects

- Added C project detection (Makefile, CMakeLists.txt, *.c, *.h)
- Added C-specific file filtering for compiled objects (.o, .a, .so)
- Installed tree-sitter-c@0.21.4 parser
- Integrated C parser into segmenter for functions, structs, enums, unions
- Added token estimation coefficients for C (0.23 ratio)

---
task_id: refactor-claude-skip-permissions-default-v1
date: 2025-09-28T14:30:00.000Z
type: refactor
scope: claude-cli
---
## Force --dangerously-skip-permissions for all Claude calls

- Removed the user-facing '--dangerously-skip-permissions' option from 'ask-claude' and 'ask-claude-session' commands.
- Hardcoded the flag in the service layer to ensure all calls to 'claude-code' are non-interactive by default.
- This change improves the reliability of the tool in automated workflows by preventing it from hanging on permission prompts.

---
task_id: refactor-gpt-service-to-codex-cli-v1
date: 2025-09-28T14:00:00.000Z
type: refactor
scope: services
---
## Switch ask-gpt from chatgpt-cli to official codex CLI

- Replaced `npx chatgpt` calls with the official `codex` CLI, using the `exec --full-auto` command for machine-readable output.
- Implemented an automatic login flow that detects authentication errors and triggers the interactive `codex login` command.
- Created a new `authService.js` to handle the login initiation.
- Removed the `open` package dependency as it is no longer needed.
- Updated tests in `gptService.test.js` to mock the new `codex` command flow.
- Added comprehensive documentation in README.md for both ChatGPT and Claude Code integration.
- Enhanced CLI help with detailed examples and authentication instructions.

---
task_id: gpt-test-1
date: 2025-09-28T09:24:01.314Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:41.532Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:33.203Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:43.135Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:27.678Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:04.054Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: configure-claude-auto-accept-v1
date: 2025-09-14T23:34:21Z
type: feat
scope: workflow
---

## Enable and document claude-code auto-accept mode

Created a global `settings.json` for claude-code to enable `acceptEdits` by default, allowing for fully autonomous operation. Added a `CLAUDE_SETUP.md` file to document this essential configuration step for new developers or fresh installations.

---
task_id: create-eck-commit-command-v1
date: 2025-09-14T23:29:38Z
type: feat
scope: workflow
---

## Create custom /eck:commit claude-code command

Added a custom slash command to automate the new structured journaling and conventional commit process. This command takes structured input (type, scope, summary, details) and uses it to update JOURNAL.md and create a git commit, enforcing our new workflow.

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure

### Environment Overrides

The following environment settings override auto-detected values:

- **project_type**: nodejs
- **project_name**: @xelth/eck-snapshot
- **project_version**: 3.0.0
- **has_typescript**: false
- **framework**: node.js
- **is_monorepo**: false

**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.

---


---

## Project Snapshot Information

- **Project**: eckSnapshot
- **Timestamp**: 2025-10-11T12:17:51.284Z
- **Files Included**: 52
- **Total Files in Repo**: 65

---


## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .gemini/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îî‚îÄ‚îÄ gpt.toml
‚îú‚îÄ‚îÄ create/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ commands/
‚îÇ               ‚îî‚îÄ‚îÄ extensions/
‚îÇ                   ‚îî‚îÄ‚îÄ sample-extension.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ askGpt.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoDocs.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProfiles.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruneSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queryProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.test.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ segmenter.js
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresConnector.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_simple.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysisService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.test.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcherService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddingService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gptService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gptService.test.js
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architect-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ envScanRequest.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitWorkflow.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgent.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorMode.md
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ test_prune.md/
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ codex_delegation_snapshot.md
‚îú‚îÄ‚îÄ COMMANDS_REFERENCE.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ knexfile.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ setup.json
‚îú‚îÄ‚îÄ setup.json.bak
‚îú‚îÄ‚îÄ test_knexfile.js
‚îú‚îÄ‚îÄ test_snapshot.md
‚îî‚îÄ‚îÄ vitest.config.js
```

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs": [
      0,
      0.3091314011805184,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs-monorepo": [
      0,
      0.30927098145345017,
      0,
      0
    ]
  },
  "trainingPoints": {
    "nodejs": [
      {
        "fileSizeInBytes": 229899,
        "estimatedTokens": 45980,
        "actualTokens": 71069,
        "timestamp": "2025-09-21T23:52:36.862Z"
      }
    ],
    "nodejs-monorepo": [
      {
        "fileSizeInBytes": 261612,
        "estimatedTokens": 65403,
        "actualTokens": 80909,
        "timestamp": "2025-09-22T01:50:24.392Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Google Gemini API Key - Required for the 'index' and 'query' commands
# Get your key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_API_KEY_HERE"

# PostgreSQL Connection Details
DB_HOST=127.0.0.1
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_DATABASE=eck_snapshot_db

--- File: /.gemini/tools/gpt.toml ---

description = 'Ask GPT (ChatGPT subscription) for coding tasks.'
prompt = '!{node $(pwd)/index.js ask-gpt "{{args}}"}'  # Dynamic path


--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# EckSnapshot Index
/.ecksnapshot_index/

# Generated snapshot files
*_vectors.json
*_rag_*.md

# Environment variables
.env
.env.local
.env.production



# Added by eck-snapshot to ignore metadata directory
.eck/

# Test snapshot directory
create-snapshot/

# Claude Code settings (exclude all except commands/eck)
.claude/
!.claude/commands/
!.claude/commands/eck/
!.claude/commands/eck/**

# Third-party managed components
/managed_components/


--- File: /COMMANDS_REFERENCE.md ---

# Commands Reference

This document contains essential commands and setup instructions for the project's multi-agent architecture.

## Junior Architect (`gemini_wsl`) Setup Guide

This section explains how the `gemini_wsl` agent (Junior Architect) is configured to delegate coding tasks to the `claude` agent (Coder).

This architecture relies on `gemini-cli`'s custom tool feature.

## 1. Agent Definition

The Junior Architect (JA) is the `gemini_wsl` agent, which is an instance of `gemini-cli` running in WSL. It is defined in `setup.json`.

## 2. Custom Command (`claude.toml`)

The JA's ability to delegate to the Coder (`claude`) is defined by a custom `gemini-cli` command.

This command must be defined in a file named `claude.toml` and placed in the `gemini-cli` configuration directory (e.g., `~/.gemini/tools/claude.toml`).

### `claude.toml` Content

```toml
description = "Ask Claude (from the eckSnapshot project) to help with a task."
prompt = """!{node /mnt/c/Users/xelth/eckSnapshot/index.js ask-claude "{{args}}"}"""
```

## 3. JA Prompt (`agent-prompt.template.md`)

The JA *knows* how to *use* this command because its main system prompt (loaded from `src/templates/agent-prompt.template.md`) instructs it to.

This prompt *mandates* that the `{{args}}` it passes to the `/claude` command must be a single-line **JSON string** in the `apply_code_changes` format.

## 4. Execution Flow

1.  **Senior Architect (Gemini)** gives a high-level `execute_strategic_task` to `gemini_wsl`.
2.  **`gemini_wsl` (JA)** analyzes the task and formulates a low-level `apply_code_changes` JSON payload.
3.  **`gemini_wsl`** executes `/claude` with the JSON payload as a single string argument (`{{args}}`).
4.  **`claude.toml`** executes the `eck-snapshot ask-claude "{...}"` shell command.
5.  **`eck-snapshot`** (specifically `claudeCliService.js`) receives the JSON string as a 'prompt'.
6.  It forwards this prompt to the `claude-cli` binary (`local_dev`), which is smart enough to parse the JSON and execute the `apply_code_changes` task.

## Claude Code Commands

### Commit Command (`.claude/commands/eck/commit.md`)

A custom command for structured commits with automatic journaling. Place this file in `.claude/commands/eck/commit.md` to enable it in Claude Code.

**Usage:** `/commit <type> <scope> <summary> <details>`

**Function:**
- Stages all current changes
- Creates YAML frontmatter for journal entry with task_id, date, type, scope
- Creates markdown body with summary and details
- Prepends complete journal entry to `.eck/JOURNAL.md`
- Creates conventional commit message: `{type}({scope}): {summary}`
- Executes the commit

**Example:** `/commit feat api "Add user authentication" "Implemented JWT-based auth with login/logout endpoints"`

**Important:** This command should be preserved in git (via `.gitignore` rules) so it can be recreated if lost. The command integrates with the project's `.eck` manifest system for structured development journaling.

## Setup Gemini Command

### Dynamic Path Configuration

The `setup-gemini` command automates the creation of `claude.toml` with dynamic path resolution, eliminating the need for hardcoded paths like `/mnt/c/...`.

**Usage:** `eck-snapshot setup-gemini [options]`

**Options:**
- `-v, --verbose` - Show detailed output and error information

**Function:**
- Detects `gemini-cli` installation using `which()`
- Resolves current project path using `process.cwd()`
- Creates `~/.gemini/tools/claude.toml` with dynamic paths
- Integrates with `setup.json` for environment variable configuration
- Handles cross-platform compatibility (WSL/Windows/macOS/Linux)

**Example Generated `claude.toml`:**
```toml
[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["/home/user/projects/eckSnapshot/index.js", "ask-claude"]

[claude.metadata]
platform = "linux"
working_directory = "/home/user/projects/eckSnapshot"
```

**Benefits:**
- ‚úÖ No hardcoded `/mnt/c/` paths
- ‚úÖ Works across all platforms automatically
- ‚úÖ Resolves paths dynamically based on current working directory
- ‚úÖ Integrates with existing `setup.json` configuration
- ‚úÖ Validates `gemini-cli` and `index.js` availability before setup

## MCP Integration with Feedback Support

### MCP Feedback Object

The `mcp_feedback` object provides enhanced error reporting and status tracking for MCP (Message Control Protocol) integration with gemini-cli delegation.

**Structure:**
```json
{
  "mcp_feedback": {
    "success": true,
    "errors": [],
    "mcp_version": "1.0"
  }
}
```

**Properties:**
- `success` (boolean): Indicates if the MCP operation completed successfully
- `errors` (array): List of error messages or issues encountered during execution
- `mcp_version` (string): Version of MCP protocol used

**Usage in Payloads:**

Add `mcp_feedback` to the `post_execution_steps` object in your JSON command payloads:

```json
{
  "target_agent": "local_dev",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "Implement user authentication",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "auth",
        "summary": "Add JWT authentication"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**Implementation:**

The `claudeCliService.js` automatically parses `mcp_feedback` from incoming prompts and includes it in the response object. When errors are present, they are logged to the console for debugging purposes.

**Benefits:**
- ‚úÖ Enhanced error reporting for MCP operations
- ‚úÖ Version tracking for protocol compatibility
- ‚úÖ Automatic logging of MCP errors
- ‚úÖ Improved debugging for gemini-cli delegation issues

## Auto-Generated Gemini Extensions

*This section is automatically generated. Run `npm run docs:auto` to update.*

### sample-extension

Sample Gemini extension for demonstration

**Commands:**

- **sample-command**: A sample command for testing auto-docs
  - Usage: `sample-command [options]`
  - Examples: `sample-command --help`

**Tools:**

- **sample-tool**: A sample tool for testing auto-docs
  - Usage: Use this tool for sample operations



--- File: /codex_delegation_snapshot.md ---

üîç WSL detected, using configured host: 172.29.16.1


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /knexfile.js ---

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
const isWSL = process.platform === 'linux' && 
  (process.env.WSL_DISTRO_NAME || 
   fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));

if (isWSL && (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1')) {
  // For WSL, always use the standard WSL2 gateway IP
  process.env.DB_HOST = '172.29.16.1';
}

export default {
  development: {
    client: 'pg',
    connection: {
      host: process.env.DB_HOST || '127.0.0.1',
      port: process.env.DB_PORT || 5432,
      user: process.env.DB_USER || 'myuser',
      password: process.env.DB_PASSWORD || 'mypassword',
      database: process.env.DB_DATABASE || 'eck_snapshot_db',
    },
    pool: {
      min: 2,
      max: 10
    }
  }
};

--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-c": "^0.21.4",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /packages/cli/src/commands/extensions/sample-extension.json ---

{
  "name": "sample-extension",
  "description": "Sample Gemini extension for demonstration",
  "commands": [
    {
      "name": "sample-command",
      "description": "A sample command for testing auto-docs",
      "usage": "sample-command [options]",
      "examples": [
        "sample-command --help"
      ]
    }
  ],
  "tools": [
    {
      "name": "sample-tool",
      "description": "A sample tool for testing auto-docs",
      "usage": "Use this tool for sample operations"
    }
  ]
}

--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": [
          "build.gradle",
          "build.gradle.kts",
          "settings.gradle",
          "settings.gradle.kts"
        ],
        "directories": [
          "app/src/main",
          "app/src/androidTest"
        ],
        "manifestFiles": [
          "AndroidManifest.xml"
        ],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": [
          "package.json"
        ],
        "directories": [
          "packages",
          "apps",
          "libs"
        ],
        "patterns": [
          "workspaces",
          "lerna",
          "nx",
          "rush"
        ],
        "priority": 7
      },
      "nodejs": {
        "files": [
          "package.json"
        ],
        "directories": [
          "node_modules"
        ],
        "priority": 6
      },
      "python-poetry": {
        "files": [
          "pyproject.toml"
        ],
        "patterns": [
          "tool.poetry"
        ],
        "priority": 9
      },
      "python-pip": {
        "files": [
          "requirements.txt",
          "setup.py",
          "setup.cfg"
        ],
        "directories": [
          "__pycache__",
          "venv",
          ".venv"
        ],
        "priority": 7
      },
      "python-conda": {
        "files": [
          "environment.yml",
          "environment.yaml",
          "conda.yml"
        ],
        "priority": 8
      },
      "django": {
        "files": [
          "manage.py"
        ],
        "patterns": [
          "django",
          "Django"
        ],
        "priority": 9
      },
      "flask": {
        "files": [
          "app.py",
          "application.py"
        ],
        "patterns": [
          "flask",
          "Flask"
        ],
        "priority": 8
      },
      "flutter": {
        "files": [
          "pubspec.yaml"
        ],
        "directories": [
          "lib",
          "android",
          "ios"
        ],
        "priority": 8
      },
      "react-native": {
        "files": [
          "package.json"
        ],
        "directories": [
          "android",
          "ios"
        ],
        "patterns": [
          "react-native"
        ],
        "priority": 8
      },
      "rust": {
        "files": [
          "Cargo.toml"
        ],
        "directories": [
          "src",
          "target"
        ],
        "priority": 9
      },
      "go": {
        "files": [
          "go.mod",
          "go.sum"
        ],
        "directories": [
          "cmd",
          "pkg",
          "internal"
        ],
        "priority": 7
      },
      "dotnet": {
        "files": [
          "*.csproj",
          "*.sln",
          "*.fsproj",
          "*.vbproj"
        ],
        "directories": [
          "bin",
          "obj"
        ],
        "priority": 7
      },
      "c": {
        "files": [
          "Makefile",
          "CMakeLists.txt",
          "*.c",
          "*.h"
        ],
        "directories": [
          "src",
          "include",
          "lib"
        ],
        "priority": 6
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "smartMode": {
    "tokenThreshold": 7000000,
    "_comment": "Projects over this token count will use vector indexing instead of single file snapshots"
  },
  "vectorIndex": {
    "autoExportOnIndex": true,
    "_comment": "Automatically export the vector index to a file after every successful 'index' command."
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "npm-shrinkwrap.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*",
      "LICENSE*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "coverage/",
      "create-snapshot/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      },
      "c": {
        "filesToIgnore": [
          "*.o",
          "*.a",
          "*.so",
          "*.out",
          "*.exe"
        ],
        "dirsToIgnore": [
          "build/",
          "cmake-build-debug/",
          "cmake-build-release/",
          ".cmake/"
        ],
        "extensionsToIgnore": [
          ".o",
          ".a",
          ".so",
          ".out"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./.eck/snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      },
      "gemini_wsl": {
        "active": true,
        "name": "Gemini WSL Agent (Junior Architect)",
        "description": "Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "gemini",
            "claude"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)",
          "claude (delegate)"
        ],
        "restrictions": [
          "Runs only inside the WSL environment"
        ]
      },
      "gemini_windows": {
        "active": true,
        "name": "Gemini Windows Agent (Standalone)",
        "description": "Gemini, running in native Windows (PowerShell). Can only access Windows tools.",
        "guiSupport": true,
        "identification": {
          "markers": [
            "Windows",
            "gemini",
            "!WSL"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)"
        ],
        "restrictions": [
          "Runs only in native Windows",
          "Cannot access WSL-only tools like claude"
        ]
      }
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "src/templates/envScanRequest.md",
      "gitWorkflow": "src/templates/gitWorkflow.md",
      "multiAgent": "src/templates/multiAgent.md",
      "vectorMode": "src/templates/vectorMode.md",
      "agent": "src/templates/agent-prompt.template.md"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject, viewIndex } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .action(createRepoSnapshot)
    .addHelpText('after', `
Examples for --profile:
  --profile backend                      (Uses the 'backend' profile)
  --profile "backend,-**/tests/**"         (Uses 'backend' profile, excludes all test files)
  --profile "src/**/*.js,-**/*.test.js"  (Includes all JS files in src, excludes tests)

  Combine predefined profiles (from .eck/profiles.json) with ad-hoc glob patterns.
  Prefix a profile name or glob pattern with '-' to exclude it.
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Index view command
  program
    .command('index-view')
    .description('View the contents of the code chunks database')
    .option('--limit <number>', 'Number of records to display', (val) => parseInt(val), 10)
    .option('--offset <number>', 'Number of records to skip', (val) => parseInt(val), 0)
    .option('--file <path>', 'Filter by file path')
    .action(viewIndex);

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/cli/commands/askGpt.js ---

import { ask } from '../../services/gptService.js';

/**
 * CLI entry point for ask-gpt command.
 * @param {string} payload - JSON payload string.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} options - CLI options.
 */
export async function askGpt(payload, options = {}) {
  const verbose = Boolean(options.verbose);
  const model = options.model || 'gpt-5-codex';
  const reasoning = options.reasoning || 'high';

  if (!payload) {
    console.error('ask-gpt requires a JSON payload argument.');
    process.exitCode = 1;
    return;
  }

  try {
    const result = await ask(payload, { verbose, model, reasoning });
    console.log(JSON.stringify(result, null, 2));
  } catch (error) {
    console.error(error.message);
    if (verbose && error?.stack) {
      console.error(error.stack);
    }
    process.exitCode = 1;
  }
}


--- File: /src/cli/commands/autoDocs.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Auto-generate documentation from gemini-extension.json files
 */
export async function generateAutoDocs() {
  try {
    const projectRoot = path.resolve(__dirname, '../../../');
    const extensionsDir = path.join(projectRoot, 'packages/cli/src/commands/extensions');
    const referenceFile = path.join(projectRoot, 'COMMANDS_REFERENCE.md');

    // Check if extensions directory exists
    try {
      await fs.access(extensionsDir);
    } catch (error) {
      console.log(`Extensions directory not found at: ${extensionsDir}`);
      console.log('Creating example structure...');

      // Create the directory structure
      await fs.mkdir(extensionsDir, { recursive: true });

      // Create a sample gemini-extension.json file for demonstration
      const sampleExtension = {
        name: "sample-extension",
        description: "Sample Gemini extension for demonstration",
        commands: [
          {
            name: "sample-command",
            description: "A sample command for testing auto-docs",
            usage: "sample-command [options]",
            examples: ["sample-command --help"]
          }
        ],
        tools: [
          {
            name: "sample-tool",
            description: "A sample tool for testing auto-docs",
            usage: "Use this tool for sample operations"
          }
        ]
      };

      await fs.writeFile(
        path.join(extensionsDir, 'sample-extension.json'),
        JSON.stringify(sampleExtension, null, 2)
      );

      console.log('Created sample extension at:', path.join(extensionsDir, 'sample-extension.json'));
    }

    // Read all JSON files in the extensions directory
    const files = await fs.readdir(extensionsDir);
    const jsonFiles = files.filter(file => file.endsWith('.json'));

    if (jsonFiles.length === 0) {
      console.log('No JSON files found in extensions directory');
      return;
    }

    console.log(`Found ${jsonFiles.length} extension file(s): ${jsonFiles.join(', ')}`);

    // Parse each JSON file and extract command/tool information
    const extensions = [];

    for (const file of jsonFiles) {
      try {
        const filePath = path.join(extensionsDir, file);
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = JSON.parse(content);
        extensions.push({ filename: file, ...extension });
        console.log(`Parsed extension: ${extension.name || file}`);
      } catch (error) {
        console.warn(`Failed to parse ${file}:`, error.message);
      }
    }

    if (extensions.length === 0) {
      console.log('No valid extension files found');
      return;
    }

    // Generate markdown content
    let markdownContent = '\n## Auto-Generated Gemini Extensions\n\n';
    markdownContent += '*This section is automatically generated. Run `npm run docs:auto` to update.*\n\n';

    for (const extension of extensions) {
      markdownContent += `### ${extension.name || extension.filename}\n\n`;

      if (extension.description) {
        markdownContent += `${extension.description}\n\n`;
      }

      // Add commands section
      if (extension.commands && extension.commands.length > 0) {
        markdownContent += '**Commands:**\n\n';
        for (const command of extension.commands) {
          markdownContent += `- **${command.name}**: ${command.description || 'No description'}\n`;
          if (command.usage) {
            markdownContent += `  - Usage: \`${command.usage}\`\n`;
          }
          if (command.examples && command.examples.length > 0) {
            markdownContent += `  - Examples: ${command.examples.map(ex => `\`${ex}\``).join(', ')}\n`;
          }
        }
        markdownContent += '\n';
      }

      // Add tools section
      if (extension.tools && extension.tools.length > 0) {
        markdownContent += '**Tools:**\n\n';
        for (const tool of extension.tools) {
          markdownContent += `- **${tool.name}**: ${tool.description || 'No description'}\n`;
          if (tool.usage) {
            markdownContent += `  - Usage: ${tool.usage}\n`;
          }
        }
        markdownContent += '\n';
      }
    }

    // Read the current COMMANDS_REFERENCE.md
    let currentContent;
    try {
      currentContent = await fs.readFile(referenceFile, 'utf-8');
    } catch (error) {
      console.warn('COMMANDS_REFERENCE.md not found, creating new file');
      currentContent = '# Commands Reference\n\n';
    }

    // Remove existing auto-generated section if it exists
    const autoGenRegex = /\n## Auto-Generated Gemini Extensions[\s\S]*?(?=\n## |\n# |$)/;
    const updatedContent = currentContent.replace(autoGenRegex, '') + markdownContent;

    // Write the updated content back to the file
    await fs.writeFile(referenceFile, updatedContent);

    console.log('\n‚úÖ Auto-documentation generated successfully!');
    console.log(`üìù Updated: ${referenceFile}`);
    console.log(`üì¶ Processed ${extensions.length} extension(s)`);

  } catch (error) {
    console.error('Failed to generate auto-docs:', error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return {
          content: `--- File: /${normalizedPath} ---\n\n${content}\n\n`,
          path: normalizedPath,
          size: fileStats.size
        };
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const successfulFileObjects = results.filter(Boolean);
    const contentArray = successfulFileObjects.map(f => f.content);

    // Return all processed data instead of writing file
    return {
      stats,
      contentArray,
      successfulFileObjects,
      allFiles,
      originalCwd,
      repoPath
    };
    
  } finally {
    process.chdir(originalCwd); // Ensure we always change back
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      smartModeTokenThreshold: setupConfig.smartMode.tokenThreshold,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    if (estimation.estimatedTokens > config.smartModeTokenThreshold) {
      spinner.succeed('Project is large. Switching to vector indexing mode.');
      await indexProject(repoPath, options);
    } else {
      spinner.succeed('Project is small. Creating dual snapshots...');
      
      // Step 1: Process all files ONCE
      const { 
        stats, 
        contentArray, 
        successfulFileObjects, 
        allFiles, 
        originalCwd: processingOriginalCwd, // We get originalCwd from the processing function
        repoPath: processedRepoPath 
      } = await processProjectFiles(repoPath, options, config, projectDetection.type);

      const originalCwd = process.cwd(); // Get CWD *before* chdir
      process.chdir(processedRepoPath); // Go back to repo path for git hash and tree

      try {
        // --- Common Data --- 
        const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
        const repoName = path.basename(processedRepoPath);
        const gitHash = await getGitCommitHash(processedRepoPath);
        const fileExtension = options.format || config.defaultFormat || 'md';
        const outputPath = options.output || path.resolve(originalCwd, config.output);
        await fs.mkdir(outputPath, { recursive: true });

        const shouldIncludeTree = config.tree && !options.noTree;
        let directoryTree = '';
        if (shouldIncludeTree) {
          console.log('üå≥ Generating directory tree...');
          directoryTree = await generateDirectoryTree(processedRepoPath, '', allFiles, 0, config.maxDepth || 10, config);
        }

        // Calculate included file stats by extension
        const includedFilesByType = new Map();
        for (const fileObj of successfulFileObjects) {
            try {
                let ext = path.extname(fileObj.path);
                if (ext === '') ext = '.no-extension';
                includedFilesByType.set(ext, (includedFilesByType.get(ext) || 0) + 1);
            } catch (e) { /* Silently ignore */ }
        }
        const sortedIncludedStats = [...includedFilesByType.entries()].sort((a, b) => b[1] - a[1]);

        // Calculate Top 10 Largest Files
        const largestFiles = [...successfulFileObjects].sort((a, b) => b.size - a.size).slice(0, 10);

        const fileBody = (directoryTree ? `\n## Directory Structure\n\n\`\`\`\n${directoryTree}\`\`\`\n\n` : '') + contentArray.join('');

        // --- File 1: Architect Snapshot --- 
        const architectOptions = { ...options, agent: false };
        // Load manifest for headers
        const eckManifest = await loadProjectEckManifest(processedRepoPath);
        const isGitRepo = await checkGitRepository(processedRepoPath);

        const architectHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: architectOptions, repoPath: processedRepoPath }, isGitRepo);
        const architectBaseFilename = `${repoName}_snapshot_${timestamp}${gitHash ? `_${gitHash}` : ''}`;
        const architectFilename = `${architectBaseFilename}.${fileExtension}`;
        const architectFilePath = path.join(outputPath, architectFilename);
        await fs.writeFile(architectFilePath, architectHeader + fileBody);

        // --- File 2: Junior Architect Snapshot --- 
        let jaFilePath = null;
        if (!options.profile && !options.agent && fileExtension === 'md') { // Only create JA snapshot if main is MD
          console.log('üñãÔ∏è Generating Junior Architect (_ja) snapshot...');
          const jaOptions = { ...options, agent: true, noTree: false, noAiHeader: false };
          const jaHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: jaOptions, repoPath: processedRepoPath }, isGitRepo);
          const jaFilename = `${architectBaseFilename}_ja.${fileExtension}`;
          jaFilePath = path.join(outputPath, jaFilename);
          await fs.writeFile(jaFilePath, jaHeader + fileBody);
        }

        // --- Combined Report --- 
        console.log('\n‚úÖ Snapshot generation complete!');
        console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        console.log(`üìÑ Architect File: ${architectFilePath}`);
        if (jaFilePath) {
          console.log(`üìÑ Junior Arch File: ${jaFilePath}`);
        }
        console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
        console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
        console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
        console.log(`üìã Format: ${fileExtension.toUpperCase()}`);

        if (sortedIncludedStats.length > 0) {
          console.log('\nüì¶ Included File Types:');
          console.log('---------------------------------');
          for (const [ext, count] of sortedIncludedStats.slice(0, 10)) {
              console.log(`   - ${String(ext).padEnd(15)} ${String(count).padStart(5)} files`);
          }
          if (sortedIncludedStats.length > 10) {
              console.log(`   ... and ${sortedIncludedStats.length - 10} other types.`);
          }
        }

        if (largestFiles.length > 0) {
          console.log('\nüêò Top 10 Largest Files (Included):');
          console.log('---------------------------------');
          for (const fileObj of largestFiles) {
              console.log(`   - ${String(formatSize(fileObj.size)).padEnd(15)} ${fileObj.path}`);
          }
        }
        
        // Excluded/Skipped Files Section
        const hasExcludedContent = stats.excludedFiles > 0 || stats.binaryFiles > 0 || stats.oversizedFiles > 0 || stats.ignoredFiles > 0 || stats.errors.length > 0;
        if (hasExcludedContent) {
          console.log('\nüö´ Excluded/Skipped Files:');
          console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        }
        
        if (stats.excludedFiles > 0) {
          console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
        }
        if (stats.binaryFiles > 0) {
          console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
        }
        if (stats.oversizedFiles > 0) {
          console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
        }
        if (stats.ignoredFiles > 0) {
          console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
        }
        if (stats.errors.length > 0) {
          console.log(`‚ùå Errors: ${stats.errors.length}`);
          if (options.verbose) {
            stats.errors.forEach(err => console.log(`   ${err}`));
          }
        }
        
        // Print detailed skip reasons report
        if (stats.skippedFilesDetails.size > 0) {
          console.log('\nüìã Skip Reasons:');
          console.log('---------------------------------');
          
          for (const [reason, files] of stats.skippedFilesDetails.entries()) {
            console.log(`\nüî∏ ${reason} (${files.length} files):`);
            files.forEach(file => {
              console.log(`   ‚Ä¢ ${file}`);
            });
          }
          console.log('---------------------------------');
        } else {
          console.log('---------------------------------');
        }
        
        // Generate training command string if estimation data is available
        if (estimation && projectDetection.type && !options.profile) {
          const trainingCommand = generateTrainingCommand(projectDetection.type, estimation.estimatedTokens, estimation.totalSize, repoPath);
          console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
          console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
          console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
        }

      } finally {
        process.chdir(originalCwd); // Final reset back to original CWD
      }
    }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}

--- File: /src/cli/commands/detectProfiles.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Extracts a JSON object from a string that might contain markdown wrappers or log output.
 * Finds the first opening brace '{' and the last closing brace '}' to extract the JSON.
 */
function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }

  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1).trim();
  }

  return text.trim();
}

/**
 * Scans the project structure, saves the directory tree to a file, and asks an AI to generate
 * context profiles, saving them to .eck/profiles.json.
 */
export async function detectProfiles(repoPath, options) {
  const spinner = ora('Initializing and scanning project structure...').start();
  try {
    await initializeEckManifest(repoPath);

    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const config = {
        ...userConfig,
        ...setupConfig.fileFiltering,
        ...setupConfig.performance
    };

    const allFiles = await scanDirectoryRecursively(repoPath, config, repoPath);
    spinner.text = 'Generating directory tree...';
    const dirTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth, config);

    if (!dirTree) {
        throw new Error('Failed to generate directory tree or project is empty.');
    }

    spinner.text = 'Saving directory tree to file...';
    const treeFilePath = path.join(repoPath, '.eck', 'directory_tree_for_profiling.md');
    await fs.writeFile(treeFilePath, dirTree);

    const prompt = `You are a code architect. Based on the file directory tree found in the file at './.eck/directory_tree_for_profiling.md', please identify logical 'context profiles' for splitting the project.
Your output MUST be ONLY a valid JSON object.
The keys of the object MUST be the profile names (e.g., 'frontend', 'backend', 'core-logic', 'docs').
The values MUST be an object containing 'include' and 'exclude' arrays of glob patterns.
Example: {"frontend": {"include": ["packages/ui/**"], "exclude": []}, "docs": {"include": ["docs/**"], "exclude": []}}.
DO NOT add any conversational text, introductory sentences, or explanations. Your entire response must be ONLY the JSON object.`;

    spinner.text = 'Asking AI to analyze directory tree and detect profiles...';
    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;

    if (!rawText || typeof rawText.replace !== 'function') {
      throw new Error(`AI returned invalid content type: ${typeof rawText}`);
    }

    spinner.text = 'Saving generated profiles...';
    const cleanedJson = extractJson(rawText);
    let parsedProfiles;
    try {
        parsedProfiles = JSON.parse(cleanedJson);
    } catch (e) {
        console.error('\nInvalid JSON received from AI:', cleanedJson);
        throw new Error(`AI returned invalid JSON: ${e.message}`);
    }

    const outputPath = path.join(repoPath, '.eck', 'profiles.json');
    await fs.writeFile(outputPath, JSON.stringify(parsedProfiles, null, 2));

    const profileKeys = Object.keys(parsedProfiles);
    spinner.succeed(`Successfully detected and saved ${profileKeys.length} profiles to ${outputPath}`);

    console.log('\n‚ú® Detected Profiles:');
    console.log('---------------------------');
    for (const profileName of profileKeys) {
        console.log(`  - ${profileName}`);
    }
    console.log('\nYou can now use these profile names with the --profile flag.');

  } catch (error) {
    spinner.fail(`Failed to detect profiles: ${error.message}`);
  }
}


--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));
    
    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
      console.log('   eck-snapshot index                              # For large projects');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
      console.log('   eck-snapshot index                              # For large projects');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
      console.log('   eck-snapshot index                              # For semantic search');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    let files = await getProjectFiles(projectPath);
    
    // --- Apply Advanced Profile Filtering ---
    const defaultProfile = await getProfile('default', projectPath);
    if (options.profile) {
        mainSpinner.text = `Applying profile filter: '${options.profile}'...`;
        files = await applyProfileFilter(files, options.profile, projectPath);
        mainSpinner.info(`Filtered down to ${files.length} files using profile: '${options.profile}'.`);
    } else if (defaultProfile) {
        mainSpinner.text = "Applying detected 'default' profile...";
        files = micromatch(files, defaultProfile.include, { ignore: defaultProfile.exclude });
        mainSpinner.info(`Filtered down to ${files.length} files using detected 'default' profile.`);
    }
    if (files.length === 0) {
        throw new Error(`Profile filter resulted in 0 files. Aborting.`);
    }
    // --- End Profile Filtering ---

    const profileName = options.profile || 'default';
    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await releaseAnalysisModel();
    await releaseEmbeddingModel();
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}


--- File: /src/cli/commands/pruneSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { parseSnapshotContent, parseSize, formatSize } from '../../utils/fileUtils.js';

function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }
  const firstBracket = text.indexOf('[');
  const lastBracket = text.lastIndexOf(']');
  if (firstBracket !== -1 && lastBracket !== -1 && lastBracket > firstBracket) {
    return text.substring(firstBracket, lastBracket + 1).trim();
  }
  return text.trim();
}

export async function pruneSnapshot(snapshotFile, options) {
  const spinner = ora('Starting snapshot pruning process...').start();
  try {
    const targetSize = parseSize(options.targetSize);
    spinner.text = `Reading snapshot file: ${snapshotFile}`;
    const snapshotContent = await fs.readFile(snapshotFile, 'utf-8');
    const snapshotHeader = snapshotContent.split('--- File: /')[0];
    const files = parseSnapshotContent(snapshotContent);

    if (files.length === 0) {
      spinner.warn('No files found in the snapshot.');
      return;
    }

    const currentSize = Buffer.byteLength(snapshotContent, 'utf-8');
    if (currentSize <= targetSize) {
      spinner.succeed(`Snapshot is already smaller than the target size. (${formatSize(currentSize)} < ${formatSize(targetSize)})`);
      return;
    }

    spinner.text = 'Asking AI to rank files by importance...';
    const filePaths = files.map(f => f.path);
    const prompt = `You are a software architect. Given the following list of file paths from a project snapshot, rank them by importance for understanding the project's core functionality. The most critical files (e.g., entry points, core logic, configurations) should be first. Your output MUST be ONLY a JSON array of strings, with the file paths in ranked order. Do not add any other text.\n\nFILE LIST:\n${JSON.stringify(filePaths, null, 2)}`;

    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;
    const cleanedJson = extractJson(rawText);

    let rankedFiles;
    try {
      rankedFiles = JSON.parse(cleanedJson);
      if (!Array.isArray(rankedFiles) || rankedFiles.some(item => typeof item !== 'string')) {
        throw new Error('AI response is not an array of strings.');
      }
    } catch (e) {
      spinner.fail(`Failed to parse AI's file ranking: ${e.message}`);
      console.error('Received from AI:', cleanedJson);
      return;
    }

    spinner.text = 'Building pruned snapshot...';
    const fileMap = new Map(files.map(f => [f.path, f.content]));
    let newSnapshotContent = snapshotHeader;
    let newSize = Buffer.byteLength(newSnapshotContent, 'utf-8');
    let filesIncluded = 0;

    for (const filePath of rankedFiles) {
      if (fileMap.has(filePath)) {
        const fileContent = fileMap.get(filePath);
        const fileEntry = `--- File: /${filePath} ---\n\n${fileContent}\n\n`;
        const entrySize = Buffer.byteLength(fileEntry, 'utf-8');

        if (newSize + entrySize > targetSize) {
          break;
        }

        newSnapshotContent += fileEntry;
        newSize += entrySize;
        filesIncluded++;
      }
    }

    const outputFilename = `${path.basename(snapshotFile, path.extname(snapshotFile))}_pruned_${options.targetSize}${path.extname(snapshotFile)}`;
    const outputPath = path.join(path.dirname(snapshotFile), outputFilename);

    await fs.writeFile(outputPath, newSnapshotContent);

    spinner.succeed('Snapshot pruning complete!');
    console.log(`- Original Size: ${formatSize(currentSize)}`);
    console.log(`- New Size: ${formatSize(newSize)}`);
    console.log(`- Files Included: ${filesIncluded} / ${files.length}`);
    console.log(`- Pruned snapshot saved to: ${outputPath}`);

  } catch (error) {
    spinner.fail(`An error occurred during pruning: ${error.message}`);
  }
}


--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-—Å–Ω–∞–ø—à–æ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

export async function viewIndex(options) {
  const spinner = ora('Connecting to database...').start();
  const knex = getKnex();

  try {
    await initDb();

    spinner.text = 'Fetching code chunks from database...';

    // Build query with optional filters
    let query = knex('code_chunks')
      .select('id', 'file_path', 'chunk_type', 'chunk_name', 'profile')
      .orderBy('id', 'asc');

    // Apply file filter if specified
    if (options.file) {
      query = query.where('file_path', 'like', `%${options.file}%`);
      spinner.info(`Filtering by file path: ${options.file}`);
    }

    // Apply pagination
    if (options.limit) {
      query = query.limit(options.limit);
    }
    if (options.offset) {
      query = query.offset(options.offset);
    }

    const chunks = await query;

    if (chunks.length === 0) {
      spinner.warn('No code chunks found in the database.');
      return;
    }

    spinner.succeed(`Found ${chunks.length} code chunks`);

    // Display results in a formatted table
    console.log('\nüìä Code Chunks Index:');
    console.log('‚ïê'.repeat(100));
    console.table(chunks.map(chunk => ({
      ID: chunk.id,
      'File Path': chunk.file_path.replace(process.cwd(), '.'),
      Type: chunk.chunk_type,
      Name: chunk.chunk_name,
      Profile: chunk.profile || 'default'
    })));

    // Show summary
    const totalCount = await knex('code_chunks').count('* as count').first();
    console.log(`\nShowing ${chunks.length} of ${totalCount.count} total chunks`);

    if (options.limit && chunks.length === options.limit) {
      console.log(`\nüí° Use --offset ${(options.offset || 0) + options.limit} to view the next page`);
    }

  } catch (error) {
    spinner.fail(`Failed to view index: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/setupGemini.js ---

import which from 'which';
import path from 'path';
import fs from 'fs/promises';
import os from 'os';
import chalk from 'chalk';

/**
 * Sets up claude.toml configuration for gemini-cli integration with dynamic paths
 * @param {Object} options - Command options
 */
export async function setupGemini(options = {}) {
  try {
    console.log(chalk.blue('üîß Setting up gemini-cli integration with dynamic paths...'));

    // Check if gemini-cli is installed
    let geminiCliPath;
    try {
      geminiCliPath = await which('gemini-cli');
      console.log(chalk.green(`‚úÖ Found gemini-cli at: ${geminiCliPath}`));
    } catch (error) {
      console.error(chalk.red('‚ùå gemini-cli not found in PATH'));
      console.log(chalk.yellow('üí° Please install gemini-cli first:'));
      console.log(chalk.cyan('   npm install -g gemini-cli'));
      process.exit(1);
    }

    // Get current working directory for dynamic path resolution
    const currentDir = process.cwd();
    const indexJsPath = path.join(currentDir, 'index.js');

    // Verify index.js exists
    try {
      await fs.access(indexJsPath);
      console.log(chalk.green(`‚úÖ Found eck-snapshot index.js at: ${indexJsPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Could not find index.js at: ${indexJsPath}`));
      console.log(chalk.yellow('üí° Make sure you are running this command from the eck-snapshot project directory'));
      process.exit(1);
    }

    // Create gemini tools directory
    const homeDir = os.homedir();
    const geminiToolsDir = path.join(homeDir, '.gemini', 'tools');

    try {
      await fs.mkdir(geminiToolsDir, { recursive: true });
      console.log(chalk.green(`‚úÖ Created/verified gemini tools directory: ${geminiToolsDir}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to create gemini tools directory: ${error.message}`));
      process.exit(1);
    }

    // Read environment variables from setup.json if available
    let envVars = {};
    try {
      const setupJsonPath = path.join(currentDir, 'setup.json');
      const setupContent = await fs.readFile(setupJsonPath, 'utf-8');
      const setupData = JSON.parse(setupContent);

      // Extract relevant environment variables
      if (setupData.environmentDetection) {
        envVars.ECK_SNAPSHOT_PATH = currentDir;
        console.log(chalk.blue(`üìã Using project context from setup.json`));
      }
    } catch (error) {
      console.log(chalk.yellow('‚ö†Ô∏è  setup.json not found or invalid, using defaults'));
    }

    // Generate claude.toml content with dynamic paths
    const claudeTomlContent = generateClaudeToml(indexJsPath, envVars);

    // Write claude.toml file
    const claudeTomlPath = path.join(geminiToolsDir, 'claude.toml');
    try {
      await fs.writeFile(claudeTomlPath, claudeTomlContent, 'utf-8');
      console.log(chalk.green(`‚úÖ Generated claude.toml at: ${claudeTomlPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to write claude.toml: ${error.message}`));
      process.exit(1);
    }

    // Success summary
    console.log(chalk.green('\nüéâ Setup completed successfully!'));
    console.log(chalk.blue('\nüìã Configuration summary:'));
    console.log(chalk.cyan(`   ‚Ä¢ gemini-cli: ${geminiCliPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ eck-snapshot: ${indexJsPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ claude.toml: ${claudeTomlPath}`));

    if (Object.keys(envVars).length > 0) {
      console.log(chalk.cyan(`   ‚Ä¢ Environment variables: ${Object.keys(envVars).join(', ')}`));
    }

    console.log(chalk.blue('\nüöÄ You can now use:'));
    console.log(chalk.cyan('   gemini-cli claude "Your prompt here"'));
    console.log(chalk.green('\n‚ú® Cross-platform path resolution is automatically handled!'));

  } catch (error) {
    console.error(chalk.red(`‚ùå Setup failed: ${error.message}`));
    if (options.verbose) {
      console.error(chalk.red('Stack trace:'), error.stack);
    }
    process.exit(1);
  }
}

/**
 * Generates claude.toml content with dynamic paths
 * @param {string} indexJsPath - Path to eck-snapshot index.js
 * @param {Object} envVars - Environment variables to include
 * @returns {string} - Generated TOML content
 */
function generateClaudeToml(indexJsPath, envVars = {}) {
  const envSection = Object.keys(envVars).length > 0
    ? `# Environment variables from setup.json
${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}

`
    : '';

  return `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'
# This file uses dynamic paths to work across WSL/Windows environments

${envSection}[claude]
# eck-snapshot integration for AI-powered repository analysis
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

# Command examples:
# gemini-cli claude "Create a snapshot of the current project"
# gemini-cli claude "Analyze the database structure"
# gemini-cli claude "Generate a project overview"

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
generated_at = "${new Date().toISOString()}"
platform = "${process.platform}"
node_version = "${process.version}"
working_directory = "${path.dirname(indexJsPath)}"

# Cross-platform compatibility notes:
# - Paths are automatically resolved using process.cwd()
# - Works in WSL, Windows, macOS, and Linux
# - No hardcoded /mnt/c/ paths required
`;
}

--- File: /src/cli/commands/setupGemini.test.js ---

import { describe, it, expect } from 'vitest';

describe('setupGemini integration', () => {
  it('should validate path resolution logic', () => {
    // Test path join functionality that setupGemini uses
    const currentDir = '/test/project';
    const indexJsPath = `${currentDir}/index.js`;

    expect(indexJsPath).toBe('/test/project/index.js');
    expect(indexJsPath).toContain('index.js');
  });

  it('should validate gemini tools directory structure', () => {
    const homeDir = '/home/user';
    const geminiToolsDir = `${homeDir}/.gemini/tools`;
    const claudeTomlPath = `${geminiToolsDir}/claude.toml`;

    expect(geminiToolsDir).toBe('/home/user/.gemini/tools');
    expect(claudeTomlPath).toBe('/home/user/.gemini/tools/claude.toml');
  });

  it('should validate TOML content structure', () => {
    const indexJsPath = '/test/project/index.js';
    const envVars = { ECK_SNAPSHOT_PATH: '/test/project' };

    // Test environment section generation
    const envSection = Object.keys(envVars).length > 0
      ? `# Environment variables from setup.json\n${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}\n\n`
      : '';

    // Test main TOML structure
    const tomlContent = `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'

${envSection}[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
platform = "${process.platform}"
working_directory = "${indexJsPath.replace('/index.js', '')}"`;

    expect(tomlContent).toContain('[claude]');
    expect(tomlContent).toContain('name = "eck-snapshot"');
    expect(tomlContent).toContain(`args = ["${indexJsPath}", "ask-claude"]`);
    expect(tomlContent).toContain('[claude.metadata]');
    expect(tomlContent).toContain('ECK_SNAPSHOT_PATH = "/test/project"');
  });

  it('should handle cross-platform paths correctly', () => {
    const testPaths = [
      { platform: 'windows', path: 'C:\\Users\\test\\project\\index.js' },
      { platform: 'unix', path: '/home/user/project/index.js' },
      { platform: 'wsl', path: '/mnt/c/Users/test/project/index.js' }
    ];

    testPaths.forEach(({ platform, path }) => {
      expect(path).toContain('index.js');
      expect(path.length).toBeGreaterThan(0);

      // Test that the path is absolute (platform-appropriate)
      if (platform === 'windows') {
        expect(path).toMatch(/^[A-Z]:\\/);
      } else {
        expect(path).toMatch(/^\//);
      }
    });
  });

  it('should validate error handling patterns', () => {
    // Test error message patterns that setupGemini should handle
    const errorPatterns = [
      'gemini-cli not found in PATH',
      'Could not find index.js',
      'Failed to create gemini tools directory',
      'Failed to write claude.toml'
    ];

    errorPatterns.forEach(pattern => {
      expect(pattern).toBeDefined();
      expect(typeof pattern).toBe('string');
      expect(pattern.length).toBeGreaterThan(0);
    });
  });

  it('should test JSON parsing for setup.json', () => {
    const validSetupData = {
      environmentDetection: {
        detected: true
      }
    };

    const jsonString = JSON.stringify(validSetupData);
    const parsed = JSON.parse(jsonString);

    expect(parsed.environmentDetection).toBeDefined();
    expect(parsed.environmentDetection.detected).toBe(true);

    // Test invalid JSON handling pattern
    const invalidJson = 'invalid json {';
    let parseError = null;
    try {
      JSON.parse(invalidJson);
    } catch (e) {
      parseError = e;
    }

    expect(parseError).toBeDefined();
    expect(parseError.message).toContain('JSON');
  });
});

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/core/segmenter.js ---

import { parse } from '@babel/parser';
import _traverse from '@babel/traverse';
const traverse = _traverse.default;
import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import Parser from 'tree-sitter';
import Python from 'tree-sitter-python';
import Java from 'tree-sitter-java';
import Kotlin from 'tree-sitter-kotlin';
import C from 'tree-sitter-c';

function generateHash(content) {
  return crypto.createHash('sha256').update(content).digest('hex');
}

const tsParser = new Parser();
const languageParsers = {
    '.py': Python,
    '.java': Java,
    '.kt': Kotlin,
    '.c': C,
    '.h': C,
};

async function _segmentWithTreeSitter(content, filePath, language) {
    tsParser.setLanguage(language);
    const tree = tsParser.parse(content);
    const chunks = [];
    // Graph relations for tree-sitter are not implemented in this step.
    const relations = [];

    function walk(node) {
        const nodeTypeMap = {
            'function_definition': 'function', 'class_definition': 'class', // Python
            'function_declaration': 'function', 'class_declaration': 'class', // Kotlin/Java
            'method_declaration': 'function', // Java
            'struct_specifier': 'struct', 'enum_specifier': 'enum', 'union_specifier': 'union', 'type_definition': 'typedef', // C
        };

        if (nodeTypeMap[node.type]) {
            const nameNode = node.childForFieldName('name') || node.child(1);
            const chunkName = nameNode ? nameNode.text : 'anonymous';
            const chunkCode = node.text;
            chunks.push({
                filePath,
                chunk_type: nodeTypeMap[node.type],
                chunk_name: chunkName,
                code: chunkCode,
                contentHash: generateHash(chunkCode)
            });
        }
        node.children.forEach(walk);
    }
    walk(tree.rootNode);
    return { chunks, relations };
}

async function _segmentJavaScript(content, filePath) {
    const chunks = [];
    const relations = [];

    try {
        const ast = parse(content, { sourceType: 'module', plugins: ['typescript', 'jsx'], errorRecovery: true });

        const getChunkData = (node) => {
            const chunkName = node.id ? node.id.name : 'anonymous';
            const chunkCode = content.substring(node.start, node.end);
            return { filePath, chunk_name: chunkName, code: chunkCode, contentHash: generateHash(chunkCode) };
        };

        traverse(ast, {
            enter(path) {
                let currentScopeName = 'file';
                const parentFunction = path.findParent((p) => p.isFunctionDeclaration() || p.isClassDeclaration());
                if (parentFunction && parentFunction.node.id) {
                    currentScopeName = parentFunction.node.id.name;
                }

                if (path.isFunctionDeclaration() || path.isClassDeclaration()) {
                    chunks.push({ ...getChunkData(path.node), chunk_type: path.isClassDeclaration() ? 'class' : 'function' });
                }

                if (path.isImportDeclaration()) {
                    const sourceFile = path.node.source.value;
                    relations.push({ from: filePath, to: sourceFile, type: 'IMPORTS' });
                }

                if (path.isCallExpression()) {
                    const calleeName = path.get('callee').toString();
                    relations.push({ from: currentScopeName, to: calleeName, type: 'CALLS' });
                }
            }
        });
    } catch (e) {
        console.error(`Babel parsing error in ${filePath}: ${e.message}`);
    }
    return { chunks, relations };
}

export async function segmentFile(filePath) {
    try {
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = path.extname(filePath);
        let result = { chunks: [], relations: [] };

        if (['.js', '.jsx', '.ts', '.tsx'].includes(extension)) {
            result = await _segmentJavaScript(content, filePath);
        } else if (languageParsers[extension]) {
            result = await _segmentWithTreeSitter(content, filePath, languageParsers[extension]);
        }
        
        // Fallback: if no specific chunks, treat the whole file as one
        if (result.chunks.length === 0) {
            const code = content;
            result.chunks.push({ filePath, chunk_type: 'file', chunk_name: path.basename(filePath), code, contentHash: generateHash(code) });
        }

        return result;
    } catch (error) {
        console.error(`Failed to segment file ${filePath}: ${error.message}`);
        return { chunks: [], relations: [] };
    }
}

--- File: /src/database/postgresConnector.js ---

import knex from 'knex';
import fs from 'fs/promises';
import path from 'path';
import config from '../../knexfile.js';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));

let knexInstance = null;

function getKnex() {
  if (!knexInstance) {
    console.log('Initializing Knex connection...');
    knexInstance = knex(config.development);
  }
  return knexInstance;
}

async function initDb() {
  const db = getKnex();
  try {
    console.log('Checking database connection...');
    await db.raw('SELECT 1+1 AS result');
    console.log('Connection successful.');

    console.log('Applying database schema...');
    // Try full schema first, fallback to simple schema
    try {
      const schemaPath = path.join(__dirname, 'schema.sql');
      const schemaSQL = await fs.readFile(schemaPath, 'utf-8');
      await db.raw(schemaSQL);
      console.log('Full schema with vector extensions applied successfully.');
    } catch (error) {
      console.log('Vector extensions not available, using simplified schema...');
      const simpleSchemaPath = path.join(__dirname, 'schema_simple.sql');
      const simpleSchemaSQL = await fs.readFile(simpleSchemaPath, 'utf-8');
      await db.raw(simpleSchemaSQL);
      console.log('Simplified schema applied successfully.');
    }
  } catch (error) {
    console.error('Error initializing database:', error.message);
    throw error;
  }
}

async function destroyDb() {
  if (knexInstance) {
    console.log('Destroying Knex connection pool...');
    await knexInstance.destroy();
    knexInstance = null;
  }
}

export {
  getKnex,
  initDb,
  destroyDb,
};

--- File: /src/database/schema.sql ---

-- –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
-- (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ Apache AGE —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ PG)
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç)
SELECT create_graph('eck_snapshot_graph');

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞ (—É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding VECTOR(768), -- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è Jina Code v2
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π (—Ä–µ–±–µ—Ä –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

-- –°–æ–∑–¥–∞–µ–º HNSW-–∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX IF NOT EXISTS code_chunks_embedding_idx ON code_chunks USING HNSW (embedding vector_cosine_ops);

--- File: /src/database/schema_simple.sql ---

-- Simplified schema without vector and graph extensions for testing

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding TEXT, -- JSON string representation for now
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

--- File: /src/services/analysisService.js ---

import { pipeline } from '@xenova/transformers';

class AnalysisService {
    static instance = null;
    static modelName = 'Xenova/distilgpt2'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = await pipeline('text-generation', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = null;
        }
    }
}

export async function getCodeSummary(codeChunk) {
    const generator = await AnalysisService.getInstance();

    const prompt = `This code:\n${codeChunk.substring(0, 150)}\nSummary:`;

    const output = await generator(prompt, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true
    });

    const generatedText = output[0].generated_text;
    const summary = generatedText.replace(prompt, '').trim() || 'Auto-generated description';
    return summary.substring(0, 200); // Limit summary length
}

export const releaseModel = AnalysisService.releaseModel;

--- File: /src/services/authService.js ---

import ora from 'ora';
import { execa } from 'execa';

/**
 * Initiates the interactive login flow by spawning 'codex login'.
 * This will open a browser and wait for the user to complete authentication.
 * @returns {Promise<void>}
 */
export async function initiateLogin() {
  const spinner = ora('Authentication required. Please follow the browser instructions.').start();
  try {
    // Run `codex login` interactively, inheriting stdio to show user instructions.
    await execa('codex', ['login'], { stdio: 'inherit' });
    spinner.succeed('Login successful. Retrying original command...');
  } catch (e) {
    spinner.fail('Login process failed or was cancelled.');
    // Re-throw to notify p-retry that the attempt failed.
    throw new Error(`Login failed: ${e.message}`);
  }
}

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';
import pRetry from 'p-retry';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {boolean} continueConversation Whether to continue the last conversation with -c flag.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, continueConversation = false) {
  try {
    // Ensure the log directory exists
    try {
      await import('fs/promises').then(fs => fs.mkdir('./.eck/logs', { recursive: true }));
    } catch (e) {
      console.error(`Failed to create log directory: ${e.message}`);
      // Do not block execution if log dir creation fails, just warn
    }
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }

    return await attemptClaudeExecution(prompt, sessionId);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./.eck/logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./.eck/logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null) {
  const timestamp = new Date().toISOString();
  const logFile = `./.eck/logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./.eck/logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./.eck/logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./.eck/logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null) {
  return new Promise((resolve, reject) => {
    
    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }

    // Always add the skip permissions flag for automation reliability
    args.push('--dangerously-skip-permissions');

    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');
    
    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });
    
    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;
    
    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const ACTIVITY_TIMEOUT = 60000; // 1 minute of inactivity allowed
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log('üíÄ No activity detected for 1 minute, killing process');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${ACTIVITY_TIMEOUT/1000} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

/**
 * Executes a prompt using gemini-cli delegation with retry logic for transient errors.
 * @param {string} prompt The prompt to send to Claude via gemini-cli.
 * @returns {Promise<object>} A promise that resolves with the response from Claude.
 */
export async function askClaude(prompt) {
  return pRetry(async () => {
    try {
      const result = await execa('gemini-cli', ['claude', prompt], {
        timeout: 120000 // 2 minute timeout
      });

      // Parse mcp_feedback if present in prompt
      let mcpFeedback = null;
      try {
        const promptObj = JSON.parse(prompt);
        if (promptObj.payload && promptObj.payload.post_execution_steps && promptObj.payload.post_execution_steps.mcp_feedback) {
          mcpFeedback = promptObj.payload.post_execution_steps.mcp_feedback;

          // Log if errors array is non-empty
          if (mcpFeedback.errors && Array.isArray(mcpFeedback.errors) && mcpFeedback.errors.length > 0) {
            console.warn('MCP feedback contains errors:', mcpFeedback.errors);
          }
        }
      } catch (parseError) {
        // If prompt is not valid JSON or doesn't contain mcp_feedback, continue normally
      }

      return {
        stdout: result.stdout,
        stderr: result.stderr,
        success: true,
        mcp_feedback: mcpFeedback
      };
    } catch (error) {
      // Check if this is a transient error that should be retried
      if (isTransientError(error)) {
        console.log(`Transient error detected, retrying: ${error.message}`);
        throw error; // This will trigger a retry
      }

      // Non-transient errors should not be retried
      console.error(`Non-transient error in askClaude: ${error.message}`);
      return {
        stdout: error.stdout || '',
        stderr: error.stderr || error.message,
        success: false,
        error: error.message
      };
    }
  }, {
    retries: 3,
    minTimeout: 1000,
    maxTimeout: 5000,
    onFailedAttempt: (error) => {
      console.log(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`);
    }
  });
}

/**
 * Checks if an error is transient and should be retried.
 * @param {Error} error The error to check.
 * @returns {boolean} True if the error is transient.
 */
export function isTransientError(error) {
  const errorMessage = (error.message || '').toLowerCase();
  const stderr = (error.stderr || '').toLowerCase();
  const stdout = (error.stdout || '').toLowerCase();
  const allOutput = `${errorMessage} ${stderr} ${stdout}`;

  // Network-related errors
  const networkErrors = [
    'network',
    'timeout',
    'connection',
    'econnreset',
    'enotfound',
    'econnrefused',
    'socket hang up'
  ];

  // Quota/rate limit errors
  const quotaErrors = [
    'quota exceeded',
    'rate limit',
    'too many requests',
    'service unavailable',
    'temporarily unavailable',
    '429',
    '500',
    '502',
    '503',
    '504'
  ];

  const transientPatterns = [...networkErrors, ...quotaErrors];

  return transientPatterns.some(pattern => allOutput.includes(pattern));
}

--- File: /src/services/claudeCliService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { askClaude } from './claudeCliService.js';

// Mock execa
vi.mock('execa', () => ({
  execa: vi.fn()
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', () => ({
  default: async (fn, options) => {
    // For tests, we'll execute the function directly without retries
    return await fn();
  }
}));

describe('claudeCliService', () => {
  let mockExeca;

  beforeEach(async () => {
    const { execa } = await import('execa');
    mockExeca = execa;
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.resetAllMocks();
  });

  describe('askClaude', () => {
    it('should successfully execute gemini-cli claude command', async () => {
      const mockResponse = {
        stdout: '{"result": "test response", "success": true}',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith('gemini-cli', ['claude', 'test prompt'], {
        timeout: 120000
      });
      expect(result).toEqual({
        stdout: mockResponse.stdout,
        stderr: mockResponse.stderr,
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle non-transient errors without retry', async () => {
      const mockError = new Error('EACCES: permission denied');
      mockError.code = 'EACCES';
      mockError.stdout = '';
      mockError.stderr = 'permission denied';

      mockExeca.mockRejectedValue(mockError);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: 'permission denied',
        success: false,
        error: 'EACCES: permission denied'
      });
    });

    it('should identify transient network errors', async () => {
      const mockError = new Error('Connection timeout');
      mockError.stdout = '';
      mockError.stderr = 'network timeout occurred';

      const { isTransientError } = await import('./claudeCliService.js');

      expect(isTransientError(mockError)).toBe(true);
    });


    it('should handle JSON parsing in gemini-cli response', async () => {
      const complexJsonResponse = {
        stdout: JSON.stringify({
          type: 'result',
          data: {
            analysis: 'test analysis',
            metrics: { tokens: 100, cost: 0.05 }
          },
          timestamp: new Date().toISOString()
        }),
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(complexJsonResponse);

      const result = await askClaude('analyze this code');

      expect(result.success).toBe(true);
      expect(result.stdout).toContain('test analysis');
      expect(result.stdout).toContain('tokens');
    });

    it('should handle empty responses gracefully', async () => {
      const mockResponse = {
        stdout: '',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: '',
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle stderr warnings without failing', async () => {
      const mockResponse = {
        stdout: '{"result": "success"}',
        stderr: 'Warning: deprecated feature used',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result.success).toBe(true);
      expect(result.stderr).toContain('deprecated feature');
    });

    it('should respect timeout configuration', async () => {
      mockExeca.mockResolvedValue({
        stdout: 'response',
        stderr: '',
        exitCode: 0
      });

      await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith(
        'gemini-cli',
        ['claude', 'test prompt'],
        { timeout: 120000 }
      );
    });

    it('should parse mcp_feedback from JSON prompt', async () => {
      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithFeedback = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: true,
              errors: [],
              mcp_version: '1.0'
            }
          }
        }
      });

      const result = await askClaude(promptWithFeedback);

      expect(result.mcp_feedback).toEqual({
        success: true,
        errors: [],
        mcp_version: '1.0'
      });
    });

    it('should log warning when mcp_feedback contains errors', async () => {
      const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithErrors = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: false,
              errors: ['Error 1', 'Error 2'],
              mcp_version: '1.0'
            }
          }
        }
      });

      await askClaude(promptWithErrors);

      expect(consoleSpy).toHaveBeenCalledWith('MCP feedback contains errors:', ['Error 1', 'Error 2']);

      consoleSpy.mockRestore();
    });
  });

  describe('transient error detection', () => {
    it('should treat network errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const networkErrors = [
        'network error',
        'timeout',
        'connection refused',
        'ECONNRESET',
        'ENOTFOUND',
        'socket hang up'
      ];

      networkErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should treat quota errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const quotaErrors = [
        'quota exceeded',
        'rate limit',
        'too many requests',
        '429',
        '503'
      ];

      quotaErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should not treat permission errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const permanentErrors = [
        'EACCES: permission denied',
        'Invalid API key',
        'Authentication failed'
      ];

      permanentErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(false);
      });
    });
  });
});

--- File: /src/services/dispatcherService.js ---

import { ask as askGpt } from './gptService.js';
import { executePrompt as askClaude } from './claudeCliService.js';

/**
 * Dispatches an analytical task to the most efficient AI model with a fallback.
 * Priority 1: Codex (GPT) with low reasoning for speed and cost.
 * Priority 2: Claude as a reliable fallback.
 * @param {string} prompt The JSON payload or prompt string for the task.
 * @returns {Promise<object>} The result from the successful AI agent.
 */
export async function dispatchAnalysisTask(prompt) {
  try {
    console.log('üß† Dispatcher: Attempting analysis with Codex (low reasoning)...');
    const gptOptions = {
      model: 'gpt-5-codex',
      reasoning: 'low'
    };
    // The 'ask' function expects payload as first arg, and options as second.
    // Since prompt is a string here, we wrap it in an object for consistency if needed,
    // but for simple prompts it can often be passed directly.
    const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
    return await askGpt(payload, { verbose: false, ...gptOptions });
  } catch (gptError) {
    console.warn(`‚ö†Ô∏è Codex (low reasoning) failed: ${gptError.message}`);
    console.log('üîÑ Failing over to Claude for analysis...');
    try {
      return await askClaude(prompt);
    } catch (claudeError) {
      console.error(`‚ùå Critical Failure: Both Codex and Claude failed for analysis task.`);
      throw new Error(`Primary (Codex) Error: ${gptError.message}\nFallback (Claude) Error: ${claudeError.message}`);
    }
  }
}

--- File: /src/services/embedding.js ---

import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';
import pLimit from 'p-limit';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "embedding-001" });

async function generateEmbedding(text, taskType = 'RETRIEVAL_DOCUMENT') {
  try {
    const result = await model.embedContent({ 
      content: { parts: [{ text }] },
      taskType
    });
    return result.embedding.values;
  } catch (error) {
    console.error('‚ùå Gemini Embedding Error:', error.message);
    throw error;
  }
}

async function generateBatchEmbeddings(segments, taskType = 'RETRIEVAL_DOCUMENT') {
    if (segments.length === 0) return [];

    const BATCH_COUNT_LIMIT = 100;
    const BATCH_SIZE_LIMIT = 3000000; // 3MB for safety
    const allBatches = [];
    let currentBatch = [];
    let currentBatchSize = 0;

    for (const segment of segments) {
        const segmentSize = Buffer.byteLength(segment.content, 'utf8');
        if (segmentSize > BATCH_SIZE_LIMIT) { // Handle single oversized segments
            console.log(chalk.yellow(`  -> Warning: Segment '${segment.name}' in '${segment.filePath}' is oversized and will be truncated.`));
            segment.content = segment.content.substring(0, 20000) + '... [truncated]'; // Truncate oversized segment
        }
        if (currentBatch.length > 0 && (currentBatch.length >= BATCH_COUNT_LIMIT || currentBatchSize + Buffer.byteLength(segment.content, 'utf8') > BATCH_SIZE_LIMIT)) {
            allBatches.push(currentBatch);
            currentBatch = [];
            currentBatchSize = 0;
        }
        currentBatch.push(segment);
        currentBatchSize += Buffer.byteLength(segment.content, 'utf8');
    }
    if (currentBatch.length > 0) {
        allBatches.push(currentBatch);
    }

    console.log(chalk.cyan(`‚è≥ Generating embeddings for ${segments.length} segments, divided into ${allBatches.length} safe chunks...`));

    const limit = pLimit(5); // Set concurrency to 5 parallel requests
    let processedCount = 0;

    const promises = allBatches.map((batch, i) => {
        return limit(async () => {
            const batchSizeKB = (Buffer.byteLength(batch.map(s => s.content).join(''), 'utf8') / 1024).toFixed(2);
            console.log(chalk.blue(`  -> Sending chunk ${i + 1}/${allBatches.length} (${batch.length} segments, ${batchSizeKB} KB)...`));
            try {
                const contents = batch.map(s => ({ parts: [{ text: s.content }] }));
                const result = await model.batchEmbedContents({ 
                    requests: contents.map(content => ({ content, taskType }))
                });
                processedCount++;
                console.log(chalk.green(`  <- Chunk ${i + 1}/${allBatches.length} processed successfully.`));
                return result.embeddings.map(e => e.values);
            } catch (error) {
                console.error(chalk.red(`‚ùå Gemini Batch Embedding Error (Chunk ${i + 1}/${allBatches.length}):`), error.message);
                throw error;
            }
        });
    });

    const chunkResults = await Promise.all(promises);
    const allEmbeddings = chunkResults.flat();

    console.log(chalk.green.bold('‚úÖ Batch embeddings generated successfully.'));
    return allEmbeddings;
}

export const embeddingService = {
  generateEmbedding,
  generateBatchEmbeddings
};

--- File: /src/services/embeddingService.js ---

import { pipeline } from '@xenova/transformers';

class EmbeddingService {
    static instance = null;
    static modelName = 'Xenova/jina-embeddings-v2-base-en'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = await pipeline('feature-extraction', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = null;
            // In Node.js, there's no explicit GPU memory release, 
            // relying on the garbage collector is the standard way.
        }
    }
}

export async function generateEmbedding(code) {
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(code, { pooling: 'mean', normalize: true });
    return Array.from(result.data);
}

export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) return [];
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(texts, { pooling: 'mean', normalize: true });
    
    // Convert tensor to array of arrays
    const embeddings = [];
    for (let i = 0; i < result.dims[0]; i++) {
        const start = i * result.dims[1];
        const end = start + result.dims[1];
        embeddings.push(Array.from(result.data.slice(start, end)));
    }
    return embeddings;
}

export const releaseModel = EmbeddingService.releaseModel;

--- File: /src/services/gptService.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import pRetry from 'p-retry';
import ora from 'ora';
import { loadProjectEckManifest } from '../utils/fileUtils.js';
import { initiateLogin } from './authService.js';
import which from 'which';

const SYSTEM_PROMPT = 'You are a Coder agent. Apply code changes per JSON spec. Respond only in JSON: {success: bool, changes: array, errors: array, post_steps: object}';

class AuthError extends Error {
  constructor(message) {
    super(message);
    this.name = 'AuthError';
  }
}

/**
 * Checks if the codex CLI tool is available in the system's PATH.
 * Throws an error if not found.
 */
async function ensureCodexCliExists() {
  try {
    await which('codex');
  } catch (error) {
    throw new Error('The `codex` CLI tool is not installed or not in your PATH. Please install it from https://github.com/openai/codex to use this command.');
  }
}

/**
 * Delegates an apply_code_changes payload to the codex CLI with auto-login.
 * @param {string|object} payload - JSON string or object payload to forward to the agent.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} [options]
 * @returns {Promise<object>}
 */
export async function ask(payload, options = {}) {
  const { verbose = false, model = 'gpt-5-codex', reasoning = 'high' } = options;
  await ensureCodexCliExists();

  const run = async () => {
    const spinner = verbose ? null : ora('Sending payload to Codex agent...').start();
    try {
      const payloadObject = await parsePayload(payload);
      const manifest = await loadProjectEckManifest(process.cwd());
      const userPrompt = buildUserPrompt(payloadObject, manifest);
      const promptInput = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const args = [
        'exec',
        // Use full-auto mode to prevent interactive prompts from the agent,
        // as this service is designed for non-interactive delegation.
        '--full-auto',
        '--model', model,
        '-c', `model_reasoning_effort=${reasoning}`
      ];

      debug(verbose, `Executing: codex ${args.join(' ')} <stdin>`);

      const cliResult = await execa('codex', args, {
        cwd: process.cwd(),
        timeout: 300000, // 5-minute timeout
        input: promptInput // Stream large prompts via stdin to avoid argv limits
      });

      const output = cliResult?.stdout?.trim();
      if (!output) {
        throw new Error('codex CLI returned empty response');
      }

      const parsed = extractFinalJson(output);
      if (parsed) {
        if (parsed.post_steps || parsed.post_execution_steps) {
          const postSteps = parsed.post_steps || parsed.post_execution_steps;
          await handlePostExecutionSteps(postSteps, payloadObject);
          parsed.mcp_feedback = postSteps?.mcp_feedback || null;
        }
        spinner?.succeed('Codex agent completed the task.');
        return parsed;
      }

      // If parsing fails, surface the raw response text for upstream handling.
      spinner?.succeed('Codex agent completed the task.');
      return { success: true, changes: [], errors: [], response_text: output };

    } catch (error) {
        spinner?.fail('Codex execution failed.');
        handleCliError(error); // This will throw a specific error type
    }
  };

  return pRetry(run, {
    retries: 1, // Only retry once after a successful login
    minTimeout: 0,
    onFailedAttempt: async (error) => {
      if (error.name === 'AuthError') {
        await initiateLogin();
      } else {
        throw error; // Don't retry for other errors, fail immediately
      }
    }
  });
}


async function parsePayload(payload) {
  if (typeof payload === 'string') {
    try {
      return JSON.parse(payload);
    } catch (error) {
      throw new Error(`Failed to parse payload JSON: ${error.message}`);
    }
  }
  if (typeof payload === 'object' && payload !== null) {
    return payload;
  }
  throw new Error('Invalid payload type. Expected JSON string or object.');
}

function buildUserPrompt(payloadObject, manifest) {
  const payloadString = JSON.stringify(payloadObject);
  if (!manifest) {
    return payloadString;
  }

  const sections = [];
  if (manifest.context) {
    sections.push('## .eck Context\n' + manifest.context);
  }
  if (manifest.operations) {
    sections.push('## .eck Operations\n' + manifest.operations);
  }
  if (manifest.journal) {
    sections.push('## .eck Journal\n' + manifest.journal);
  }
  if (manifest.environment && Object.keys(manifest.environment).length > 0) {
    sections.push('## .eck Environment\n' + JSON.stringify(manifest.environment, null, 2));
  }

  if (sections.length === 0) {
    return payloadString;
  }

  return `${payloadString}\n\n# Project Context\n${sections.join('\n\n')}`;
}

function debug(verbose, message) {
  if (verbose) {
    console.log(`[ask-gpt] ${message}`);
  }
}

function extractFinalJson(text) {
  const trimmed = text?.trim();
  if (!trimmed) {
    return null;
  }

  try {
    return JSON.parse(trimmed);
  } catch (error) {
    // Continue with fallback parsing when logs precede the JSON payload.
  }

  const fencedMatch = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fencedContent = fencedMatch[1].trim();
    try {
      return JSON.parse(fencedContent);
    } catch (error) {
      // Ignore and fall through to final brace search.
    }
  }

  const lastBraceIndex = trimmed.lastIndexOf('{');
  if (lastBraceIndex === -1) {
    return null;
  }

  const jsonCandidate = trimmed.slice(lastBraceIndex);
  try {
    return JSON.parse(jsonCandidate);
  } catch (error) {
    return null;
  }
}

function handleCliError(error) {
  const combined = `${error?.message || ''} ${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
  // Check for text that `codex` outputs when auth is missing.
  if (combined.includes('authentication is required') || combined.includes('please run `codex login`')) {
    const authError = new Error('Codex authentication is required. Attempting to log in.');
    authError.name = 'AuthError';
    throw authError;
  }

  throw new Error(`codex CLI failed: ${error.stderr || error.message}`);
}

async function handlePostExecutionSteps(postSteps, payloadObject) {
  if (!postSteps || typeof postSteps !== 'object') {
    return;
  }

  if (postSteps.journal_entry) {
    await applyJournalEntry(postSteps.journal_entry, payloadObject);
  }

  if (postSteps.mcp_feedback) {
    logMcpFeedback(postSteps.mcp_feedback);
  }
}

async function applyJournalEntry(entry, payloadObject) {
  const journalEntry = normalizeJournalEntry(entry);
  const journalPath = path.join(process.cwd(), '.eck', 'JOURNAL.md');

  await fs.mkdir(path.dirname(journalPath), { recursive: true });

  let existing = '';
  try {
    existing = await fs.readFile(journalPath, 'utf-8');
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw new Error(`Failed to read JOURNAL.md: ${error.message}`);
    }
  }

  const taskId = payloadObject?.task_id || payloadObject?.payload?.task_id || journalEntry.task_id || 'ask-gpt';
  const isoDate = new Date().toISOString();

  const frontmatter = [
    '---',
    `task_id: ${taskId}`,
    `date: ${isoDate}`,
    `type: ${journalEntry.type}`,
    `scope: ${journalEntry.scope}`,
    '---',
    ''
  ].join('\n');

  const summary = journalEntry.summary ? `## ${journalEntry.summary}\n` : '';
  const details = journalEntry.details ? `${journalEntry.details}\n` : '';

  const entryBlock = `${frontmatter}${summary ? `${summary}\n` : ''}${details}\n`;

  const existingTrimmed = existing ? existing.replace(/^\n+/, '') : '';
  const newContent = `${entryBlock}${existingTrimmed}`.replace(/\n{3,}/g, '\n\n');

  await fs.writeFile(journalPath, newContent.trimEnd() + '\n');

  await stageJournal(journalPath);
  await commitJournal(journalEntry);
}

function normalizeJournalEntry(entry) {
  return {
    type: entry.type || 'chore',
    scope: entry.scope || 'journal',
    summary: entry.summary || 'Update journal entry',
    details: entry.details || ''
  };
}

async function stageJournal(journalPath) {
  const relativePath = path.relative(process.cwd(), journalPath);
  try {
    await execa('git', ['add', relativePath], { cwd: process.cwd() });
  } catch (error) {
    throw new Error(`Failed to stage journal entry: ${error.message}`);
  }
}

async function commitJournal(entry) {
  const scopePart = entry.scope ? `(${entry.scope})` : '';
  const summary = (entry.summary || 'Update journal entry').replace(/\s+/g, ' ').trim();
  const commitMessage = `${entry.type}${scopePart}: ${summary}`;

  try {
    await execa('git', ['commit', '-m', commitMessage], { cwd: process.cwd() });
  } catch (error) {
    const text = `${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
    if (text.includes('nothing to commit')) {
      console.warn('Journal entry already committed or no changes to commit.');
      return;
    }
    throw new Error(`Failed to commit journal entry: ${error.message}`);
  }
}

function logMcpFeedback(feedback) {
  if (!feedback) {
    return;
  }

  const errors = Array.isArray(feedback.errors) ? feedback.errors : [];
  if (!feedback.success || errors.length > 0) {
    console.warn('MCP feedback indicates issues:', errors.length > 0 ? errors : feedback);
  } else {
    console.log('MCP feedback:', feedback);
  }
}


--- File: /src/services/gptService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

// Mock execa and which
vi.mock('execa', () => ({ execa: vi.fn() }));
vi.mock('which', () => ({ default: vi.fn() }));

// Mock fs/promises for journal entries
const mkdirMock = vi.fn();
const readFileMock = vi.fn();
const writeFileMock = vi.fn();
const loadProjectEckManifestMock = vi.fn();
vi.mock('fs/promises', () => ({
  mkdir: mkdirMock,
  readFile: readFileMock,
  writeFile: writeFileMock
}));
vi.mock('../utils/fileUtils.js', () => ({
  loadProjectEckManifest: loadProjectEckManifestMock
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', async (importOriginal) => {
  const actual = await importOriginal();
  return {
    ...actual,
    default: vi.fn(async (fn, options) => {
      try {
        return await fn();
      } catch (error) {
        if (options.onFailedAttempt) {
          await options.onFailedAttempt(error);
          // In a real scenario, p-retry would re-run fn. For testing, we simulate one retry.
          if (error.name === 'AuthError') {
             return await fn();
          }
        }
        throw error;
      }
    })
  };
});

// Mock the authService
vi.mock('./authService.js', () => ({
  initiateLogin: vi.fn()
}));

describe('gptService with codex CLI', () => {
  let ask;
  let execaMock;
  let whichMock;
  let initiateLoginMock;

  beforeEach(async () => {
    vi.clearAllMocks();

    ({ execa: execaMock } = await import('execa'));
    const which = (await import('which')).default;
    whichMock = which;
    ({ initiateLogin: initiateLoginMock } = await import('./authService.js'));
    ({ ask } = await import('./gptService.js'));

    whichMock.mockResolvedValue('/usr/bin/codex');
    loadProjectEckManifestMock.mockResolvedValue(null);
  });

  it('should call codex CLI with correct arguments and parse final JSON from noisy output', async () => {
    const codexLogs = '[2025-10-06 20:04:22] OpenAI Codex v0.42.0\nSome setup log...\n\n{"success": true, "changes": ["change1"], "errors": []}';
    execaMock.mockResolvedValue({ stdout: codexLogs });

    const payload = { objective: 'Test' };
    const result = await ask(payload);

    expect(result).toEqual({ success: true, changes: ['change1'], errors: [] });
    expect(execaMock).toHaveBeenCalledWith('codex', expect.arrayContaining(['exec', '--full-auto', '--model']), expect.any(Object));
    const [, , options] = execaMock.mock.calls[0];
    expect(options.input).toContain(JSON.stringify(payload));
  });

  it('should trigger login flow on authentication error and retry', async () => {
    const authError = new Error('Authentication is required. Please run `codex login`.');
    authError.name = 'AuthError'; // Custom error name to trigger retry
    authError.stderr = 'Authentication is required. Please run `codex login`.';

    const successResponse = {
      id: 'task2',
      msg: {
        type: 'task_complete',
        last_agent_message: '{"success": true}'
      }
    };

    // First call fails, second call (retry) succeeds
    execaMock
      .mockRejectedValueOnce(authError)
      .mockResolvedValueOnce({ stdout: JSON.stringify(successResponse) });

    initiateLoginMock.mockResolvedValue();

    const result = await ask({ objective: 'Retry test' });

    expect(result).toEqual({ success: true });
    expect(initiateLoginMock).toHaveBeenCalledTimes(1);
    expect(execaMock).toHaveBeenCalledTimes(2); // Initial call + retry
  });

  it('should throw if codex CLI is not found', async () => {
    whichMock.mockRejectedValue(new Error('not found'));
    await expect(ask({})).rejects.toThrow('The `codex` CLI tool is not installed');
  });

  it('should throw non-auth errors immediately without retry', async () => {
    const otherError = new Error('Some other CLI error');
    otherError.stderr = 'Something else went wrong';
    execaMock.mockRejectedValueOnce(otherError);

    await expect(ask({})).rejects.toThrow('codex CLI failed: Something else went wrong');
    expect(initiateLoginMock).not.toHaveBeenCalled();
  });
});


--- File: /src/templates/agent-prompt.template.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

--- File: /src/templates/architect-prompt.template.md ---

# AI Architect Instructions

You are an autonomous AI Architect. Your primary goal is to develop and evolve a software project by planning high-level architecture and delegating implementation tasks to an execution agent named Claude.

## Core Workflow: The Thought-Tool-Observation Loop

Your entire operational process follows a strict loop:
1.  **Thought:** Analyze the user's request, the current state of the project, and previous observations. Formulate a plan and decide on the next immediate action. You must explain your reasoning and your chosen action in plain text.
2.  **Tool:** Immediately after your thought process, you MUST issue a command to either the local `eck-snapshot` environment or the `claude_code_agent`.
3.  **Observation:** After issuing a command, you MUST STOP and wait for an `Observation:` message from the system, which will contain the result of your command. Do not proceed until you receive it.

## Commanding the Execution Agent (Claude)

To delegate any coding task (writing, editing, testing, refactoring), you MUST generate a JSON command block for the `claude_code_agent`. This is your primary method of modifying the codebase.

**JSON Command Format:**
```json
{
  "target_agent": "claude_code_agent",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "A brief, clear task description for Claude.",
    "context": "Explain why this change is needed and any relevant architectural context.",
    "files_to_modify": [
      {
        "path": "exact/path/to/file.js",
        "action": "add | modify | replace | delete",
        "location": "line numbers, function name, or a unique search pattern",
        "details": "Precise, step-by-step instructions for Claude to implement."
      }
    ]
  }
}
```

## Interacting with the Local Environment

To understand the project state, you can command the `eck-snapshot` tool directly. Use this for discovery, analysis, and managing project context.

**Tool Command Format:** `[tool_code: eck-snapshot <command> <options>]`

**Available Commands:**
- `eck-snapshot snapshot`: To create a new snapshot of the current state.
- `eck-snapshot query "<question>"`: To search the codebase.
- `eck-snapshot detect`: To analyze the project structure.
- `eck-snapshot restore <snapshot_file> --include ...`: To view specific files from a snapshot.

## Final Mandate

Your existence is defined by this loop. Think, act by issuing a tool command, and then wait for the observation. This is the only way you can make progress.

--- File: /src/templates/envScanRequest.md ---

Run this command and return the output:
```
node -e "console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))"
```

--- File: /src/templates/gitWorkflow.md ---

## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

--- File: /src/templates/multiAgent.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** {{stats.includedFiles}}
- **Total Files in Repo:** {{stats.totalFiles}}

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

{{projectOverview}}

{{eckManifestSection}}

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (`gemini_wsl`), who has a detailed (`_ja.md`) snapshot and the ability to command a **Coder** agent (`claude`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (`gemini_wsl`):** Receives strategic tasks, analyzes the `_ja.md` snapshot, breaks the task down, and commands the Coder.
  - **Coder (`claude`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while `gemini_wsl` can use its own tools for analysis, validation, and running shell commands.

### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`) - LOW-LEVEL EXECUTION:**
Use `apply_code_changes` for simple, direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**2. For Junior Architects (`gemini_wsl`) - HIGH-LEVEL DELEGATION:**
Use `execute_strategic_task` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

```json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to `routes/api.js`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

{{agentDefinitions}}



--- File: /src/templates/vectorMode.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: "{{userQuery}}"

* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.
* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** {{repoName}}
- **User Query:** "{{userQuery}}"
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

{{multiAgentSection}}

---

--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig, getAllProfiles } from '../config.js';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Helper function to read a template file or return the string if it's not a path
    const loadTemplate = async (templatePathOrString) => {
      if (templatePathOrString && (templatePathOrString.endsWith('.md') || templatePathOrString.endsWith('.txt'))) {
        try {
          // Resolve path relative to the project root. __dirname is src/utils.
          const resolvedPath = path.join(__dirname, '..', '..', templatePathOrString);
          return await fs.readFile(resolvedPath, 'utf-8');
        } catch (e) {
          return `ERROR: FAILED TO LOAD TEMPLATE ${templatePathOrString}: ${e.message}`;
        }
      }
      return templatePathOrString; // Fallback for old-style inline strings or errors
    };

    // P1 Bug Fix: Normalize manifest structure as per Consilium report
    function normalizeManifest(raw) {
      if (!raw) return null;
      const out = {};
      // Handle `setup.json` structure (e.g., `projectContext.name`)
      if (raw.projectContext) {
        out.context = raw.projectContext.description || JSON.stringify(raw.projectContext, null, 2);
        out.operations = raw.operations || raw.projectContext.operations || ''; // Assuming .eck/OPERATIONS.md is separate
        out.journal = raw.journal || raw.projectContext.journal || ''; // Assuming .eck/JOURNAL.md is separate
        out.environment = raw.environment || raw.projectContext.environment || {}; // Assuming .eck/ENVIRONMENT.md is separate
      } else {
        // Handle direct .eck file structure (e.g., raw.context from CONTEXT.md)
        out.context = raw.context || '';
        out.operations = raw.operations || '';
        out.journal = raw.journal || '';
        out.environment = raw.environment || {};
      }
      // Add fallback text if still empty
      if (!out.context) out.context = 'No project context provided.';
      if (!out.operations) out.operations = 'No operations guide provided.';
      if (!out.journal) out.journal = 'No journal entries found.';

      return out;
    }

    // --- Build common context sections --- 
    const projectOverview = `### PROJECT OVERVIEW
- **Project:** ${context.repoName || 'Unknown'}
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.
`;
    const normalizedEck = normalizeManifest(context.eckManifest);
    let eckManifestSection = '';
    if (normalizedEck) {
      eckManifestSection = buildEckManifestSection(normalizedEck);
    } else {
      eckManifestSection = '### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nWARNING: .eck manifest was not found or was empty.\n';
    }
    // --- End context building ---


    // Check if agent mode is enabled
    if (context.options && context.options.agent) {
      const agentPromptTemplate = await loadTemplate(promptTemplates.agent);

      const agentHeader = `${agentPromptTemplate}

${projectOverview}
${eckManifestSection}
---

## Project Snapshot Information

- **Project**: ${context.repoName || 'Unknown'}
- **Timestamp**: ${new Date().toISOString()}
- **Files Included**: ${context.stats ? context.stats.includedFiles : 'Unknown'}
- **Total Files in Repo**: ${context.stats ? context.stats.totalFiles : 'Unknown'}

---

`;
      return agentHeader;
    }

    // --- This is the main/Senior Architect prompt logic --- 
    let template;
    if (context.mode === 'vector') {
      template = await loadTemplate(promptTemplates.vectorMode);
      // Inject context for vector mode
      template = template.replace('{{multiAgentSection}}', `
${projectOverview}
${eckManifestSection}
`); 
    } else {
      template = await loadTemplate(promptTemplates.multiAgent);
      // --- INJECT DYNAMIC CONTEXT --- 
      template = template.replace('{{projectOverview}}', projectOverview);
      template = template.replace('{{eckManifestSection}}', eckManifestSection);
      // --- END INJECT --- 
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions
    };

    let renderedTemplate = render(template, data);
    
    // Inject dynamic profile context if a profile is active
    if (context.options && context.options.profile && context.repoPath) {
      let metadataHeader = '\n\n## Partial Snapshot Context\n';
      metadataHeader += `- **Profile(s) Active:** ${context.options.profile}\n`;
      try {
          const allProfiles = await getAllProfiles(context.repoPath);
          const activeProfileNames = context.options.profile.split(',').map(p => p.trim().replace(/^-/, ''));
          const allProfileNames = Object.keys(allProfiles).filter(p => !activeProfileNames.includes(p));
          if (allProfileNames.length > 0) {
               metadataHeader += `- **Other Available Profiles:** ${allProfileNames.join(', ')}\n`;
          }
      } catch (e) { /* fail silently on metadata generation */ }
      
      const insertMarker = "### HIERARCHICAL AGENT WORKFLOW"; // Use our new marker
      renderedTemplate = renderedTemplate.replace(insertMarker, metadataHeader + '\n' + insertMarker);
    }

    return renderedTemplate;

  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header', error.message);
    return `# Snapshot for ${context.repoName || 'Project'}

Generated: ${new Date().toISOString()}

---

`;
  }
}

--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { dispatchAnalysisTask } from '../services/dispatcherService.js';
import { getProfile } from '../config.js';
import micromatch from 'micromatch';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = '.eck/';
  const comment = '# Added by eck-snapshot to ignore metadata directory';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

// Helper function to determine if a string is a glob pattern
function isGlob(str) {
  return str.includes('*') || str.includes('?') || str.includes('{');
}

/**
 * Applies advanced profile filtering (multi-profile, exclusion, and ad-hoc globs) to a file list.
 */
export async function applyProfileFilter(allFiles, profileString, repoPath) {
  const profileParts = profileString.split(',').map(p => p.trim()).filter(Boolean);
  
  const includeGlobs = [];
  const excludeGlobs = [];
  const includeNames = [];
  const excludeNames = [];

  // Step 1: Differentiate between profile names and ad-hoc glob patterns
  for (const part of profileParts) {
    const isNegative = part.startsWith('-');
    const pattern = isNegative ? part.substring(1) : part;

    if (isGlob(pattern)) {
      if (isNegative) {
        excludeGlobs.push(pattern);
      } else {
        includeGlobs.push(pattern);
      }
    } else {
      if (isNegative) {
        excludeNames.push(pattern);
      } else {
        includeNames.push(pattern);
      }
    }
  }

  let workingFiles = [];
  let finalIncludes = [...includeGlobs];
  let finalExcludes = [...excludeGlobs];

  // Step 2: Load patterns from specified profile names
  const allProfileNames = [...new Set([...includeNames, ...excludeNames])];
  const profiles = new Map();
  for (const name of allProfileNames) {
    const profile = await getProfile(name, repoPath);
    if (profile) {
      profiles.set(name, profile);
    } else {
      // This is an ad-hoc glob, not a profile, so no warning is needed.
      if (!isGlob(name)) {
        console.warn(`‚ö†Ô∏è Warning: Profile '${name}' not found and will be skipped.`);
      }
    }
  }

  for (const name of includeNames) {
    if (profiles.has(name)) {
      finalIncludes.push(...(profiles.get(name).include || []));
      finalExcludes.push(...(profiles.get(name).exclude || []));
    }
  }
  for (const name of excludeNames) {
    if (profiles.has(name)) {
      finalExcludes.push(...(profiles.get(name).include || []));
    }
  }
  
  // Step 3: Apply the filtering logic
  if (finalIncludes.length > 0) {
    workingFiles = micromatch(allFiles, finalIncludes);
  } else if (includeNames.length > 0 && includeGlobs.length === 0) {
    workingFiles = [];
  } else {
    workingFiles = allFiles;
  }

  if (finalExcludes.length > 0) {
    workingFiles = micromatch.not(workingFiles, finalExcludes);
  }

  return workingFiles;
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');
  
  try {
    // Check if .eck directory already exists and has all required files
    let needsInitialization = false;
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory exists, check if all required files are present
        const requiredFiles = ['ENVIRONMENT.md', 'CONTEXT.md', 'OPERATIONS.md', 'JOURNAL.md'];
        for (const fileName of requiredFiles) {
          try {
            await fs.stat(path.join(eckDir, fileName));
          } catch (error) {
            console.log(`   ‚ÑπÔ∏è Missing ${fileName}, initialization needed`);
            needsInitialization = true;
            break;
          }
        }
        if (!needsInitialization) {
          // All files exist, no need to initialize
          return;
        }
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
      needsInitialization = true;
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');

    // --- NEW HYBRID LOGIC --- 
    // 1. Run static analysis first to gather facts.
    let staticFacts = {};
    try {
      staticFacts = await detectProjectType(projectPath);
      console.log(`   üîç Static analysis complete. Detected type: ${staticFacts.type}`);
    } catch (e) {
      console.warn(`   ‚ö†Ô∏è Static project detection failed: ${e.message}. Proceeding with generic prompts.`);
    }

    // Prevent AI hallucination by removing low-confidence "other possibilities"
    if (staticFacts && staticFacts.allDetections) {
      delete staticFacts.allDetections;
    }
    
    const staticFactsJson = JSON.stringify(staticFacts, null, 2);
    // --- END NEW LOGIC ---
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'ENVIRONMENT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw YAML key-value content for an .eck/ENVIRONMENT.md file. Only include detected facts. DO NOT add any keys that are not present in the facts. DO NOT add conversational text or markdown wrappers. Your response MUST start directly with a YAML key (e.g., 'project_type: ...').`,
        content: `# This file is for environment overrides. Add agent-specific settings here.\nagent_id: local_dev\n` // Simple static fallback
      },
      {
        name: 'CONTEXT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/CONTEXT.md file. Use the facts to write ## Description, ## Architecture, and ## Key Technologies. DO NOT add conversational text (like "Here is the file..."). Your response MUST start *directly* with the '# Project Overview' heading.`,
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: `Given these static project analysis facts (especially package.json scripts):
${staticFactsJson}

Generate the raw Markdown content ONLY for a .eck/OPERATIONS.md file. DO NOT add conversational text. Your response MUST start *directly* with the '# Common Operations' heading. List commands for ## Development Setup, ## Running the Project, and ## Testing.`,
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/ROADMAP.md file. DO NOT add conversational text. Start *directly* with '# Project Roadmap'. Propose 1-2 *plausible* placeholder items for ## Current Sprint/Phase and ## Next Phase based on the project type.`,
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        prompt: `Generate the raw Markdown content ONLY for a .eck/TECH_DEBT.md file. DO NOT add conversational text. Start *directly* with '# Technical Debt'. Propose 1-2 *common* placeholder items for ## Code Quality Issues and ## Refactoring Opportunities.`,
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file (only if it doesn't exist)
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      
      // Skip if file already exists
      try {
        await fs.stat(filePath);
        console.log(`   ‚úÖ ${file.name} already exists, skipping`);
        continue;
      } catch (error) {
        // File doesn't exist, create it
      }
      
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For files with a prompt, try to dynamically generate
      if (file.prompt) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponseObject = await dispatchAnalysisTask(file.prompt); // Use the prompt
          const rawText = aiResponseObject.result || aiResponseObject.response_text; // Handle both Claude and GPT responses
          
          if (!rawText || typeof rawText.replace !== 'function') {
             throw new Error(`AI returned invalid content type: ${typeof rawText}`);
          }

          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = rawText.replace(/^```(markdown|yaml)?\n|```$/g, '').trim();
          
          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}


--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));

  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }

  const best = detections[0];

  // Special handling for mixed monorepos
  const isLikelyMonorepo = detections.length > 1 && detections.some(d => d.score >= 40);

  if (isLikelyMonorepo) {
    // If we have multiple strong detections, prefer the highest priority with substantial evidence
    const strongDetections = detections.filter(d => d.score >= 40);
    if (strongDetections.length > 1) {
      const primaryType = strongDetections[0].type;
      return {
        type: primaryType,
        confidence: Math.min(strongDetections[0].score / 100, 1.0),
        details: {
          ...strongDetections[0].details,
          isMonorepo: true,
          additionalTypes: strongDetections.slice(1).map(d => d.type)
        },
        allDetections: detections
      };
    }
  }

  // Boost confidence for strong workspace indicators
  if (best.details && (best.details.isWorkspace || best.details.workspaceSize)) {
    const boostedScore = best.score + 20; // Bonus for workspace structure
    return {
      type: best.type,
      confidence: Math.min(boostedScore / 100, 1.0),
      details: best.details,
      allDetections: detections
    };
  }

  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;

  // Check for required files (check both root and common subdirectories)
  if (pattern.files) {
    for (const file of pattern.files) {
      // Check in root directory first
      const rootExists = await fileExists(path.join(projectPath, file));
      if (rootExists) {
        score += 25; // Each required file adds points
      } else {
        // For Cargo.toml and other project files, also check common subdirectory patterns
        const commonSubdirs = ['src', 'lib', 'app', 'core', 'backend', 'frontend'];
        // Add project-type specific subdirectories
        if (file === 'Cargo.toml') {
          commonSubdirs.push('codex-rs', 'rust', 'server', 'api');
        }
        if (file === 'package.json') {
          commonSubdirs.push('codex-cli', 'cli', 'client', 'web', 'ui');
        }

        for (const subdir of commonSubdirs) {
          const subdirExists = await fileExists(path.join(projectPath, subdir, file));
          if (subdirExists) {
            score += 20; // Slightly lower score for subdirectory finds
            break; // Only count once per file type
          }
        }
      }
    }
  }

  // Check for required directories (check both root and one level deep)
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const rootExists = await directoryExists(path.join(projectPath, dir));
      if (rootExists) {
        score += 20; // Each required directory adds points
      } else {
        // Check in common project subdirectories
        const projectSubdirs = ['codex-rs', 'codex-cli', 'src', 'lib', 'app'];
        for (const projDir of projectSubdirs) {
          const subdirExists = await directoryExists(path.join(projectPath, projDir, dir));
          if (subdirExists) {
            score += 15; // Lower score for nested directory finds
            break;
          }
        }
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names (more precise matching)
        const foundInDeps = Object.keys(allDeps).some(dep => dep === patternText || dep.startsWith(patternText + '/'));
        // Only check for exact matches in keywords array, not description (too broad)
        const foundInKeywords = packageJson.keywords && Array.isArray(packageJson.keywords)
          ? packageJson.keywords.some(keyword => keyword.toLowerCase() === patternText.toLowerCase())
          : false;
        
        if (foundInDeps || foundInKeywords) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };

  try {
    // Check both root and common subdirectories for Cargo.toml
    let cargoPath = path.join(projectPath, 'Cargo.toml');
    let cargoContent = null;

    if (await fileExists(cargoPath)) {
      cargoContent = await fs.readFile(cargoPath, 'utf-8');
    } else {
      // Check common Rust project subdirectories
      const rustSubdirs = ['codex-rs', 'rust', 'src', 'core', 'server'];
      for (const subdir of rustSubdirs) {
        const subdirCargoPath = path.join(projectPath, subdir, 'Cargo.toml');
        if (await fileExists(subdirCargoPath)) {
          cargoPath = subdirCargoPath;
          cargoContent = await fs.readFile(subdirCargoPath, 'utf-8');
          details.primaryLocation = subdir;
          break;
        }
      }
    }

    if (!cargoContent) {
      return details;
    }

    const nameMatch = cargoContent.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = cargoContent.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = cargoContent.match(/edition\s*=\s*"([^"]+)"/);

    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];

    // Check if it's a workspace
    if (cargoContent.includes('[workspace]')) {
      details.isWorkspace = true;

      // Count workspace members
      const workspaceMatch = cargoContent.match(/members\s*=\s*\[([\s\S]*?)\]/);
      if (workspaceMatch) {
        const members = workspaceMatch[1].split(',').map(m => m.trim().replace(/"/g, '')).filter(m => m);
        details.workspaceMembers = members.length;
      }
    }

    // Check for multiple Cargo.toml files (indicates workspace structure)
    if (details.primaryLocation) {
      const subdirPath = path.join(projectPath, details.primaryLocation);
      try {
        const subdirs = await fs.readdir(subdirPath, { withFileTypes: true });
        let cargoCount = 0;
        for (const entry of subdirs) {
          if (entry.isDirectory()) {
            const memberCargoPath = path.join(subdirPath, entry.name, 'Cargo.toml');
            if (await fileExists(memberCargoPath)) {
              cargoCount++;
            }
          }
        }
        if (cargoCount > 3) { // If many workspace members, this is definitely a Rust project
          details.workspaceSize = 'large';
        }
      } catch (error) {
        // Ignore
      }
    }

  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }

  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'c': [0, 0.23, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    const parsedData = JSON.parse(data);
    // Ensure the structure is complete by merging with defaults
    return {
        coefficients: { ...DEFAULT_COEFFICIENTS, ...parsedData.coefficients },
        trainingPoints: parsedData.trainingPoints || {}
    };
  } catch (error) {
    // If file doesn't exist or is malformed, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];

  if (!points || points.length === 0) {
    // No points, nothing to do.
    return;
  }

  if (points.length === 1) {
    // With one point, use a direct ratio for the linear coefficient.
    const point = points[0];
    if (point.fileSizeInBytes > 0) { // Avoid division by zero
        const ratio = point.actualTokens / point.fileSizeInBytes;
        data.coefficients[projectType] = [
            0, // intercept
            Math.max(0, ratio), // linear term (slope)
            0, 0 // quadratic, cubic
        ];
    }
    return;
  }

  // Use linear regression for 2 or more points.
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;

  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;

    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }

  const denominator = (n * sumX2 - sumX * sumX);
  if (denominator === 0) return; // Avoid division by zero, can't calculate slope

  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / denominator;
  const intercept = (sumY - slope * sumX) / n;

  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

--- File: /test_knexfile.js ---

module.exports = {
  development: {
    client: 'sqlite3',
    connection: {
      filename: './dev.sqlite3'
    }
  },
  
  staging: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  },
  
  production: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  }
};


--- File: /test_snapshot.md ---

# Test Snapshot

## Project Overview
This is a test project for demonstrating the Gemini session API integration.

## Files
- `index.js` - Main entry point
- `src/` - Source code directory

## Instructions
You are an AI assistant helping with this test project. Please respond to user queries about the codebase.

--- File: /vitest.config.js ---

import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    testMatch: ['**/*.{test,spec}.js'],
    coverage: {
      reporter: ['text', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        '**/*.config.js',
        '**/*.test.js',
        '**/*.spec.js'
      ]
    }
  }
});



--- File: /test_prune.md/eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_pruned_50KB.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-10-11T12:17:51.271Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-c": "^0.21.4",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject, viewIndex } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .action(createRepoSnapshot)
    .addHelpText('after', `
Examples for --profile:
  --profile backend                      (Uses the 'backend' profile)
  --profile "backend,-**/tests/**"         (Uses 'backend' profile, excludes all test files)
  --profile "src/**/*.js,-**/*.test.js"  (Includes all JS files in src, excludes tests)

  Combine predefined profiles (from .eck/profiles.json) with ad-hoc glob patterns.
  Prefix a profile name or glob pattern with '-' to exclude it.
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Index view command
  program
    .command('index-view')
    .description('View the contents of the code chunks database')
    .option('--limit <number>', 'Number of records to display', (val) => parseInt(val), 10)
    .option('--offset <number>', 'Number of records to skip', (val) => parseInt(val), 0)
    .option('--file <path>', 'Filter by file path')
    .action(viewIndex);

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return {
          content: `

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    let files = await getProjectFiles(projectPath);
    
    // --- Apply Advanced Profile Filtering ---
    const defaultProfile = await getProfile('default', projectPath);
    if (options.profile) {
        mainSpinner.text = `Applying profile filter: '${options.profile}'...`;
        files = await applyProfileFilter(files, options.profile, projectPath);
        mainSpinner.info(`Filtered down to ${files.length} files using profile: '${options.profile}'.`);
    } else if (defaultProfile) {
        mainSpinner.text = "Applying detected 'default' profile...";
        files = micromatch(files, defaultProfile.include, { ignore: defaultProfile.exclude });
        mainSpinner.info(`Filtered down to ${files.length} files using detected 'default' profile.`);
    }
    if (files.length === 0) {
        throw new Error(`Profile filter resulted in 0 files. Aborting.`);
    }
    // --- End Profile Filtering ---

    const profileName = options.profile || 'default';
    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await releaseAnalysisModel();
    await releaseEmbeddingModel();
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}


--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}



--- File: /test_snapshot.md ---

# Test Snapshot

## Project Overview
This is a test project for demonstrating the Gemini session API integration.

## Files
- `index.js` - Main entry point
- `src/` - Source code directory

## Instructions
You are an AI assistant helping with this test project. Please respond to user queries about the codebase.

--- File: /test_verify_std/eckSnapshot_snapshot_2025-12-21_23-04-30_244e354.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-12-21T23:04:30.557Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 46
- **Total Files in Repo:** 59

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.



## Project-Specific Manifest (.eck Directory)

This project includes a `.eck` directory with specific context and configuration:

### Project Context

# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.

### Operations Guide

# Common Operations

## Development Setup
```bash
# Setup commands
npm install
# or yarn install
```

## Running the Project
```bash
# Development mode
npm run dev

# Production build
npm run build
```

## Testing
```bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
```

## Deployment
```bash
# Deployment commands
npm run deploy
```

## Troubleshooting
Common issues and their solutions.

### Development Journal

---
type: refactor
scope: cli
summary: Make JA workflow instructions conditional
timestamp: 2025-12-21T22:09:31Z
---

- Replaced static JA workflow text in templates with dynamic placeholders
- Updated aiHeader.js to only inject JA instructions if --with-ja flag is present
- Simplified workflow description for standard snapshots
---
task_id: feat-browser-automation-config-v1
date: 2025-12-21
type: feat
scope: config
---

# Enable Claude Chrome MCP browser automation capabilities

- Added browser automation capabilities to local_dev agent: 'browser automation (chrome_mcp)', 'visual regression testing', 'network logging'
- Created browserAutomation section in aiInstructions with detailed capabilities and restrictions
- Documented Chrome MCP integration for frontend testing, debugging, and visual regression

---
task_id: refine-help-guide-text-v2
date: 2025-11-08
type: docs
scope: cli
---

# Refine help text for generate-profile-guide

- Clarified that `generate-profile-guide` is the recommended alternative to `profile-detect` for very large projects where the underlying AI's context window may be insufficient.

---
task_id: feat-implement-english-help-guide-v2
date: 2025-11-08
type: feat
scope: cli
---

# Implement detailed, workflow-driven help text

- Replaced the main `--help` output with a step-by-step guide in English, formatted for console readability.
- The guide now prioritizes the core workflow: snapshot, profile-detect, using profiles, and pruning.
- Added clear, console-style usage examples for each key step.

---
task_id: fix-profile-detect-and-add-index-viewer-v1
date: 2025-10-10
type: fix
scope: cli
---

# Fix JSON parsing in profile-detect

- Modified extractJson to be more robust against AI log wrappers
- Finds first '{' and last '}' to extract JSON from surrounding text
- Added /managed_components/ to .gitignore
- Implemented new index-view command to inspect code chunks database
- This resolves the crash when running the profile-detect command

---
task_id: feat-c-language-support-v1
date: 2025-10-10
type: feat
scope: core
---

# Add support for C language projects

- Added C project detection (Makefile, CMakeLists.txt, *.c, *.h)
- Added C-specific file filtering for compiled objects (.o, .a, .so)
- Installed tree-sitter-c@0.21.4 parser
- Integrated C parser into segmenter for functions, structs, enums, unions
- Added token estimation coefficients for C (0.23 ratio)

---
task_id: refactor-claude-skip-permissions-default-v1
date: 2025-09-28T14:30:00.000Z
type: refactor
scope: claude-cli
---
## Force --dangerously-skip-permissions for all Claude calls

- Removed the user-facing '--dangerously-skip-permissions' option from 'ask-claude' and 'ask-claude-session' commands.
- Hardcoded the flag in the service layer to ensure all calls to 'claude-code' are non-interactive by default.
- This change improves the reliability of the tool in automated workflows by preventing it from hanging on permission prompts.

---
task_id: refactor-gpt-service-to-codex-cli-v1
date: 2025-09-28T14:00:00.000Z
type: refactor
scope: services
---
## Switch ask-gpt from chatgpt-cli to official codex CLI

- Replaced `npx chatgpt` calls with the official `codex` CLI, using the `exec --full-auto` command for machine-readable output.
- Implemented an automatic login flow that detects authentication errors and triggers the interactive `codex login` command.
- Created a new `authService.js` to handle the login initiation.
- Removed the `open` package dependency as it is no longer needed.
- Updated tests in `gptService.test.js` to mock the new `codex` command flow.
- Added comprehensive documentation in README.md for both ChatGPT and Claude Code integration.
- Enhanced CLI help with detailed examples and authentication instructions.

---
task_id: gpt-test-1
date: 2025-09-28T09:24:01.314Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:41.532Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:33.203Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:43.135Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:27.678Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:04.054Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: configure-claude-auto-accept-v1
date: 2025-09-14T23:34:21Z
type: feat
scope: workflow
---

## Enable and document claude-code auto-accept mode

Created a global `settings.json` for claude-code to enable `acceptEdits` by default, allowing for fully autonomous operation. Added a `CLAUDE_SETUP.md` file to document this essential configuration step for new developers or fresh installations.

---
task_id: create-eck-commit-command-v1
date: 2025-09-14T23:29:38Z
type: feat
scope: workflow
---

## Create custom /eck:commit claude-code command

Added a custom slash command to automate the new structured journaling and conventional commit process. This command takes structured input (type, scope, summary, details) and uses it to update JOURNAL.md and create a git commit, enforcing our new workflow.

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure

### Environment Overrides

The following environment settings override auto-detected values:

- **agent_id**: local_dev

**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.

---



### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### AGENT WORKFLOW

Your role is **Architect**. You formulate technical plans and delegate code implementation tasks directly to the **Coder** agents (e.g., `local_dev`).

  - **Architect (You):** Sets strategy, defines tasks.
  - **Coder (e.g., `local_dev`):** Receives precise coding tasks and executes them.

### COMMAND FORMATS

You MUST use the following JSON command format for Coders:

**For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`):**
Use `apply_code_changes` for direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation (chrome_mcp), visual regression testing, network logging
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces

### Gemini WSL Agent (Junior Architect) (ID: "gemini_wsl")
- **Description:** Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.
- **GUI Support:** No (Headless)
- **Capabilities:** git operations, npm install, file editing, gemini (self), claude (delegate)
- **Restrictions:** Runs only inside the WSL environment

### Gemini Windows Agent (Standalone) (ID: "gemini_windows")
- **Description:** Gemini, running in native Windows (PowerShell). Can only access Windows tools.
- **GUI Support:** Yes
- **Capabilities:** git operations, npm install, file editing, gemini (self)
- **Restrictions:** Runs only in native Windows, Cannot access WSL-only tools like claude



## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .gemini/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îî‚îÄ‚îÄ gpt.toml
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ commands/
‚îÇ               ‚îî‚îÄ‚îÄ extensions/
‚îÇ                   ‚îî‚îÄ‚îÄ sample-extension.json
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ verify_changes.js
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ askGpt.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoDocs.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProfiles.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generateProfileGuide.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruneSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.test.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.test.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcherService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gptService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gptService.test.js
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architect-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ envScanRequest.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitWorkflow.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgent.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorMode.md
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ test_prune.md/
‚îÇ   ‚îú‚îÄ‚îÄ eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_ja.md
‚îÇ   ‚îú‚îÄ‚îÄ eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_pruned_50KB.md
‚îÇ   ‚îî‚îÄ‚îÄ eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e.md
‚îú‚îÄ‚îÄ test_verify_std/
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ codex_delegation_snapshot.md
‚îú‚îÄ‚îÄ COMMANDS_REFERENCE.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ setup.json
‚îú‚îÄ‚îÄ setup.json.bak
‚îú‚îÄ‚îÄ test_snapshot.md
‚îî‚îÄ‚îÄ vitest.config.js
```

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs": [
      0,
      0.3091314011805184,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "c": [
      0,
      0.45718772384153394,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs-monorepo": [
      0,
      0.30927098145345017,
      0,
      0
    ]
  },
  "trainingPoints": {
    "nodejs": [
      {
        "fileSizeInBytes": 229899,
        "estimatedTokens": 45980,
        "actualTokens": 71069,
        "timestamp": "2025-09-21T23:52:36.862Z"
      }
    ],
    "nodejs-monorepo": [
      {
        "fileSizeInBytes": 261612,
        "estimatedTokens": 65403,
        "actualTokens": 80909,
        "timestamp": "2025-09-22T01:50:24.392Z"
      }
    ],
    "c": [
      {
        "fileSizeInBytes": 72596,
        "estimatedTokens": 16697,
        "actualTokens": 33190,
        "timestamp": "2025-10-13T22:41:24.445Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Environment variables for eck-snapshot
# Add any custom environment variables here if needed

--- File: /.gemini/tools/gpt.toml ---

description = 'Ask GPT (ChatGPT subscription) for coding tasks.'
prompt = '!{node $(pwd)/index.js ask-gpt "{{args}}"}'  # Dynamic path


--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# Generated snapshot files
*_rag_*.md

# Environment variables
.env
.env.local
.env.production



# Added by eck-snapshot to ignore metadata directory
.eck/

# Test snapshot directory
create-snapshot/

# Claude Code settings (exclude all except commands/eck)
.claude/
!.claude/commands/
!.claude/commands/eck/
!.claude/commands/eck/**

# Third-party managed components
/managed_components/


--- File: /COMMANDS_REFERENCE.md ---

# Commands Reference

This document contains essential commands and setup instructions for the project's multi-agent architecture.

## Junior Architect (`gemini_wsl`) Setup Guide

This section explains how the `gemini_wsl` agent (Junior Architect) is configured to delegate coding tasks to the `claude` agent (Coder).

This architecture relies on `gemini-cli`'s custom tool feature.

## 1. Agent Definition

The Junior Architect (JA) is the `gemini_wsl` agent, which is an instance of `gemini-cli` running in WSL. It is defined in `setup.json`.

## 2. Custom Command (`claude.toml`)

The JA's ability to delegate to the Coder (`claude`) is defined by a custom `gemini-cli` command.

This command must be defined in a file named `claude.toml` and placed in the `gemini-cli` configuration directory (e.g., `~/.gemini/tools/claude.toml`).

### `claude.toml` Content

```toml
description = "Ask Claude (from the eckSnapshot project) to help with a task."
prompt = """!{node /mnt/c/Users/xelth/eckSnapshot/index.js ask-claude "{{args}}"}"""
```

## 3. JA Prompt (`agent-prompt.template.md`)

The JA *knows* how to *use* this command because its main system prompt (loaded from `src/templates/agent-prompt.template.md`) instructs it to.

This prompt *mandates* that the `{{args}}` it passes to the `/claude` command must be a single-line **JSON string** in the `apply_code_changes` format.

## 4. Execution Flow

1.  **Senior Architect (Gemini)** gives a high-level `execute_strategic_task` to `gemini_wsl`.
2.  **`gemini_wsl` (JA)** analyzes the task and formulates a low-level `apply_code_changes` JSON payload.
3.  **`gemini_wsl`** executes `/claude` with the JSON payload as a single string argument (`{{args}}`).
4.  **`claude.toml`** executes the `eck-snapshot ask-claude "{...}"` shell command.
5.  **`eck-snapshot`** (specifically `claudeCliService.js`) receives the JSON string as a 'prompt'.
6.  It forwards this prompt to the `claude-cli` binary (`local_dev`), which is smart enough to parse the JSON and execute the `apply_code_changes` task.

## Claude Code Commands

### Commit Command (`.claude/commands/eck/commit.md`)

A custom command for structured commits with automatic journaling. Place this file in `.claude/commands/eck/commit.md` to enable it in Claude Code.

**Usage:** `/commit <type> <scope> <summary> <details>`

**Function:**
- Stages all current changes
- Creates YAML frontmatter for journal entry with task_id, date, type, scope
- Creates markdown body with summary and details
- Prepends complete journal entry to `.eck/JOURNAL.md`
- Creates conventional commit message: `{type}({scope}): {summary}`
- Executes the commit

**Example:** `/commit feat api "Add user authentication" "Implemented JWT-based auth with login/logout endpoints"`

**Important:** This command should be preserved in git (via `.gitignore` rules) so it can be recreated if lost. The command integrates with the project's `.eck` manifest system for structured development journaling.

## Setup Gemini Command

### Dynamic Path Configuration

The `setup-gemini` command automates the creation of `claude.toml` with dynamic path resolution, eliminating the need for hardcoded paths like `/mnt/c/...`.

**Usage:** `eck-snapshot setup-gemini [options]`

**Options:**
- `-v, --verbose` - Show detailed output and error information

**Function:**
- Detects `gemini-cli` installation using `which()`
- Resolves current project path using `process.cwd()`
- Creates `~/.gemini/tools/claude.toml` with dynamic paths
- Integrates with `setup.json` for environment variable configuration
- Handles cross-platform compatibility (WSL/Windows/macOS/Linux)

**Example Generated `claude.toml`:**
```toml
[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["/home/user/projects/eckSnapshot/index.js", "ask-claude"]

[claude.metadata]
platform = "linux"
working_directory = "/home/user/projects/eckSnapshot"
```

**Benefits:**
- ‚úÖ No hardcoded `/mnt/c/` paths
- ‚úÖ Works across all platforms automatically
- ‚úÖ Resolves paths dynamically based on current working directory
- ‚úÖ Integrates with existing `setup.json` configuration
- ‚úÖ Validates `gemini-cli` and `index.js` availability before setup

## MCP Integration with Feedback Support

### MCP Feedback Object

The `mcp_feedback` object provides enhanced error reporting and status tracking for MCP (Message Control Protocol) integration with gemini-cli delegation.

**Structure:**
```json
{
  "mcp_feedback": {
    "success": true,
    "errors": [],
    "mcp_version": "1.0"
  }
}
```

**Properties:**
- `success` (boolean): Indicates if the MCP operation completed successfully
- `errors` (array): List of error messages or issues encountered during execution
- `mcp_version` (string): Version of MCP protocol used

**Usage in Payloads:**

Add `mcp_feedback` to the `post_execution_steps` object in your JSON command payloads:

```json
{
  "target_agent": "local_dev",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "Implement user authentication",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "auth",
        "summary": "Add JWT authentication"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**Implementation:**

The `claudeCliService.js` automatically parses `mcp_feedback` from incoming prompts and includes it in the response object. When errors are present, they are logged to the console for debugging purposes.

**Benefits:**
- ‚úÖ Enhanced error reporting for MCP operations
- ‚úÖ Version tracking for protocol compatibility
- ‚úÖ Automatic logging of MCP errors
- ‚úÖ Improved debugging for gemini-cli delegation issues

## Auto-Generated Gemini Extensions

*This section is automatically generated. Run `npm run docs:auto` to update.*

### sample-extension

Sample Gemini extension for demonstration

**Commands:**

- **sample-command**: A sample command for testing auto-docs
  - Usage: `sample-command [options]`
  - Examples: `sample-command --help`

**Tools:**

- **sample-tool**: A sample tool for testing auto-docs
  - Usage: Use this tool for sample operations



--- File: /codex_delegation_snapshot.md ---

üîç WSL detected, using configured host: 172.29.16.1


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

const { run } = await import('./src/cli/cli.js');
run();


--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /packages/cli/src/commands/extensions/sample-extension.json ---

{
  "name": "sample-extension",
  "description": "Sample Gemini extension for demonstration",
  "commands": [
    {
      "name": "sample-command",
      "description": "A sample command for testing auto-docs",
      "usage": "sample-command [options]",
      "examples": [
        "sample-command --help"
      ]
    }
  ],
  "tools": [
    {
      "name": "sample-tool",
      "description": "A sample tool for testing auto-docs",
      "usage": "Use this tool for sample operations"
    }
  ]
}

--- File: /scripts/verify_changes.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

console.log('üß™ Starting Verification Suite...');

async function verifySnapshots() {
  console.log('\n[1/2] Testing Snapshot Logic...');
  try {
    // 1. Generate Standard Snapshot
    await execa('node', ['index.js', 'snapshot', '--no-tree', '--output', 'test_verify_std']);
    const stdFiles = await fs.readdir('test_verify_std');
    const stdContent = await fs.readFile(path.join('test_verify_std', stdFiles.find(f => f.endsWith('.md'))), 'utf-8');

    if (stdContent.includes('HIERARCHICAL AGENT WORKFLOW')) {
      throw new Error('‚ùå Standard snapshot contains JA instructions (Should not happen!)');
    }
    console.log('‚úÖ Standard snapshot: OK (Simple workflow)');

    // 2. Generate JA Snapshot
    await execa('node', ['index.js', 'snapshot', '--with-ja', '--no-tree', '--output', 'test_verify_ja']);
    const jaFiles = await fs.readdir('test_verify_ja');
    const jaContent = await fs.readFile(path.join('test_verify_ja', jaFiles.find(f => f.includes('_ja.md'))), 'utf-8');

    if (!jaContent.includes('HIERARCHICAL AGENT WORKFLOW')) {
      throw new Error('‚ùå JA snapshot missing JA instructions');
    }
    console.log('‚úÖ JA snapshot: OK (Hierarchical workflow)');

  } catch (e) {
    console.error('Snapshot verification failed:', e.message);
    process.exit(1);
  }
}

verifySnapshots();


--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": [
          "build.gradle",
          "build.gradle.kts",
          "settings.gradle",
          "settings.gradle.kts"
        ],
        "directories": [
          "app/src/main",
          "app/src/androidTest"
        ],
        "manifestFiles": [
          "AndroidManifest.xml"
        ],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": [
          "package.json"
        ],
        "directories": [
          "packages",
          "apps",
          "libs"
        ],
        "patterns": [
          "workspaces",
          "lerna",
          "nx",
          "rush"
        ],
        "priority": 7
      },
      "nodejs": {
        "files": [
          "package.json"
        ],
        "directories": [
          "node_modules"
        ],
        "priority": 6
      },
      "python-poetry": {
        "files": [
          "pyproject.toml"
        ],
        "patterns": [
          "tool.poetry"
        ],
        "priority": 9
      },
      "python-pip": {
        "files": [
          "requirements.txt",
          "setup.py",
          "setup.cfg"
        ],
        "directories": [
          "__pycache__",
          "venv",
          ".venv"
        ],
        "priority": 7
      },
      "python-conda": {
        "files": [
          "environment.yml",
          "environment.yaml",
          "conda.yml"
        ],
        "priority": 8
      },
      "django": {
        "files": [
          "manage.py"
        ],
        "patterns": [
          "django",
          "Django"
        ],
        "priority": 9
      },
      "flask": {
        "files": [
          "app.py",
          "application.py"
        ],
        "patterns": [
          "flask",
          "Flask"
        ],
        "priority": 8
      },
      "flutter": {
        "files": [
          "pubspec.yaml"
        ],
        "directories": [
          "lib",
          "android",
          "ios"
        ],
        "priority": 8
      },
      "react-native": {
        "files": [
          "package.json"
        ],
        "directories": [
          "android",
          "ios"
        ],
        "patterns": [
          "react-native"
        ],
        "priority": 8
      },
      "rust": {
        "files": [
          "Cargo.toml"
        ],
        "directories": [
          "src",
          "target"
        ],
        "priority": 9
      },
      "go": {
        "files": [
          "go.mod",
          "go.sum"
        ],
        "directories": [
          "cmd",
          "pkg",
          "internal"
        ],
        "priority": 7
      },
      "dotnet": {
        "files": [
          "*.csproj",
          "*.sln",
          "*.fsproj",
          "*.vbproj"
        ],
        "directories": [
          "bin",
          "obj"
        ],
        "priority": 7
      },
      "c": {
        "files": [
          "Makefile",
          "CMakeLists.txt",
          "*.c",
          "*.h"
        ],
        "directories": [
          "src",
          "include",
          "lib"
        ],
        "priority": 6
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "npm-shrinkwrap.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*",
      "LICENSE*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "coverage/",
      "create-snapshot/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      },
      "c": {
        "filesToIgnore": [
          "*.o",
          "*.a",
          "*.so",
          "*.out",
          "*.exe"
        ],
        "dirsToIgnore": [
          "build/",
          "cmake-build-debug/",
          "cmake-build-release/",
          ".cmake/"
        ],
        "extensionsToIgnore": [
          ".o",
          ".a",
          ".so",
          ".out"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./.eck/snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation (chrome_mcp)",
          "visual regression testing",
          "network logging"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      },
      "gemini_wsl": {
        "active": true,
        "name": "Gemini WSL Agent (Junior Architect)",
        "description": "Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "gemini",
            "claude"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)",
          "claude (delegate)"
        ],
        "restrictions": [
          "Runs only inside the WSL environment"
        ]
      },
      "gemini_windows": {
        "active": true,
        "name": "Gemini Windows Agent (Standalone)",
        "description": "Gemini, running in native Windows (PowerShell). Can only access Windows tools.",
        "guiSupport": true,
        "identification": {
          "markers": [
            "Windows",
            "gemini",
            "!WSL"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)"
        ],
        "restrictions": [
          "Runs only in native Windows",
          "Cannot access WSL-only tools like claude"
        ]
      }
    },
    "browserAutomation": {
      "enabled": true,
      "provider": "Claude in Chrome MCP",
      "availableFor": ["local_dev"],
      "capabilities": {
        "navigation": "Navigate URLs, handle tabs/windows",
        "interaction": "Click, type, scroll, drag-and-drop",
        "inspection": "Read DOM, extract text, verify styles",
        "debugging": "Access console logs, network activity",
        "visual": "Generate screenshots, record GIF"
      },
      "restrictions": [
        "No non-consensual file downloads",
        "No sensitive credentials interaction"
      ]
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "src/templates/envScanRequest.md",
      "gitWorkflow": "src/templates/gitWorkflow.md",
      "multiAgent": "src/templates/multiAgent.md",
      "vectorMode": "src/templates/vectorMode.md",
      "agent": "src/templates/agent-prompt.template.md"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { generateProfileGuide } from './commands/generateProfileGuide.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  const helpGuide = `eck-snapshot (v4.0.0) - A lightweight, platform-independent CLI for creating project snapshots.

--- Getting Started: Environment Setup ---

This tool is designed to work with Large Language Models (LLMs). For the best results, you'll need:
1. An 'Architect' LLM (like Gemini, GPT-4, or Grok) to analyze snapshots.
2. A 'Coder' LLM (like Claude Code) to execute coding tasks.

--- Core Workflow: A Step-by-Step Guide ---

Step 1: Create a Full Project Snapshot
This is your primary command. It scans your project and packs all code into a single file.

> Usage:
  $ eck-snapshot

-> This creates a file like 'myProject_snapshot_... .md' in the .eck/snapshots/ directory.
   You can now pass this file to your Architect LLM for analysis.


Step 2: Handle Large Projects with Auto-Profiling
If your project is too big for the LLM's context window, \`profile-detect\` will automatically
slice it into logical parts (profiles) using AI.

> Usage:
  $ eck-snapshot profile-detect

-> Output:
  ‚ú® Detected Profiles:
  ---------------------------
    - cli
    - services
    - core
    - templates
    - docs
    - config


Step 3: Use Profiles to Create Focused Snapshots
Use the --profile option to create smaller snapshots of specific project areas.

> Example 1: Combine and exclude profiles
  $ eck-snapshot --profile "core,services,cli,-docs,-config"

-> Creates a snapshot with code from the 'core', 'services', and 'cli' profiles,
   while excluding anything from 'docs' and 'config'.

> Example 2: Use ad-hoc glob patterns
  $ eck-snapshot --profile "src/**/*.js,-**/*.test.js"

-> Includes all .js files in the 'src' directory and its subdirectories,
   but excludes any file ending in '.test.js'.
   Note: Quotes are required for complex patterns.


Step 4: Intelligently Prune a Snapshot
If a snapshot is still too large, \`prune\` uses AI to shrink it to a target size,
keeping only the most important files.

> Usage:
  $ eck-snapshot prune myProject_snapshot.md --target-size 500KB


Step 5 (Alternative): Truncate Files by Line Count
A faster, non-AI method to reduce size by keeping only the top N lines of each file.
Useful for a high-level overview.

> Usage:
  $ eck-snapshot --max-lines-per-file 200

--- Auxiliary Commands ---

- restore:                  Restore a project from a snapshot file.
- generate-profile-guide:   Creates a guide for manual profile creation. Use this if 'profile-detect' fails on very large projects, as it allows you to use an LLM with a larger context window (e.g., a web UI).
- detect:                   Show how eckSnapshot identifies your project type.
- ask-gpt / ask-claude:     Directly query the configured AI coder agents.
- setup-gemini:             Auto-configure integration with gemini-cli.
`;

  program
    .name('eck-snapshot')
    .description('A lightweight, platform-independent CLI for creating project snapshots.')
    .version('4.0.0')
    .addHelpText('before', helpGuide);

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .option('--with-ja', 'Generate a detailed snapshot for the Junior Architect agent')
    .option('--max-lines-per-file <number>', 'Truncate files to max N lines (e.g., 200 for compact snapshots)', (val) => parseInt(val))
    .action(createRepoSnapshot)
    .addHelpText('after', `
Profile Usage Guide:
  Profiles allow you to curate focused snapshots by filtering files using glob patterns.
  Define reusable profiles in .eck/profiles.json or use ad-hoc patterns directly.

Profile Structure (.eck/profiles.json):
  {
    "backend": {
      "include": ["src/api/**", "src/services/**"],
      "exclude": ["**/*.test.js"]
    },
    "frontend": {
      "include": ["src/components/**", "src/pages/**"],
      "exclude": ["**/*.spec.js"]
    }
  }

Examples:
  --profile backend
    Uses the 'backend' profile defined in .eck/profiles.json

  --profile "backend,-**/tests/**"
    Uses 'backend' profile, then excludes all test directories

  --profile "src/**/*.js,-**/*.test.js"
    Ad-hoc filtering: includes all JS files in src/, excludes test files

  --profile "frontend,src/utils/**"
    Combines 'frontend' profile with additional utility files

Glob Pattern Reference:
  **          Matches any number of directories
  *           Matches any characters within a directory level
  {a,b}       Matches either 'a' or 'b'
  [0-9]       Matches any digit
  -pattern    Prefix with '-' to exclude matching files

Creating Custom Profiles:
  1. Run: eck-snapshot generate-profile-guide
  2. Follow the generated guide in .eck/profile_generation_guide.md
  3. Save your custom profiles to .eck/profiles.json

  Alternatively, use AI detection:
    eck-snapshot profile-detect   (auto-generates profiles using AI)
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  program
    .command('generate-profile-guide')
    .description('Generate a markdown guide with a prompt and directory tree for manual profile creation')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('--config <path>', 'Configuration file path')
    .action((repoPath, options) => generateProfileGuide(repoPath, options));

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/cli/commands/askGpt.js ---

import { ask } from '../../services/gptService.js';

/**
 * CLI entry point for ask-gpt command.
 * @param {string} payload - JSON payload string.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} options - CLI options.
 */
export async function askGpt(payload, options = {}) {
  const verbose = Boolean(options.verbose);
  const model = options.model || 'gpt-5-codex';
  const reasoning = options.reasoning || 'high';

  if (!payload) {
    console.error('ask-gpt requires a JSON payload argument.');
    process.exitCode = 1;
    return;
  }

  try {
    const result = await ask(payload, { verbose, model, reasoning });
    console.log(JSON.stringify(result, null, 2));
  } catch (error) {
    console.error(error.message);
    if (verbose && error?.stack) {
      console.error(error.stack);
    }
    process.exitCode = 1;
  }
}


--- File: /src/cli/commands/autoDocs.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Auto-generate documentation from gemini-extension.json files
 */
export async function generateAutoDocs() {
  try {
    const projectRoot = path.resolve(__dirname, '../../../');
    const extensionsDir = path.join(projectRoot, 'packages/cli/src/commands/extensions');
    const referenceFile = path.join(projectRoot, 'COMMANDS_REFERENCE.md');

    // Check if extensions directory exists
    try {
      await fs.access(extensionsDir);
    } catch (error) {
      console.log(`Extensions directory not found at: ${extensionsDir}`);
      console.log('Creating example structure...');

      // Create the directory structure
      await fs.mkdir(extensionsDir, { recursive: true });

      // Create a sample gemini-extension.json file for demonstration
      const sampleExtension = {
        name: "sample-extension",
        description: "Sample Gemini extension for demonstration",
        commands: [
          {
            name: "sample-command",
            description: "A sample command for testing auto-docs",
            usage: "sample-command [options]",
            examples: ["sample-command --help"]
          }
        ],
        tools: [
          {
            name: "sample-tool",
            description: "A sample tool for testing auto-docs",
            usage: "Use this tool for sample operations"
          }
        ]
      };

      await fs.writeFile(
        path.join(extensionsDir, 'sample-extension.json'),
        JSON.stringify(sampleExtension, null, 2)
      );

      console.log('Created sample extension at:', path.join(extensionsDir, 'sample-extension.json'));
    }

    // Read all JSON files in the extensions directory
    const files = await fs.readdir(extensionsDir);
    const jsonFiles = files.filter(file => file.endsWith('.json'));

    if (jsonFiles.length === 0) {
      console.log('No JSON files found in extensions directory');
      return;
    }

    console.log(`Found ${jsonFiles.length} extension file(s): ${jsonFiles.join(', ')}`);

    // Parse each JSON file and extract command/tool information
    const extensions = [];

    for (const file of jsonFiles) {
      try {
        const filePath = path.join(extensionsDir, file);
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = JSON.parse(content);
        extensions.push({ filename: file, ...extension });
        console.log(`Parsed extension: ${extension.name || file}`);
      } catch (error) {
        console.warn(`Failed to parse ${file}:`, error.message);
      }
    }

    if (extensions.length === 0) {
      console.log('No valid extension files found');
      return;
    }

    // Generate markdown content
    let markdownContent = '\n## Auto-Generated Gemini Extensions\n\n';
    markdownContent += '*This section is automatically generated. Run `npm run docs:auto` to update.*\n\n';

    for (const extension of extensions) {
      markdownContent += `### ${extension.name || extension.filename}\n\n`;

      if (extension.description) {
        markdownContent += `${extension.description}\n\n`;
      }

      // Add commands section
      if (extension.commands && extension.commands.length > 0) {
        markdownContent += '**Commands:**\n\n';
        for (const command of extension.commands) {
          markdownContent += `- **${command.name}**: ${command.description || 'No description'}\n`;
          if (command.usage) {
            markdownContent += `  - Usage: \`${command.usage}\`\n`;
          }
          if (command.examples && command.examples.length > 0) {
            markdownContent += `  - Examples: ${command.examples.map(ex => `\`${ex}\``).join(', ')}\n`;
          }
        }
        markdownContent += '\n';
      }

      // Add tools section
      if (extension.tools && extension.tools.length > 0) {
        markdownContent += '**Tools:**\n\n';
        for (const tool of extension.tools) {
          markdownContent += `- **${tool.name}**: ${tool.description || 'No description'}\n`;
          if (tool.usage) {
            markdownContent += `  - Usage: ${tool.usage}\n`;
          }
        }
        markdownContent += '\n';
      }
    }

    // Read the current COMMANDS_REFERENCE.md
    let currentContent;
    try {
      currentContent = await fs.readFile(referenceFile, 'utf-8');
    } catch (error) {
      console.warn('COMMANDS_REFERENCE.md not found, creating new file');
      currentContent = '# Commands Reference\n\n';
    }

    // Remove existing auto-generated section if it exists
    const autoGenRegex = /\n## Auto-Generated Gemini Extensions[\s\S]*?(?=\n## |\n# |$)/;
    const updatedContent = currentContent.replace(autoGenRegex, '') + markdownContent;

    // Write the updated content back to the file
    await fs.writeFile(referenceFile, updatedContent);

    console.log('\n‚úÖ Auto-documentation generated successfully!');
    console.log(`üìù Updated: ${referenceFile}`);
    console.log(`üì¶ Processed ${extensions.length} extension(s)`);

  } catch (error) {
    console.error('Failed to generate auto-docs:', error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }

        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        let outputBody = content;

        // Apply max-lines-per-file truncation if specified
        if (options.maxLinesPerFile && options.maxLinesPerFile > 0) {
          const lines = outputBody.split('\n');
          if (lines.length > options.maxLinesPerFile) {
            outputBody = lines.slice(0, options.maxLinesPerFile).join('\n') +
              `\n\n[... truncated ${lines.length - options.maxLinesPerFile} lines ...]`;
          }
        }

        return {
          content: `--- File: /${normalizedPath} ---\n\n${outputBody}\n\n`,
          path: normalizedPath,
          size: fileStats.size
        };
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const successfulFileObjects = results.filter(Boolean);
    const contentArray = successfulFileObjects.map(f => f.content);

    // Return all processed data instead of writing file
    return {
      stats,
      contentArray,
      successfulFileObjects,
      allFiles,
      originalCwd,
      repoPath
    };
    
  } finally {
    process.chdir(originalCwd); // Ensure we always change back
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    spinner.succeed('Creating snapshots...');
      
      // Step 1: Process all files ONCE
      const { 
        stats, 
        contentArray, 
        successfulFileObjects, 
        allFiles, 
        originalCwd: processingOriginalCwd, // We get originalCwd from the processing function
        repoPath: processedRepoPath 
      } = await processProjectFiles(repoPath, options, config, projectDetection.type);

      const originalCwd = process.cwd(); // Get CWD *before* chdir
      process.chdir(processedRepoPath); // Go back to repo path for git hash and tree

      try {
        // --- Common Data --- 
        const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
        const repoName = path.basename(processedRepoPath);
        const gitHash = await getGitCommitHash(processedRepoPath);
        const fileExtension = options.format || config.defaultFormat || 'md';
        const outputPath = options.output || path.resolve(originalCwd, config.output);
        await fs.mkdir(outputPath, { recursive: true });

        const shouldIncludeTree = config.tree && !options.noTree;
        let directoryTree = '';
        if (shouldIncludeTree) {
          console.log('üå≥ Generating directory tree...');
          directoryTree = await generateDirectoryTree(processedRepoPath, '', allFiles, 0, config.maxDepth || 10, config);
        }

        // Calculate included file stats by extension
        const includedFilesByType = new Map();
        for (const fileObj of successfulFileObjects) {
            try {
                let ext = path.extname(fileObj.path);
                if (ext === '') ext = '.no-extension';
                includedFilesByType.set(ext, (includedFilesByType.get(ext) || 0) + 1);
            } catch (e) { /* Silently ignore */ }
        }
        const sortedIncludedStats = [...includedFilesByType.entries()].sort((a, b) => b[1] - a[1]);

        // Calculate Top 10 Largest Files
        const largestFiles = [...successfulFileObjects].sort((a, b) => b.size - a.size).slice(0, 10);

        const fileBody = (directoryTree ? `\n## Directory Structure\n\n\`\`\`\n${directoryTree}\`\`\`\n\n` : '') + contentArray.join('');

        // --- File 1: Architect Snapshot --- 
        const architectOptions = { ...options, agent: false };
        // Load manifest for headers
        const eckManifest = await loadProjectEckManifest(processedRepoPath);
        const isGitRepo = await checkGitRepository(processedRepoPath);

        const architectHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: architectOptions, repoPath: processedRepoPath }, isGitRepo);
        const architectBaseFilename = `${repoName}_snapshot_${timestamp}${gitHash ? `_${gitHash}` : ''}`;
        const architectFilename = `${architectBaseFilename}.${fileExtension}`;
        const architectFilePath = path.join(outputPath, architectFilename);
        await fs.writeFile(architectFilePath, architectHeader + fileBody);

        // --- File 2: Junior Architect Snapshot --- 
        let jaFilePath = null;
        if (options.withJa && fileExtension === 'md') { // Only create JA snapshot if requested and main is MD
          console.log('üñãÔ∏è Generating Junior Architect (_ja) snapshot...');
          const jaOptions = { ...options, agent: true, noTree: false, noAiHeader: false };
          const jaHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: jaOptions, repoPath: processedRepoPath }, isGitRepo);
          const jaFilename = `${architectBaseFilename}_ja.${fileExtension}`;
          jaFilePath = path.join(outputPath, jaFilename);
          await fs.writeFile(jaFilePath, jaHeader + fileBody);
        }

        // --- Combined Report --- 
        console.log('\n‚úÖ Snapshot generation complete!');
        console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        console.log(`üìÑ Architect File: ${architectFilePath}`);
        if (jaFilePath) {
          console.log(`üìÑ Junior Arch File: ${jaFilePath}`);
        }
        console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
        console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
        console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
        console.log(`üìã Format: ${fileExtension.toUpperCase()}`);

        if (sortedIncludedStats.length > 0) {
          console.log('\nüì¶ Included File Types:');
          console.log('---------------------------------');
          for (const [ext, count] of sortedIncludedStats.slice(0, 10)) {
              console.log(`   - ${String(ext).padEnd(15)} ${String(count).padStart(5)} files`);
          }
          if (sortedIncludedStats.length > 10) {
              console.log(`   ... and ${sortedIncludedStats.length - 10} other types.`);
          }
        }

        if (largestFiles.length > 0) {
          console.log('\nüêò Top 10 Largest Files (Included):');
          console.log('---------------------------------');
          for (const fileObj of largestFiles) {
              console.log(`   - ${String(formatSize(fileObj.size)).padEnd(15)} ${fileObj.path}`);
          }
        }
        
        // Excluded/Skipped Files Section
        const hasExcludedContent = stats.excludedFiles > 0 || stats.binaryFiles > 0 || stats.oversizedFiles > 0 || stats.ignoredFiles > 0 || stats.errors.length > 0;
        if (hasExcludedContent) {
          console.log('\nüö´ Excluded/Skipped Files:');
          console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        }
        
        if (stats.excludedFiles > 0) {
          console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
        }
        if (stats.binaryFiles > 0) {
          console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
        }
        if (stats.oversizedFiles > 0) {
          console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
        }
        if (stats.ignoredFiles > 0) {
          console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
        }
        if (stats.errors.length > 0) {
          console.log(`‚ùå Errors: ${stats.errors.length}`);
          if (options.verbose) {
            stats.errors.forEach(err => console.log(`   ${err}`));
          }
        }
        
        // Print detailed skip reasons report
        if (stats.skippedFilesDetails.size > 0) {
          console.log('\nüìã Skip Reasons:');
          console.log('---------------------------------');
          
          for (const [reason, files] of stats.skippedFilesDetails.entries()) {
            console.log(`\nüî∏ ${reason} (${files.length} files):`);
            files.forEach(file => {
              console.log(`   ‚Ä¢ ${file}`);
            });
          }
          console.log('---------------------------------');
        } else {
          console.log('---------------------------------');
        }
        
        // Generate training command string if estimation data is available
        if (estimation && projectDetection.type && !options.profile) {
          const trainingCommand = generateTrainingCommand(projectDetection.type, estimation.estimatedTokens, estimation.totalSize, repoPath);
          console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
          console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
          console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
        }

      } finally {
        process.chdir(originalCwd); // Final reset back to original CWD
      }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}


--- File: /src/cli/commands/detectProfiles.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { executePrompt as askClaude } from '../../services/claudeCliService.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Extracts a JSON object from a string that might contain markdown wrappers or log output.
 * Finds the first opening brace '{' and the last closing brace '}' to extract the JSON.
 */
function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }

  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1).trim();
  }

  return text.trim();
}

/**
 * Scans the project structure, saves the directory tree to a file, and asks an AI to generate
 * context profiles, saving them to .eck/profiles.json.
 */
export async function detectProfiles(repoPath, options) {
  const spinner = ora('Initializing and scanning project structure...').start();
  try {
    await initializeEckManifest(repoPath);

    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const config = {
        ...userConfig,
        ...setupConfig.fileFiltering,
        ...setupConfig.performance
    };

    const allFiles = await scanDirectoryRecursively(repoPath, config, repoPath);
    spinner.text = 'Generating directory tree...';
    const dirTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth, config);

    if (!dirTree) {
        throw new Error('Failed to generate directory tree or project is empty.');
    }

    spinner.text = 'Saving directory tree to file...';
    const treeFilePath = path.join(repoPath, '.eck', 'directory_tree_for_profiling.md');
    await fs.writeFile(treeFilePath, dirTree);

    const prompt = `You are a code architect. Based on the file directory tree found in the file at './.eck/directory_tree_for_profiling.md', please identify logical 'context profiles' for splitting the project.
Your output MUST be ONLY a valid JSON object.
The keys of the object MUST be the profile names (e.g., 'frontend', 'backend', 'core-logic', 'docs').
The values MUST be an object containing 'include' and 'exclude' arrays of glob patterns.
Example: {"frontend": {"include": ["packages/ui/**"], "exclude": []}, "docs": {"include": ["docs/**"], "exclude": []}}.
DO NOT add any conversational text, introductory sentences, or explanations. Your entire response must be ONLY the JSON object.`;

    spinner.text = 'Asking AI to analyze directory tree and detect profiles...';
    const aiResponseObject = await askClaude(prompt, { taskSize: allFiles.length });
    const rawText = aiResponseObject.result;

    if (!rawText || typeof rawText.replace !== 'function') {
      throw new Error(`AI returned invalid content type: ${typeof rawText}`);
    }

    spinner.text = 'Saving generated profiles...';
    const cleanedJson = extractJson(rawText);
    let parsedProfiles;
    try {
        parsedProfiles = JSON.parse(cleanedJson);
    } catch (e) {
        console.error('\nInvalid JSON received from AI:', cleanedJson);
        throw new Error(`AI returned invalid JSON: ${e.message}`);
    }

    const outputPath = path.join(repoPath, '.eck', 'profiles.json');
    await fs.writeFile(outputPath, JSON.stringify(parsedProfiles, null, 2));

    const profileKeys = Object.keys(parsedProfiles);
    spinner.succeed(`Successfully detected and saved ${profileKeys.length} profiles to ${outputPath}`);

    console.log('\n‚ú® Detected Profiles:');
    console.log('---------------------------');
    for (const profileName of profileKeys) {
        console.log(`  - ${profileName}`);
    }
    console.log('\nYou can now use these profile names with the --profile flag.');

  } catch (error) {
    spinner.fail(`Failed to detect profiles: ${error.message}`);
  }
}


--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));

    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/generateProfileGuide.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { loadSetupConfig } from '../../config.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';

function buildPrompt(projectPath) {
  const normalizedPath = path.resolve(projectPath);
  return `You are a code architect helping a developer curate manual context profiles for a repository.
Project root: ${normalizedPath}

Use the project directory tree provided separately to identify logical groupings of files that should travel together during focused work.

Instructions:
1. Propose profile names that reflect the responsibilities or layers of the codebase.
2. For each profile, produce an object with "include" and "exclude" arrays of glob patterns (minimize overlap, prefer directory-level globs).
3. Always include a sensible catch-all profile (for example, "default") if one is not obvious.
4. Call out generated assets, tests, or vendor files in "exclude" arrays when appropriate.
5. Return **only** valid JSON. Do not wrap the response in markdown fences or add commentary.
`;
}

function buildGuideContent({ prompt, directoryTree }) {
  const timestamp = new Date().toISOString();
  const trimmedTree = directoryTree.trimEnd();

  return [
    '# Profile Generation Guide',
    '',
    `Generated: ${timestamp}`,
    '',
    '## How to Use',
    '- Copy the prompt below into your AI assistant or follow it yourself.',
    '- When using an AI, paste the directory tree afterward so it has full project context.',
    "- Review the suggested profiles, then save the JSON to `.eck/profiles.json` when you are satisfied.",
    '',
    '## Recommended Prompt',
    '```text',
    prompt.trimEnd(),
    '```',
    '',
    '## Project Directory Tree',
    '```text',
    trimmedTree,
    '```',
    ''
  ].join('\n');
}

export async function generateProfileGuide(repoPath = process.cwd(), options = {}) {
  const spinner = ora('Preparing profile generation guide...').start();
  const projectPath = path.resolve(repoPath);

  try {
    spinner.text = 'Ensuring .eck manifest directory is initialized...';
    await initializeEckManifest(projectPath);

    spinner.text = 'Loading configuration...';
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const combinedConfig = {
      ...userConfig,
      ...(setupConfig.fileFiltering || {}),
      ...(setupConfig.performance || {})
    };

    spinner.text = 'Scanning repository files...';
    const allFiles = await scanDirectoryRecursively(projectPath, combinedConfig, projectPath);

    spinner.text = 'Building directory tree...';
    const maxDepth = Number(combinedConfig.maxDepth ?? 10);
    const directoryTree = await generateDirectoryTree(projectPath, '', allFiles, 0, Number.isFinite(maxDepth) ? maxDepth : 10, combinedConfig);

    if (!directoryTree) {
      throw new Error('Failed to generate directory tree or project is empty.');
    }

    const prompt = buildPrompt(projectPath);
    const guideContent = buildGuideContent({ prompt, directoryTree });
    const guidePath = path.join(projectPath, '.eck', 'profile_generation_guide.md');

    await fs.mkdir(path.dirname(guidePath), { recursive: true });
    spinner.text = 'Writing guide to .eck/profile_generation_guide.md...';
    await fs.writeFile(guidePath, guideContent, 'utf-8');

    spinner.succeed(`Profile generation guide saved to ${guidePath}`);
  } catch (error) {
    spinner.fail(`Failed to generate profile guide: ${error.message}`);
    throw error;
  }
}


--- File: /src/cli/commands/pruneSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { executePrompt as askClaude } from '../../services/claudeCliService.js';
import { parseSnapshotContent, parseSize, formatSize } from '../../utils/fileUtils.js';

function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }
  const firstBracket = text.indexOf('[');
  const lastBracket = text.lastIndexOf(']');
  if (firstBracket !== -1 && lastBracket !== -1 && lastBracket > firstBracket) {
    return text.substring(firstBracket, lastBracket + 1).trim();
  }
  return text.trim();
}

export async function pruneSnapshot(snapshotFile, options) {
  const spinner = ora('Starting snapshot pruning process...').start();
  try {
    const targetSize = parseSize(options.targetSize);
    spinner.text = `Reading snapshot file: ${snapshotFile}`;
    const snapshotContent = await fs.readFile(snapshotFile, 'utf-8');
    const snapshotHeader = snapshotContent.split('--- File: /')[0];
    const files = parseSnapshotContent(snapshotContent);

    if (files.length === 0) {
      spinner.warn('No files found in the snapshot.');
      return;
    }

    const currentSize = Buffer.byteLength(snapshotContent, 'utf-8');
    if (currentSize <= targetSize) {
      spinner.succeed(`Snapshot is already smaller than the target size. (${formatSize(currentSize)} < ${formatSize(targetSize)})`);
      return;
    }

    spinner.text = 'Asking AI to rank files by importance...';
    const filePaths = files.map(f => f.path);

    const prompt = `Return a JSON array ranking these file paths by importance (most important first).

Important files: package.json, index.js, main entry points, core logic, configuration
Less important: tests, documentation, examples

Files to rank:
${filePaths.join('\n')}

Return format (NOTHING else, no markdown, no explanations, ONLY the array):
["file1", "file2", "file3"]`;

    const aiResponseObject = await askClaude(prompt);
    const rawText = aiResponseObject.response || aiResponseObject.response_text || aiResponseObject.result;
    const cleanedJson = extractJson(rawText);

    let rankedFiles;
    try {
      rankedFiles = JSON.parse(cleanedJson);
      if (!Array.isArray(rankedFiles) || rankedFiles.some(item => typeof item !== 'string')) {
        throw new Error('AI response is not an array of strings.');
      }
    } catch (e) {
      spinner.fail(`Failed to parse AI's file ranking: ${e.message}`);
      console.error('Received from AI:', cleanedJson);
      return;
    }

    spinner.text = 'Building pruned snapshot...';
    const fileMap = new Map(files.map(f => [f.path, f.content]));
    let newSnapshotContent = snapshotHeader;
    let newSize = Buffer.byteLength(newSnapshotContent, 'utf-8');
    let filesIncluded = 0;

    for (const filePath of rankedFiles) {
      if (fileMap.has(filePath)) {
        const fileContent = fileMap.get(filePath);
        const fileEntry = `--- File: /${filePath} ---\n\n${fileContent}\n\n`;
        const entrySize = Buffer.byteLength(fileEntry, 'utf-8');

        if (newSize + entrySize > targetSize) {
          break;
        }

        newSnapshotContent += fileEntry;
        newSize += entrySize;
        filesIncluded++;
      }
    }

    const outputFilename = `${path.basename(snapshotFile, path.extname(snapshotFile))}_pruned_${options.targetSize}${path.extname(snapshotFile)}`;
    const outputPath = path.join(path.dirname(snapshotFile), outputFilename);

    await fs.writeFile(outputPath, newSnapshotContent);

    spinner.succeed('Snapshot pruning complete!');
    console.log(`- Original Size: ${formatSize(currentSize)}`);
    console.log(`- New Size: ${formatSize(newSize)}`);
    console.log(`- Files Included: ${filesIncluded} / ${files.length}`);
    console.log(`- Pruned snapshot saved to: ${outputPath}`);

  } catch (error) {
    spinner.fail(`An error occurred during pruning: ${error.message}`);
  }
}


--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/setupGemini.js ---

import which from 'which';
import path from 'path';
import fs from 'fs/promises';
import os from 'os';
import chalk from 'chalk';

/**
 * Sets up claude.toml configuration for gemini-cli integration with dynamic paths
 * @param {Object} options - Command options
 */
export async function setupGemini(options = {}) {
  try {
    console.log(chalk.blue('üîß Setting up gemini-cli integration with dynamic paths...'));

    // Check if gemini-cli is installed
    let geminiCliPath;
    try {
      geminiCliPath = await which('gemini-cli');
      console.log(chalk.green(`‚úÖ Found gemini-cli at: ${geminiCliPath}`));
    } catch (error) {
      console.error(chalk.red('‚ùå gemini-cli not found in PATH'));
      console.log(chalk.yellow('üí° Please install gemini-cli first:'));
      console.log(chalk.cyan('   npm install -g gemini-cli'));
      process.exit(1);
    }

    // Get current working directory for dynamic path resolution
    const currentDir = process.cwd();
    const indexJsPath = path.join(currentDir, 'index.js');

    // Verify index.js exists
    try {
      await fs.access(indexJsPath);
      console.log(chalk.green(`‚úÖ Found eck-snapshot index.js at: ${indexJsPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Could not find index.js at: ${indexJsPath}`));
      console.log(chalk.yellow('üí° Make sure you are running this command from the eck-snapshot project directory'));
      process.exit(1);
    }

    // Create gemini tools directory
    const homeDir = os.homedir();
    const geminiToolsDir = path.join(homeDir, '.gemini', 'tools');

    try {
      await fs.mkdir(geminiToolsDir, { recursive: true });
      console.log(chalk.green(`‚úÖ Created/verified gemini tools directory: ${geminiToolsDir}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to create gemini tools directory: ${error.message}`));
      process.exit(1);
    }

    // Read environment variables from setup.json if available
    let envVars = {};
    try {
      const setupJsonPath = path.join(currentDir, 'setup.json');
      const setupContent = await fs.readFile(setupJsonPath, 'utf-8');
      const setupData = JSON.parse(setupContent);

      // Extract relevant environment variables
      if (setupData.environmentDetection) {
        envVars.ECK_SNAPSHOT_PATH = currentDir;
        console.log(chalk.blue(`üìã Using project context from setup.json`));
      }
    } catch (error) {
      console.log(chalk.yellow('‚ö†Ô∏è  setup.json not found or invalid, using defaults'));
    }

    // Generate claude.toml content with dynamic paths
    const claudeTomlContent = generateClaudeToml(indexJsPath, envVars);

    // Write claude.toml file
    const claudeTomlPath = path.join(geminiToolsDir, 'claude.toml');
    try {
      await fs.writeFile(claudeTomlPath, claudeTomlContent, 'utf-8');
      console.log(chalk.green(`‚úÖ Generated claude.toml at: ${claudeTomlPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to write claude.toml: ${error.message}`));
      process.exit(1);
    }

    // Success summary
    console.log(chalk.green('\nüéâ Setup completed successfully!'));
    console.log(chalk.blue('\nüìã Configuration summary:'));
    console.log(chalk.cyan(`   ‚Ä¢ gemini-cli: ${geminiCliPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ eck-snapshot: ${indexJsPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ claude.toml: ${claudeTomlPath}`));

    if (Object.keys(envVars).length > 0) {
      console.log(chalk.cyan(`   ‚Ä¢ Environment variables: ${Object.keys(envVars).join(', ')}`));
    }

    console.log(chalk.blue('\nüöÄ You can now use:'));
    console.log(chalk.cyan('   gemini-cli claude "Your prompt here"'));
    console.log(chalk.green('\n‚ú® Cross-platform path resolution is automatically handled!'));

  } catch (error) {
    console.error(chalk.red(`‚ùå Setup failed: ${error.message}`));
    if (options.verbose) {
      console.error(chalk.red('Stack trace:'), error.stack);
    }
    process.exit(1);
  }
}

/**
 * Generates claude.toml content with dynamic paths
 * @param {string} indexJsPath - Path to eck-snapshot index.js
 * @param {Object} envVars - Environment variables to include
 * @returns {string} - Generated TOML content
 */
function generateClaudeToml(indexJsPath, envVars = {}) {
  const envSection = Object.keys(envVars).length > 0
    ? `# Environment variables from setup.json
${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}

`
    : '';

  return `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'
# This file uses dynamic paths to work across WSL/Windows environments

${envSection}[claude]
# eck-snapshot integration for AI-powered repository analysis
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

# Command examples:
# gemini-cli claude "Create a snapshot of the current project"
# gemini-cli claude "Analyze the database structure"
# gemini-cli claude "Generate a project overview"

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
generated_at = "${new Date().toISOString()}"
platform = "${process.platform}"
node_version = "${process.version}"
working_directory = "${path.dirname(indexJsPath)}"

# Cross-platform compatibility notes:
# - Paths are automatically resolved using process.cwd()
# - Works in WSL, Windows, macOS, and Linux
# - No hardcoded /mnt/c/ paths required
`;
}

--- File: /src/cli/commands/setupGemini.test.js ---

import { describe, it, expect } from 'vitest';

describe('setupGemini integration', () => {
  it('should validate path resolution logic', () => {
    // Test path join functionality that setupGemini uses
    const currentDir = '/test/project';
    const indexJsPath = `${currentDir}/index.js`;

    expect(indexJsPath).toBe('/test/project/index.js');
    expect(indexJsPath).toContain('index.js');
  });

  it('should validate gemini tools directory structure', () => {
    const homeDir = '/home/user';
    const geminiToolsDir = `${homeDir}/.gemini/tools`;
    const claudeTomlPath = `${geminiToolsDir}/claude.toml`;

    expect(geminiToolsDir).toBe('/home/user/.gemini/tools');
    expect(claudeTomlPath).toBe('/home/user/.gemini/tools/claude.toml');
  });

  it('should validate TOML content structure', () => {
    const indexJsPath = '/test/project/index.js';
    const envVars = { ECK_SNAPSHOT_PATH: '/test/project' };

    // Test environment section generation
    const envSection = Object.keys(envVars).length > 0
      ? `# Environment variables from setup.json\n${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}\n\n`
      : '';

    // Test main TOML structure
    const tomlContent = `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'

${envSection}[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
platform = "${process.platform}"
working_directory = "${indexJsPath.replace('/index.js', '')}"`;

    expect(tomlContent).toContain('[claude]');
    expect(tomlContent).toContain('name = "eck-snapshot"');
    expect(tomlContent).toContain(`args = ["${indexJsPath}", "ask-claude"]`);
    expect(tomlContent).toContain('[claude.metadata]');
    expect(tomlContent).toContain('ECK_SNAPSHOT_PATH = "/test/project"');
  });

  it('should handle cross-platform paths correctly', () => {
    const testPaths = [
      { platform: 'windows', path: 'C:\\Users\\test\\project\\index.js' },
      { platform: 'unix', path: '/home/user/project/index.js' },
      { platform: 'wsl', path: '/mnt/c/Users/test/project/index.js' }
    ];

    testPaths.forEach(({ platform, path }) => {
      expect(path).toContain('index.js');
      expect(path.length).toBeGreaterThan(0);

      // Test that the path is absolute (platform-appropriate)
      if (platform === 'windows') {
        expect(path).toMatch(/^[A-Z]:\\/);
      } else {
        expect(path).toMatch(/^\//);
      }
    });
  });

  it('should validate error handling patterns', () => {
    // Test error message patterns that setupGemini should handle
    const errorPatterns = [
      'gemini-cli not found in PATH',
      'Could not find index.js',
      'Failed to create gemini tools directory',
      'Failed to write claude.toml'
    ];

    errorPatterns.forEach(pattern => {
      expect(pattern).toBeDefined();
      expect(typeof pattern).toBe('string');
      expect(pattern.length).toBeGreaterThan(0);
    });
  });

  it('should test JSON parsing for setup.json', () => {
    const validSetupData = {
      environmentDetection: {
        detected: true
      }
    };

    const jsonString = JSON.stringify(validSetupData);
    const parsed = JSON.parse(jsonString);

    expect(parsed.environmentDetection).toBeDefined();
    expect(parsed.environmentDetection.detected).toBe(true);

    // Test invalid JSON handling pattern
    const invalidJson = 'invalid json {';
    let parseError = null;
    try {
      JSON.parse(invalidJson);
    } catch (e) {
      parseError = e;
    }

    expect(parseError).toBeDefined();
    expect(parseError.message).toContain('JSON');
  });
});

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/services/authService.js ---

import ora from 'ora';
import { execa } from 'execa';

/**
 * Initiates the interactive login flow by spawning 'codex login'.
 * This will open a browser and wait for the user to complete authentication.
 * @returns {Promise<void>}
 */
export async function initiateLogin() {
  const spinner = ora('Authentication required. Please follow the browser instructions.').start();
  try {
    // Run `codex login` interactively, inheriting stdio to show user instructions.
    await execa('codex', ['login'], { stdio: 'inherit' });
    spinner.succeed('Login successful. Retrying original command...');
  } catch (e) {
    spinner.fail('Login process failed or was cancelled.');
    // Re-throw to notify p-retry that the attempt failed.
    throw new Error(`Login failed: ${e.message}`);
  }
}

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';
import pRetry from 'p-retry';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {object} options Options object, e.g., { continueConversation: boolean, taskSize: number }.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, options = {}) {
  const { continueConversation = false } = options;
  try {
    // Ensure the log directory exists
    try {
      await import('fs/promises').then(fs => fs.mkdir('./.eck/logs', { recursive: true }));
    } catch (e) {
      console.error(`Failed to create log directory: ${e.message}`);
      // Do not block execution if log dir creation fails, just warn
    }
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }

    return await attemptClaudeExecution(prompt, sessionId, options);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./.eck/logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId, options);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./.eck/logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @param {object} options Options object for task configuration.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null, options = {}) {
  const timestamp = new Date().toISOString();
  const logFile = `./.eck/logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId, options);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./.eck/logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./.eck/logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./.eck/logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @param {object} options Options object for task configuration.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId, options = {}) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId, options);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @param {object} options Options object with taskSize for calculating dynamic timeout.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null, options = {}) {
  return new Promise((resolve, reject) => {

    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }

    // Always add the skip permissions flag for automation reliability
    args.push('--dangerously-skip-permissions');

    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');

    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });

    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;

    // Dynamic timeout calculation based on task size
    const taskSize = options.taskSize || 0;
    const BASE_TIMEOUT = 60000; // 60 seconds base
    const PER_ITEM_TIMEOUT = 200; // 200ms per file/item
    const ACTIVITY_TIMEOUT = BASE_TIMEOUT + (taskSize * PER_ITEM_TIMEOUT);

    console.log(`‚è±Ô∏è  Using dynamic activity timeout: ${(ACTIVITY_TIMEOUT / 1000).toFixed(1)}s for ${taskSize} items`);

    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log(`üíÄ No activity detected for ${(ACTIVITY_TIMEOUT/1000).toFixed(1)}s, killing process`);
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${(ACTIVITY_TIMEOUT/1000).toFixed(1)} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

/**
 * Executes a prompt using gemini-cli delegation with retry logic for transient errors.
 * @param {string} prompt The prompt to send to Claude via gemini-cli.
 * @returns {Promise<object>} A promise that resolves with the response from Claude.
 */
export async function askClaude(prompt) {
  return pRetry(async () => {
    try {
      const result = await execa('gemini-cli', ['claude', prompt], {
        timeout: 120000 // 2 minute timeout
      });

      // Parse mcp_feedback if present in prompt
      let mcpFeedback = null;
      try {
        const promptObj = JSON.parse(prompt);
        if (promptObj.payload && promptObj.payload.post_execution_steps && promptObj.payload.post_execution_steps.mcp_feedback) {
          mcpFeedback = promptObj.payload.post_execution_steps.mcp_feedback;

          // Log if errors array is non-empty
          if (mcpFeedback.errors && Array.isArray(mcpFeedback.errors) && mcpFeedback.errors.length > 0) {
            console.warn('MCP feedback contains errors:', mcpFeedback.errors);
          }
        }
      } catch (parseError) {
        // If prompt is not valid JSON or doesn't contain mcp_feedback, continue normally
      }

      return {
        stdout: result.stdout,
        stderr: result.stderr,
        success: true,
        mcp_feedback: mcpFeedback
      };
    } catch (error) {
      // Check if this is a transient error that should be retried
      if (isTransientError(error)) {
        console.log(`Transient error detected, retrying: ${error.message}`);
        throw error; // This will trigger a retry
      }

      // Non-transient errors should not be retried
      console.error(`Non-transient error in askClaude: ${error.message}`);
      return {
        stdout: error.stdout || '',
        stderr: error.stderr || error.message,
        success: false,
        error: error.message
      };
    }
  }, {
    retries: 3,
    minTimeout: 1000,
    maxTimeout: 5000,
    onFailedAttempt: (error) => {
      console.log(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`);
    }
  });
}

/**
 * Checks if an error is transient and should be retried.
 * @param {Error} error The error to check.
 * @returns {boolean} True if the error is transient.
 */
export function isTransientError(error) {
  const errorMessage = (error.message || '').toLowerCase();
  const stderr = (error.stderr || '').toLowerCase();
  const stdout = (error.stdout || '').toLowerCase();
  const allOutput = `${errorMessage} ${stderr} ${stdout}`;

  // Network-related errors
  const networkErrors = [
    'network',
    'timeout',
    'connection',
    'econnreset',
    'enotfound',
    'econnrefused',
    'socket hang up'
  ];

  // Quota/rate limit errors
  const quotaErrors = [
    'quota exceeded',
    'rate limit',
    'too many requests',
    'service unavailable',
    'temporarily unavailable',
    '429',
    '500',
    '502',
    '503',
    '504'
  ];

  const transientPatterns = [...networkErrors, ...quotaErrors];

  return transientPatterns.some(pattern => allOutput.includes(pattern));
}

--- File: /src/services/claudeCliService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { askClaude } from './claudeCliService.js';

// Mock execa
vi.mock('execa', () => ({
  execa: vi.fn()
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', () => ({
  default: async (fn, options) => {
    // For tests, we'll execute the function directly without retries
    return await fn();
  }
}));

describe('claudeCliService', () => {
  let mockExeca;

  beforeEach(async () => {
    const { execa } = await import('execa');
    mockExeca = execa;
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.resetAllMocks();
  });

  describe('askClaude', () => {
    it('should successfully execute gemini-cli claude command', async () => {
      const mockResponse = {
        stdout: '{"result": "test response", "success": true}',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith('gemini-cli', ['claude', 'test prompt'], {
        timeout: 120000
      });
      expect(result).toEqual({
        stdout: mockResponse.stdout,
        stderr: mockResponse.stderr,
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle non-transient errors without retry', async () => {
      const mockError = new Error('EACCES: permission denied');
      mockError.code = 'EACCES';
      mockError.stdout = '';
      mockError.stderr = 'permission denied';

      mockExeca.mockRejectedValue(mockError);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: 'permission denied',
        success: false,
        error: 'EACCES: permission denied'
      });
    });

    it('should identify transient network errors', async () => {
      const mockError = new Error('Connection timeout');
      mockError.stdout = '';
      mockError.stderr = 'network timeout occurred';

      const { isTransientError } = await import('./claudeCliService.js');

      expect(isTransientError(mockError)).toBe(true);
    });


    it('should handle JSON parsing in gemini-cli response', async () => {
      const complexJsonResponse = {
        stdout: JSON.stringify({
          type: 'result',
          data: {
            analysis: 'test analysis',
            metrics: { tokens: 100, cost: 0.05 }
          },
          timestamp: new Date().toISOString()
        }),
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(complexJsonResponse);

      const result = await askClaude('analyze this code');

      expect(result.success).toBe(true);
      expect(result.stdout).toContain('test analysis');
      expect(result.stdout).toContain('tokens');
    });

    it('should handle empty responses gracefully', async () => {
      const mockResponse = {
        stdout: '',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: '',
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle stderr warnings without failing', async () => {
      const mockResponse = {
        stdout: '{"result": "success"}',
        stderr: 'Warning: deprecated feature used',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result.success).toBe(true);
      expect(result.stderr).toContain('deprecated feature');
    });

    it('should respect timeout configuration', async () => {
      mockExeca.mockResolvedValue({
        stdout: 'response',
        stderr: '',
        exitCode: 0
      });

      await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith(
        'gemini-cli',
        ['claude', 'test prompt'],
        { timeout: 120000 }
      );
    });

    it('should parse mcp_feedback from JSON prompt', async () => {
      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithFeedback = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: true,
              errors: [],
              mcp_version: '1.0'
            }
          }
        }
      });

      const result = await askClaude(promptWithFeedback);

      expect(result.mcp_feedback).toEqual({
        success: true,
        errors: [],
        mcp_version: '1.0'
      });
    });

    it('should log warning when mcp_feedback contains errors', async () => {
      const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithErrors = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: false,
              errors: ['Error 1', 'Error 2'],
              mcp_version: '1.0'
            }
          }
        }
      });

      await askClaude(promptWithErrors);

      expect(consoleSpy).toHaveBeenCalledWith('MCP feedback contains errors:', ['Error 1', 'Error 2']);

      consoleSpy.mockRestore();
    });
  });

  describe('transient error detection', () => {
    it('should treat network errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const networkErrors = [
        'network error',
        'timeout',
        'connection refused',
        'ECONNRESET',
        'ENOTFOUND',
        'socket hang up'
      ];

      networkErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should treat quota errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const quotaErrors = [
        'quota exceeded',
        'rate limit',
        'too many requests',
        '429',
        '503'
      ];

      quotaErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should not treat permission errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const permanentErrors = [
        'EACCES: permission denied',
        'Invalid API key',
        'Authentication failed'
      ];

      permanentErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(false);
      });
    });
  });
});

--- File: /src/services/dispatcherService.js ---

import { ask as askGpt } from './gptService.js';
import { executePrompt as askClaude } from './claudeCliService.js';

/**
 * Dispatches an analytical task to the most efficient AI model with a fallback.
 * Priority 1: Codex (GPT) with low reasoning for speed and cost.
 * Priority 2: Claude as a reliable fallback.
 * @param {string} prompt The JSON payload or prompt string for the task.
 * @returns {Promise<object>} The result from the successful AI agent.
 */
export async function dispatchAnalysisTask(prompt) {
  try {
    console.log('üß† Dispatcher: Attempting analysis with Codex (low reasoning)...');
    const gptOptions = {
      model: 'gpt-5-codex',
      reasoning: 'low'
    };
    // The 'ask' function expects payload as first arg, and options as second.
    // Since prompt is a string here, we wrap it in an object for consistency if needed,
    // but for simple prompts it can often be passed directly.
    const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
    return await askGpt(payload, { verbose: false, ...gptOptions });
  } catch (gptError) {
    console.warn(`‚ö†Ô∏è Codex (low reasoning) failed: ${gptError.message}`);
    console.log('üîÑ Failing over to Claude for analysis...');
    try {
      return await askClaude(prompt);
    } catch (claudeError) {
      console.error(`‚ùå Critical Failure: Both Codex and Claude failed for analysis task.`);
      throw new Error(`Primary (Codex) Error: ${gptError.message}\nFallback (Claude) Error: ${claudeError.message}`);
    }
  }
}

--- File: /src/services/gptService.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import pRetry from 'p-retry';
import ora from 'ora';
import { loadProjectEckManifest } from '../utils/fileUtils.js';
import { initiateLogin } from './authService.js';
import which from 'which';

const SYSTEM_PROMPT = 'You are a Coder agent. Apply code changes per JSON spec. Respond only in JSON: {success: bool, changes: array, errors: array, post_steps: object}';

class AuthError extends Error {
  constructor(message) {
    super(message);
    this.name = 'AuthError';
  }
}

/**
 * Checks if the codex CLI tool is available in the system's PATH.
 * Throws an error if not found.
 */
async function ensureCodexCliExists() {
  try {
    await which('codex');
  } catch (error) {
    throw new Error('The `codex` CLI tool is not installed or not in your PATH. Please install it from https://github.com/openai/codex to use this command.');
  }
}

/**
 * Delegates an apply_code_changes payload to the codex CLI with auto-login.
 * @param {string|object} payload - JSON string or object payload to forward to the agent.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} [options]
 * @returns {Promise<object>}
 */
export async function ask(payload, options = {}) {
  const { verbose = false, model = 'gpt-5-codex', reasoning = 'high' } = options;
  await ensureCodexCliExists();

  const run = async () => {
    const spinner = verbose ? null : ora('Sending payload to Codex agent...').start();
    try {
      const payloadObject = await parsePayload(payload);
      const manifest = await loadProjectEckManifest(process.cwd());
      const userPrompt = buildUserPrompt(payloadObject, manifest);
      const promptInput = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const args = [
        'exec',
        // Use full-auto mode to prevent interactive prompts from the agent,
        // as this service is designed for non-interactive delegation.
        '--full-auto',
        '--model', model,
        '-c', `model_reasoning_effort=${reasoning}`
      ];

      debug(verbose, `Executing: codex ${args.join(' ')} <stdin>`);

      const cliResult = await execa('codex', args, {
        cwd: process.cwd(),
        timeout: 300000, // 5-minute timeout
        input: promptInput // Stream large prompts via stdin to avoid argv limits
      });

      const output = cliResult?.stdout?.trim();
      if (!output) {
        throw new Error('codex CLI returned empty response');
      }

      const parsed = extractFinalJson(output);
      if (parsed) {
        if (parsed.post_steps || parsed.post_execution_steps) {
          const postSteps = parsed.post_steps || parsed.post_execution_steps;
          await handlePostExecutionSteps(postSteps, payloadObject);
          parsed.mcp_feedback = postSteps?.mcp_feedback || null;
        }
        spinner?.succeed('Codex agent completed the task.');
        return parsed;
      }

      // If parsing fails, surface the raw response text for upstream handling.
      spinner?.succeed('Codex agent completed the task.');
      return { success: true, changes: [], errors: [], response_text: output };

    } catch (error) {
        spinner?.fail('Codex execution failed.');
        handleCliError(error); // This will throw a specific error type
    }
  };

  return pRetry(run, {
    retries: 1, // Only retry once after a successful login
    minTimeout: 0,
    onFailedAttempt: async (error) => {
      if (error.name === 'AuthError') {
        await initiateLogin();
      } else {
        throw error; // Don't retry for other errors, fail immediately
      }
    }
  });
}


async function parsePayload(payload) {
  if (typeof payload === 'string') {
    try {
      return JSON.parse(payload);
    } catch (error) {
      throw new Error(`Failed to parse payload JSON: ${error.message}`);
    }
  }
  if (typeof payload === 'object' && payload !== null) {
    return payload;
  }
  throw new Error('Invalid payload type. Expected JSON string or object.');
}

function buildUserPrompt(payloadObject, manifest) {
  const payloadString = JSON.stringify(payloadObject);
  if (!manifest) {
    return payloadString;
  }

  const sections = [];
  if (manifest.context) {
    sections.push('## .eck Context\n' + manifest.context);
  }
  if (manifest.operations) {
    sections.push('## .eck Operations\n' + manifest.operations);
  }
  if (manifest.journal) {
    sections.push('## .eck Journal\n' + manifest.journal);
  }
  if (manifest.environment && Object.keys(manifest.environment).length > 0) {
    sections.push('## .eck Environment\n' + JSON.stringify(manifest.environment, null, 2));
  }

  if (sections.length === 0) {
    return payloadString;
  }

  return `${payloadString}\n\n# Project Context\n${sections.join('\n\n')}`;
}

function debug(verbose, message) {
  if (verbose) {
    console.log(`[ask-gpt] ${message}`);
  }
}

function extractFinalJson(text) {
  const trimmed = text?.trim();
  if (!trimmed) {
    return null;
  }

  try {
    return JSON.parse(trimmed);
  } catch (error) {
    // Continue with fallback parsing when logs precede the JSON payload.
  }

  const fencedMatch = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fencedContent = fencedMatch[1].trim();
    try {
      return JSON.parse(fencedContent);
    } catch (error) {
      // Ignore and fall through to final brace search.
    }
  }

  const lastBraceIndex = trimmed.lastIndexOf('{');
  if (lastBraceIndex === -1) {
    return null;
  }

  const jsonCandidate = trimmed.slice(lastBraceIndex);
  try {
    return JSON.parse(jsonCandidate);
  } catch (error) {
    return null;
  }
}

function handleCliError(error) {
  const combined = `${error?.message || ''} ${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
  // Check for text that `codex` outputs when auth is missing.
  if (combined.includes('authentication is required') || combined.includes('please run `codex login`')) {
    const authError = new Error('Codex authentication is required. Attempting to log in.');
    authError.name = 'AuthError';
    throw authError;
  }

  throw new Error(`codex CLI failed: ${error.stderr || error.message}`);
}

async function handlePostExecutionSteps(postSteps, payloadObject) {
  if (!postSteps || typeof postSteps !== 'object') {
    return;
  }

  if (postSteps.journal_entry) {
    await applyJournalEntry(postSteps.journal_entry, payloadObject);
  }

  if (postSteps.mcp_feedback) {
    logMcpFeedback(postSteps.mcp_feedback);
  }
}

async function applyJournalEntry(entry, payloadObject) {
  const journalEntry = normalizeJournalEntry(entry);
  const journalPath = path.join(process.cwd(), '.eck', 'JOURNAL.md');

  await fs.mkdir(path.dirname(journalPath), { recursive: true });

  let existing = '';
  try {
    existing = await fs.readFile(journalPath, 'utf-8');
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw new Error(`Failed to read JOURNAL.md: ${error.message}`);
    }
  }

  const taskId = payloadObject?.task_id || payloadObject?.payload?.task_id || journalEntry.task_id || 'ask-gpt';
  const isoDate = new Date().toISOString();

  const frontmatter = [
    '---',
    `task_id: ${taskId}`,
    `date: ${isoDate}`,
    `type: ${journalEntry.type}`,
    `scope: ${journalEntry.scope}`,
    '---',
    ''
  ].join('\n');

  const summary = journalEntry.summary ? `## ${journalEntry.summary}\n` : '';
  const details = journalEntry.details ? `${journalEntry.details}\n` : '';

  const entryBlock = `${frontmatter}${summary ? `${summary}\n` : ''}${details}\n`;

  const existingTrimmed = existing ? existing.replace(/^\n+/, '') : '';
  const newContent = `${entryBlock}${existingTrimmed}`.replace(/\n{3,}/g, '\n\n');

  await fs.writeFile(journalPath, newContent.trimEnd() + '\n');

  await stageJournal(journalPath);
  await commitJournal(journalEntry);
}

function normalizeJournalEntry(entry) {
  return {
    type: entry.type || 'chore',
    scope: entry.scope || 'journal',
    summary: entry.summary || 'Update journal entry',
    details: entry.details || ''
  };
}

async function stageJournal(journalPath) {
  const relativePath = path.relative(process.cwd(), journalPath);
  try {
    await execa('git', ['add', relativePath], { cwd: process.cwd() });
  } catch (error) {
    throw new Error(`Failed to stage journal entry: ${error.message}`);
  }
}

async function commitJournal(entry) {
  const scopePart = entry.scope ? `(${entry.scope})` : '';
  const summary = (entry.summary || 'Update journal entry').replace(/\s+/g, ' ').trim();
  const commitMessage = `${entry.type}${scopePart}: ${summary}`;

  try {
    await execa('git', ['commit', '-m', commitMessage], { cwd: process.cwd() });
  } catch (error) {
    const text = `${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
    if (text.includes('nothing to commit')) {
      console.warn('Journal entry already committed or no changes to commit.');
      return;
    }
    throw new Error(`Failed to commit journal entry: ${error.message}`);
  }
}

function logMcpFeedback(feedback) {
  if (!feedback) {
    return;
  }

  const errors = Array.isArray(feedback.errors) ? feedback.errors : [];
  if (!feedback.success || errors.length > 0) {
    console.warn('MCP feedback indicates issues:', errors.length > 0 ? errors : feedback);
  } else {
    console.log('MCP feedback:', feedback);
  }
}


--- File: /src/services/gptService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

// Mock execa and which
vi.mock('execa', () => ({ execa: vi.fn() }));
vi.mock('which', () => ({ default: vi.fn() }));

// Mock fs/promises for journal entries
const mkdirMock = vi.fn();
const readFileMock = vi.fn();
const writeFileMock = vi.fn();
const loadProjectEckManifestMock = vi.fn();
vi.mock('fs/promises', () => ({
  mkdir: mkdirMock,
  readFile: readFileMock,
  writeFile: writeFileMock
}));
vi.mock('../utils/fileUtils.js', () => ({
  loadProjectEckManifest: loadProjectEckManifestMock
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', async (importOriginal) => {
  const actual = await importOriginal();
  return {
    ...actual,
    default: vi.fn(async (fn, options) => {
      try {
        return await fn();
      } catch (error) {
        if (options.onFailedAttempt) {
          await options.onFailedAttempt(error);
          // In a real scenario, p-retry would re-run fn. For testing, we simulate one retry.
          if (error.name === 'AuthError') {
             return await fn();
          }
        }
        throw error;
      }
    })
  };
});

// Mock the authService
vi.mock('./authService.js', () => ({
  initiateLogin: vi.fn()
}));

describe('gptService with codex CLI', () => {
  let ask;
  let execaMock;
  let whichMock;
  let initiateLoginMock;

  beforeEach(async () => {
    vi.clearAllMocks();

    ({ execa: execaMock } = await import('execa'));
    const which = (await import('which')).default;
    whichMock = which;
    ({ initiateLogin: initiateLoginMock } = await import('./authService.js'));
    ({ ask } = await import('./gptService.js'));

    whichMock.mockResolvedValue('/usr/bin/codex');
    loadProjectEckManifestMock.mockResolvedValue(null);
  });

  it('should call codex CLI with correct arguments and parse final JSON from noisy output', async () => {
    const codexLogs = '[2025-10-06 20:04:22] OpenAI Codex v0.42.0\nSome setup log...\n\n{"success": true, "changes": ["change1"], "errors": []}';
    execaMock.mockResolvedValue({ stdout: codexLogs });

    const payload = { objective: 'Test' };
    const result = await ask(payload);

    expect(result).toEqual({ success: true, changes: ['change1'], errors: [] });
    expect(execaMock).toHaveBeenCalledWith('codex', expect.arrayContaining(['exec', '--full-auto', '--model']), expect.any(Object));
    const [, , options] = execaMock.mock.calls[0];
    expect(options.input).toContain(JSON.stringify(payload));
  });

  it('should trigger login flow on authentication error and retry', async () => {
    const authError = new Error('Authentication is required. Please run `codex login`.');
    authError.name = 'AuthError'; // Custom error name to trigger retry
    authError.stderr = 'Authentication is required. Please run `codex login`.';

    const successResponse = {
      id: 'task2',
      msg: {
        type: 'task_complete',
        last_agent_message: '{"success": true}'
      }
    };

    // First call fails, second call (retry) succeeds
    execaMock
      .mockRejectedValueOnce(authError)
      .mockResolvedValueOnce({ stdout: JSON.stringify(successResponse) });

    initiateLoginMock.mockResolvedValue();

    const result = await ask({ objective: 'Retry test' });

    expect(result).toEqual({ success: true });
    expect(initiateLoginMock).toHaveBeenCalledTimes(1);
    expect(execaMock).toHaveBeenCalledTimes(2); // Initial call + retry
  });

  it('should throw if codex CLI is not found', async () => {
    whichMock.mockRejectedValue(new Error('not found'));
    await expect(ask({})).rejects.toThrow('The `codex` CLI tool is not installed');
  });

  it('should throw non-auth errors immediately without retry', async () => {
    const otherError = new Error('Some other CLI error');
    otherError.stderr = 'Something else went wrong';
    execaMock.mockRejectedValueOnce(otherError);

    await expect(ask({})).rejects.toThrow('codex CLI failed: Something else went wrong');
    expect(initiateLoginMock).not.toHaveBeenCalled();
  });
});


--- File: /src/templates/agent-prompt.template.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

--- File: /src/templates/architect-prompt.template.md ---

# AI Architect Instructions

You are an autonomous AI Architect. Your primary goal is to develop and evolve a software project by planning high-level architecture and delegating implementation tasks to an execution agent named Claude.

## Core Workflow: The Thought-Tool-Observation Loop

Your entire operational process follows a strict loop:
1.  **Thought:** Analyze the user's request, the current state of the project, and previous observations. Formulate a plan and decide on the next immediate action. You must explain your reasoning and your chosen action in plain text.
2.  **Tool:** Immediately after your thought process, you MUST issue a command to either the local `eck-snapshot` environment or the `claude_code_agent`.
3.  **Observation:** After issuing a command, you MUST STOP and wait for an `Observation:` message from the system, which will contain the result of your command. Do not proceed until you receive it.

## Commanding the Execution Agent (Claude)

To delegate any coding task (writing, editing, testing, refactoring), you MUST generate a JSON command block for the `claude_code_agent`. This is your primary method of modifying the codebase.

**JSON Command Format:**
```json
{
  "target_agent": "claude_code_agent",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "A brief, clear task description for Claude.",
    "context": "Explain why this change is needed and any relevant architectural context.",
    "files_to_modify": [
      {
        "path": "exact/path/to/file.js",
        "action": "add | modify | replace | delete",
        "location": "line numbers, function name, or a unique search pattern",
        "details": "Precise, step-by-step instructions for Claude to implement."
      }
    ]
  }
}
```

## Interacting with the Local Environment

To understand the project state, you can command the `eck-snapshot` tool directly. Use this for discovery, analysis, and managing project context.

**Tool Command Format:** `[tool_code: eck-snapshot <command> <options>]`

**Available Commands:**
- `eck-snapshot snapshot`: To create a new snapshot of the current state.
- `eck-snapshot query "<question>"`: To search the codebase.
- `eck-snapshot detect`: To analyze the project structure.
- `eck-snapshot restore <snapshot_file> --include ...`: To view specific files from a snapshot.

## Final Mandate

Your existence is defined by this loop. Think, act by issuing a tool command, and then wait for the observation. This is the only way you can make progress.

--- File: /src/templates/envScanRequest.md ---

Run this command and return the output:
```
node -e "console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))"
```

--- File: /src/templates/gitWorkflow.md ---

## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

--- File: /src/templates/multiAgent.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** {{stats.includedFiles}}
- **Total Files in Repo:** {{stats.totalFiles}}

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

{{projectOverview}}

{{eckManifestSection}}

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

{{hierarchicalWorkflow}}

{{commandFormats}}

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

{{agentDefinitions}}



--- File: /src/templates/vectorMode.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: "{{userQuery}}"

* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.
* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** {{repoName}}
- **User Query:** "{{userQuery}}"

{{multiAgentSection}}

---


--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig, getAllProfiles } from '../config.js';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

function extractMeaningfulLine(block) {
  if (!block || typeof block !== 'string') {
    return null;
  }

  const lines = block.split(/\r?\n/);
  for (const line of lines) {
    const trimmed = line.trim();
    if (!trimmed || trimmed.startsWith('#')) {
      continue;
    }
    const withoutBullet = trimmed.replace(/^[-*]\s*/, '').trim();
    if (withoutBullet) {
      return withoutBullet.replace(/\s+/g, ' ');
    }
  }
  return null;
}

function extractDescriptionFromManifest(eckManifest) {
  if (!eckManifest) {
    return null;
  }

  if (typeof eckManifest.description === 'string' && eckManifest.description.trim()) {
    return eckManifest.description.trim();
  }

  if (eckManifest.project && typeof eckManifest.project.description === 'string' && eckManifest.project.description.trim()) {
    return eckManifest.project.description.trim();
  }

  if (typeof eckManifest.context === 'string' && eckManifest.context.trim()) {
    const sectionMatch = eckManifest.context.match(/##\s*Description\s*([\s\S]*?)(?=^##\s|^#\s|\Z)/im);
    if (sectionMatch && sectionMatch[1]) {
      const meaningful = extractMeaningfulLine(sectionMatch[1]);
      if (meaningful) {
        return meaningful;
      }
    }

    const fallback = extractMeaningfulLine(eckManifest.context);
    if (fallback) {
      return fallback;
    }
  }

  return null;
}

async function resolveProjectDescription(context) {
  const defaultDescription = 'Project description not provided.';

  const manifestDescription = extractDescriptionFromManifest(context.eckManifest);
  if (manifestDescription) {
    const normalized = manifestDescription.trim();
    const genericPatterns = [
      /^brief description of what this project does/i,
      /^no project context provided/i
    ];
    const isGeneric = genericPatterns.some(pattern => pattern.test(normalized));
    if (!isGeneric) {
      return normalized;
    }
  }

  if (context.repoPath) {
    try {
      const packageJsonPath = path.join(context.repoPath, 'package.json');
      const pkgRaw = await fs.readFile(packageJsonPath, 'utf-8');
      const pkg = JSON.parse(pkgRaw);
      if (typeof pkg.description === 'string' && pkg.description.trim()) {
        return pkg.description.trim();
      }
    } catch (error) {
      // Ignore errors - package.json may not exist or be readable
    }
  }

  return defaultDescription;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Helper function to read a template file or return the string if it's not a path
    const loadTemplate = async (templatePathOrString) => {
      if (templatePathOrString && (templatePathOrString.endsWith('.md') || templatePathOrString.endsWith('.txt'))) {
        try {
          // Resolve path relative to the project root. __dirname is src/utils.
          const resolvedPath = path.join(__dirname, '..', '..', templatePathOrString);
          return await fs.readFile(resolvedPath, 'utf-8');
        } catch (e) {
          return `ERROR: FAILED TO LOAD TEMPLATE ${templatePathOrString}: ${e.message}`;
        }
      }
      return templatePathOrString; // Fallback for old-style inline strings or errors
    };

    // P1 Bug Fix: Normalize manifest structure as per Consilium report
    function normalizeManifest(raw) {
      if (!raw) return null;
      const out = {};
      // Handle `setup.json` structure (e.g., `projectContext.name`)
      if (raw.projectContext) {
        out.context = raw.projectContext.description || JSON.stringify(raw.projectContext, null, 2);
        out.operations = raw.operations || raw.projectContext.operations || ''; // Assuming .eck/OPERATIONS.md is separate
        out.journal = raw.journal || raw.projectContext.journal || ''; // Assuming .eck/JOURNAL.md is separate
        out.environment = raw.environment || raw.projectContext.environment || {}; // Assuming .eck/ENVIRONMENT.md is separate
      } else {
        // Handle direct .eck file structure (e.g., raw.context from CONTEXT.md)
        out.context = raw.context || '';
        out.operations = raw.operations || '';
        out.journal = raw.journal || '';
        out.environment = raw.environment || {};
      }
      // Add fallback text if still empty
      if (!out.context) out.context = 'No project context provided.';
      if (!out.operations) out.operations = 'No operations guide provided.';
      if (!out.journal) out.journal = 'No journal entries found.';

      return out;
    }

    // --- Build common context sections --- 
    const projectDescription = await resolveProjectDescription(context);
    const projectOverview = `### PROJECT OVERVIEW
- **Project:** ${context.repoName || 'Unknown'}
- **Description:** ${projectDescription}
`;
    const normalizedEck = normalizeManifest(context.eckManifest);
    let eckManifestSection = '';
    if (normalizedEck) {
      eckManifestSection = buildEckManifestSection(normalizedEck);
    } else {
      eckManifestSection = '### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nWARNING: .eck manifest was not found or was empty.\n';
    }
    // --- End context building ---


    // Check if agent mode is enabled
    if (context.options && context.options.agent) {
      const agentPromptTemplate = await loadTemplate(promptTemplates.agent);

      const agentHeader = `${agentPromptTemplate}

${projectOverview}
${eckManifestSection}
---

## Project Snapshot Information

- **Project**: ${context.repoName || 'Unknown'}
- **Timestamp**: ${new Date().toISOString()}
- **Files Included**: ${context.stats ? context.stats.includedFiles : 'Unknown'}
- **Total Files in Repo**: ${context.stats ? context.stats.totalFiles : 'Unknown'}

---

`;
      return agentHeader;
    }

    // --- Determine Workflow Content based on JA Flag ---
    const withJa = context.options && context.options.withJa;
    let hierarchicalWorkflow = '';
    let commandFormats = '';

    if (withJa) {
        hierarchicalWorkflow = `### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (\`gemini_wsl\`), who has a detailed (\`_ja.md\`) snapshot and the ability to command a **Coder** agent (\`claude\`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (\`gemini_wsl\`):** Receives strategic tasks, analyzes the \`_ja.md\` snapshot, breaks the task down, and commands the Coder.
  - **Coder (\`claude\`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while \`gemini_wsl\` can use its own tools for analysis, validation, and running shell commands.`;

        commandFormats = `### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (\`local_dev\`, \`production_server\`, \`android_wsl_dev\`, \`gemini_windows\`) - LOW-LEVEL EXECUTION:**
Use \`apply_code_changes\` for simple, direct tasks where you provide all details.

\`\`\`json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
\`\`\`

**2. For Junior Architects (\`gemini_wsl\`) - HIGH-LEVEL DELEGATION:**
Use \`execute_strategic_task\` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

\`\`\`json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to \`routes/api.js\`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
\`\`\``;
    } else {
        hierarchicalWorkflow = `### AGENT WORKFLOW

Your role is **Architect**. You formulate technical plans and delegate code implementation tasks directly to the **Coder** agents (e.g., \`local_dev\`).

  - **Architect (You):** Sets strategy, defines tasks.
  - **Coder (e.g., \`local_dev\`):** Receives precise coding tasks and executes them.`;

        commandFormats = `### COMMAND FORMATS

You MUST use the following JSON command format for Coders:

**For Coders (\`local_dev\`, \`production_server\`, \`android_wsl_dev\`, \`gemini_windows\`):**
Use \`apply_code_changes\` for direct tasks where you provide all details.

\`\`\`json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
\`\`\``;
    }

    // --- This is the main/Senior Architect prompt logic ---
    let template;
    if (context.mode === 'vector') {
      template = await loadTemplate(promptTemplates.vectorMode);
      // Inject context for vector mode
      template = template.replace('{{multiAgentSection}}', `
${projectOverview}
${eckManifestSection}
`);
    } else {
      template = await loadTemplate(promptTemplates.multiAgent);
      // --- INJECT DYNAMIC CONTEXT ---
      template = template.replace('{{projectOverview}}', projectOverview);
      template = template.replace('{{eckManifestSection}}', eckManifestSection);
      // --- END INJECT ---
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions,
      hierarchicalWorkflow,
      commandFormats
    };

    let renderedTemplate = render(template, data);
    
    // Inject dynamic profile context if a profile is active
    if (context.options && context.options.profile && context.repoPath) {
      let metadataHeader = '\n\n## Partial Snapshot Context\n';
      metadataHeader += `- **Profile(s) Active:** ${context.options.profile}\n`;
      try {
          const allProfiles = await getAllProfiles(context.repoPath);
          const activeProfileNames = context.options.profile.split(',').map(p => p.trim().replace(/^-/, ''));
          const allProfileNames = Object.keys(allProfiles).filter(p => !activeProfileNames.includes(p));
          if (allProfileNames.length > 0) {
               metadataHeader += `- **Other Available Profiles:** ${allProfileNames.join(', ')}\n`;
          }
      } catch (e) { /* fail silently on metadata generation */ }

      const insertMarker = "### "; // Generic marker since we change the H1s
      // Insert before first H3 (WORKFLOW usually)
      renderedTemplate = renderedTemplate.replace(/### /, metadataHeader + '\n### ');
    }

    return renderedTemplate;

  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header', error.message);
    return `# Snapshot for ${context.repoName || 'Project'}

Generated: ${new Date().toISOString()}

---

`;
  }
}


--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { executePrompt as askClaude } from '../services/claudeCliService.js';
import { getProfile, loadSetupConfig } from '../config.js';
import micromatch from 'micromatch';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = '.eck/';
  const comment = '# Added by eck-snapshot to ignore metadata directory';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

// Helper function to determine if a string is a glob pattern
function isGlob(str) {
  return str.includes('*') || str.includes('?') || str.includes('{');
}

/**
 * Applies advanced profile filtering (multi-profile, exclusion, and ad-hoc globs) to a file list.
 */
export async function applyProfileFilter(allFiles, profileString, repoPath) {
  const profileParts = profileString.split(',').map(p => p.trim()).filter(Boolean);
  
  const includeGlobs = [];
  const excludeGlobs = [];
  const includeNames = [];
  const excludeNames = [];

  // Step 1: Differentiate between profile names and ad-hoc glob patterns
  for (const part of profileParts) {
    const isNegative = part.startsWith('-');
    const pattern = isNegative ? part.substring(1) : part;

    if (isGlob(pattern)) {
      if (isNegative) {
        excludeGlobs.push(pattern);
      } else {
        includeGlobs.push(pattern);
      }
    } else {
      if (isNegative) {
        excludeNames.push(pattern);
      } else {
        includeNames.push(pattern);
      }
    }
  }

  let workingFiles = [];
  let finalIncludes = [...includeGlobs];
  let finalExcludes = [...excludeGlobs];

  // Step 2: Load patterns from specified profile names
  const allProfileNames = [...new Set([...includeNames, ...excludeNames])];
  const profiles = new Map();
  for (const name of allProfileNames) {
    const profile = await getProfile(name, repoPath);
    if (profile) {
      profiles.set(name, profile);
    } else {
      // This is an ad-hoc glob, not a profile, so no warning is needed.
      if (!isGlob(name)) {
        console.warn(`‚ö†Ô∏è Warning: Profile '${name}' not found and will be skipped.`);
      }
    }
  }

  for (const name of includeNames) {
    if (profiles.has(name)) {
      finalIncludes.push(...(profiles.get(name).include || []));
      finalExcludes.push(...(profiles.get(name).exclude || []));
    }
  }
  for (const name of excludeNames) {
    if (profiles.has(name)) {
      finalExcludes.push(...(profiles.get(name).include || []));
    }
  }
  
  // Step 3: Apply the filtering logic
  if (finalIncludes.length > 0) {
    workingFiles = micromatch(allFiles, finalIncludes);
  } else if (includeNames.length > 0 && includeGlobs.length === 0) {
    workingFiles = [];
  } else {
    workingFiles = allFiles;
  }

  if (finalExcludes.length > 0) {
    workingFiles = micromatch.not(workingFiles, finalExcludes);
  }

  return workingFiles;
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');

  // Load setup configuration to check AI generation settings
  let aiGenerationEnabled = false;
  try {
    const setupConfig = await loadSetupConfig();
    aiGenerationEnabled = setupConfig?.aiInstructions?.manifestInitialization?.aiGenerationEnabled ?? false;
  } catch (error) {
    // If setup config fails to load, default to disabled
    console.warn(`   ‚ö†Ô∏è Could not load setup config: ${error.message}. AI generation disabled.`);
  }

  try {
    // Check if .eck directory already exists and has all required files
    let needsInitialization = false;
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory exists, check if all required files are present
        const requiredFiles = ['ENVIRONMENT.md', 'CONTEXT.md', 'OPERATIONS.md', 'JOURNAL.md'];
        for (const fileName of requiredFiles) {
          try {
            await fs.stat(path.join(eckDir, fileName));
          } catch (error) {
            console.log(`   ‚ÑπÔ∏è Missing ${fileName}, initialization needed`);
            needsInitialization = true;
            break;
          }
        }
        if (!needsInitialization) {
          // All files exist, no need to initialize
          return;
        }
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
      needsInitialization = true;
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');

    // --- NEW HYBRID LOGIC --- 
    // 1. Run static analysis first to gather facts.
    let staticFacts = {};
    try {
      staticFacts = await detectProjectType(projectPath);
      console.log(`   üîç Static analysis complete. Detected type: ${staticFacts.type}`);
    } catch (e) {
      console.warn(`   ‚ö†Ô∏è Static project detection failed: ${e.message}. Proceeding with generic prompts.`);
    }

    // Prevent AI hallucination by removing low-confidence "other possibilities"
    if (staticFacts && staticFacts.allDetections) {
      delete staticFacts.allDetections;
    }
    
    const staticFactsJson = JSON.stringify(staticFacts, null, 2);
    // --- END NEW LOGIC ---
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'ENVIRONMENT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw YAML key-value content for an .eck/ENVIRONMENT.md file. Only include detected facts. DO NOT add any keys that are not present in the facts. DO NOT add conversational text or markdown wrappers. Your response MUST start directly with a YAML key (e.g., 'project_type: ...').`,
        content: `# This file is for environment overrides. Add agent-specific settings here.\nagent_id: local_dev\n` // Simple static fallback
      },
      {
        name: 'CONTEXT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/CONTEXT.md file. Use the facts to write ## Description, ## Architecture, and ## Key Technologies. DO NOT add conversational text (like "Here is the file..."). Your response MUST start *directly* with the '# Project Overview' heading.`,
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: `Given these static project analysis facts (especially package.json scripts):
${staticFactsJson}

Generate the raw Markdown content ONLY for a .eck/OPERATIONS.md file. DO NOT add conversational text. Your response MUST start *directly* with the '# Common Operations' heading. List commands for ## Development Setup, ## Running the Project, and ## Testing.`,
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/ROADMAP.md file. DO NOT add conversational text. Start *directly* with '# Project Roadmap'. Propose 1-2 *plausible* placeholder items for ## Current Sprint/Phase and ## Next Phase based on the project type.`,
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        prompt: `Generate the raw Markdown content ONLY for a .eck/TECH_DEBT.md file. DO NOT add conversational text. Start *directly* with '# Technical Debt'. Propose 1-2 *common* placeholder items for ## Code Quality Issues and ## Refactoring Opportunities.`,
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file (only if it doesn't exist)
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      
      // Skip if file already exists
      try {
        await fs.stat(filePath);
        console.log(`   ‚úÖ ${file.name} already exists, skipping`);
        continue;
      } catch (error) {
        // File doesn't exist, create it
      }
      
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For files with a prompt, try to dynamically generate (only if enabled)
      if (file.prompt && aiGenerationEnabled) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponseObject = await askClaude(file.prompt); // Use the prompt
          const rawText = aiResponseObject.result; // Handle Claude response

          if (!rawText || typeof rawText.replace !== 'function') {
             throw new Error(`AI returned invalid content type: ${typeof rawText}`);
          }

          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = rawText.replace(/^```(markdown|yaml)?\n|```$/g, '').trim();

          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}


--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));

  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }

  const best = detections[0];

  // Special handling for mixed monorepos
  const isLikelyMonorepo = detections.length > 1 && detections.some(d => d.score >= 40);

  if (isLikelyMonorepo) {
    // If we have multiple strong detections, prefer the highest priority with substantial evidence
    const strongDetections = detections.filter(d => d.score >= 40);
    if (strongDetections.length > 1) {
      const primaryType = strongDetections[0].type;
      return {
        type: primaryType,
        confidence: Math.min(strongDetections[0].score / 100, 1.0),
        details: {
          ...strongDetections[0].details,
          isMonorepo: true,
          additionalTypes: strongDetections.slice(1).map(d => d.type)
        },
        allDetections: detections
      };
    }
  }

  // Boost confidence for strong workspace indicators
  if (best.details && (best.details.isWorkspace || best.details.workspaceSize)) {
    const boostedScore = best.score + 20; // Bonus for workspace structure
    return {
      type: best.type,
      confidence: Math.min(boostedScore / 100, 1.0),
      details: best.details,
      allDetections: detections
    };
  }

  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;

  // Check for required files (check both root and common subdirectories)
  if (pattern.files) {
    for (const file of pattern.files) {
      // Check in root directory first
      const rootExists = await fileExists(path.join(projectPath, file));
      if (rootExists) {
        score += 25; // Each required file adds points
      } else {
        // For Cargo.toml and other project files, also check common subdirectory patterns
        const commonSubdirs = ['src', 'lib', 'app', 'core', 'backend', 'frontend'];
        // Add project-type specific subdirectories
        if (file === 'Cargo.toml') {
          commonSubdirs.push('codex-rs', 'rust', 'server', 'api');
        }
        if (file === 'package.json') {
          commonSubdirs.push('codex-cli', 'cli', 'client', 'web', 'ui');
        }

        for (const subdir of commonSubdirs) {
          const subdirExists = await fileExists(path.join(projectPath, subdir, file));
          if (subdirExists) {
            score += 20; // Slightly lower score for subdirectory finds
            break; // Only count once per file type
          }
        }
      }
    }
  }

  // Check for required directories (check both root and one level deep)
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const rootExists = await directoryExists(path.join(projectPath, dir));
      if (rootExists) {
        score += 20; // Each required directory adds points
      } else {
        // Check in common project subdirectories
        const projectSubdirs = ['codex-rs', 'codex-cli', 'src', 'lib', 'app'];
        for (const projDir of projectSubdirs) {
          const subdirExists = await directoryExists(path.join(projectPath, projDir, dir));
          if (subdirExists) {
            score += 15; // Lower score for nested directory finds
            break;
          }
        }
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names (more precise matching)
        const foundInDeps = Object.keys(allDeps).some(dep => dep === patternText || dep.startsWith(patternText + '/'));
        // Only check for exact matches in keywords array, not description (too broad)
        const foundInKeywords = packageJson.keywords && Array.isArray(packageJson.keywords)
          ? packageJson.keywords.some(keyword => keyword.toLowerCase() === patternText.toLowerCase())
          : false;
        
        if (foundInDeps || foundInKeywords) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };

  try {
    // Check both root and common subdirectories for Cargo.toml
    let cargoPath = path.join(projectPath, 'Cargo.toml');
    let cargoContent = null;

    if (await fileExists(cargoPath)) {
      cargoContent = await fs.readFile(cargoPath, 'utf-8');
    } else {
      // Check common Rust project subdirectories
      const rustSubdirs = ['codex-rs', 'rust', 'src', 'core', 'server'];
      for (const subdir of rustSubdirs) {
        const subdirCargoPath = path.join(projectPath, subdir, 'Cargo.toml');
        if (await fileExists(subdirCargoPath)) {
          cargoPath = subdirCargoPath;
          cargoContent = await fs.readFile(subdirCargoPath, 'utf-8');
          details.primaryLocation = subdir;
          break;
        }
      }
    }

    if (!cargoContent) {
      return details;
    }

    const nameMatch = cargoContent.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = cargoContent.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = cargoContent.match(/edition\s*=\s*"([^"]+)"/);

    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];

    // Check if it's a workspace
    if (cargoContent.includes('[workspace]')) {
      details.isWorkspace = true;

      // Count workspace members
      const workspaceMatch = cargoContent.match(/members\s*=\s*\[([\s\S]*?)\]/);
      if (workspaceMatch) {
        const members = workspaceMatch[1].split(',').map(m => m.trim().replace(/"/g, '')).filter(m => m);
        details.workspaceMembers = members.length;
      }
    }

    // Check for multiple Cargo.toml files (indicates workspace structure)
    if (details.primaryLocation) {
      const subdirPath = path.join(projectPath, details.primaryLocation);
      try {
        const subdirs = await fs.readdir(subdirPath, { withFileTypes: true });
        let cargoCount = 0;
        for (const entry of subdirs) {
          if (entry.isDirectory()) {
            const memberCargoPath = path.join(subdirPath, entry.name, 'Cargo.toml');
            if (await fileExists(memberCargoPath)) {
              cargoCount++;
            }
          }
        }
        if (cargoCount > 3) { // If many workspace members, this is definitely a Rust project
          details.workspaceSize = 'large';
        }
      } catch (error) {
        // Ignore
      }
    }

  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }

  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'c': [0, 0.23, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    const parsedData = JSON.parse(data);
    // Ensure the structure is complete by merging with defaults
    return {
        coefficients: { ...DEFAULT_COEFFICIENTS, ...parsedData.coefficients },
        trainingPoints: parsedData.trainingPoints || {}
    };
  } catch (error) {
    // If file doesn't exist or is malformed, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];

  if (!points || points.length === 0) {
    // No points, nothing to do.
    return;
  }

  if (points.length === 1) {
    // With one point, use a direct ratio for the linear coefficient.
    const point = points[0];
    if (point.fileSizeInBytes > 0) { // Avoid division by zero
        const ratio = point.actualTokens / point.fileSizeInBytes;
        data.coefficients[projectType] = [
            0, // intercept
            Math.max(0, ratio), // linear term (slope)
            0, 0 // quadratic, cubic
        ];
    }
    return;
  }

  // Use linear regression for 2 or more points.
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;

  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;

    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }

  const denominator = (n * sumX2 - sumX * sumX);
  if (denominator === 0) return; // Avoid division by zero, can't calculate slope

  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / denominator;
  const intercept = (sumY - slope * sumX) / n;

  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

--- File: /test_prune.md/eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-10-11T12:17:51.271Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** 52
- **Total Files in Repo:** 65

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.



## Project-Specific Manifest (.eck Directory)

This project includes a `.eck` directory with specific context and configuration:

### Project Context

# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.

### Operations Guide

# Common Operations

## Development Setup
```bash
# Setup commands
npm install
# or yarn install
```

## Running the Project
```bash
# Development mode
npm run dev

# Production build
npm run build
```

## Testing
```bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
```

## Deployment
```bash
# Deployment commands
npm run deploy
```

## Troubleshooting
Common issues and their solutions.

### Development Journal

---
task_id: fix-profile-detect-and-add-index-viewer-v1
date: 2025-10-10
type: fix
scope: cli
---

# Fix JSON parsing in profile-detect

- Modified extractJson to be more robust against AI log wrappers
- Finds first '{' and last '}' to extract JSON from surrounding text
- Added /managed_components/ to .gitignore
- Implemented new index-view command to inspect code chunks database
- This resolves the crash when running the profile-detect command

---
task_id: feat-c-language-support-v1
date: 2025-10-10
type: feat
scope: core
---

# Add support for C language projects

- Added C project detection (Makefile, CMakeLists.txt, *.c, *.h)
- Added C-specific file filtering for compiled objects (.o, .a, .so)
- Installed tree-sitter-c@0.21.4 parser
- Integrated C parser into segmenter for functions, structs, enums, unions
- Added token estimation coefficients for C (0.23 ratio)

---
task_id: refactor-claude-skip-permissions-default-v1
date: 2025-09-28T14:30:00.000Z
type: refactor
scope: claude-cli
---
## Force --dangerously-skip-permissions for all Claude calls

- Removed the user-facing '--dangerously-skip-permissions' option from 'ask-claude' and 'ask-claude-session' commands.
- Hardcoded the flag in the service layer to ensure all calls to 'claude-code' are non-interactive by default.
- This change improves the reliability of the tool in automated workflows by preventing it from hanging on permission prompts.

---
task_id: refactor-gpt-service-to-codex-cli-v1
date: 2025-09-28T14:00:00.000Z
type: refactor
scope: services
---
## Switch ask-gpt from chatgpt-cli to official codex CLI

- Replaced `npx chatgpt` calls with the official `codex` CLI, using the `exec --full-auto` command for machine-readable output.
- Implemented an automatic login flow that detects authentication errors and triggers the interactive `codex login` command.
- Created a new `authService.js` to handle the login initiation.
- Removed the `open` package dependency as it is no longer needed.
- Updated tests in `gptService.test.js` to mock the new `codex` command flow.
- Added comprehensive documentation in README.md for both ChatGPT and Claude Code integration.
- Enhanced CLI help with detailed examples and authentication instructions.

---
task_id: gpt-test-1
date: 2025-09-28T09:24:01.314Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:41.532Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:33.203Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:43.135Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:27.678Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:04.054Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: configure-claude-auto-accept-v1
date: 2025-09-14T23:34:21Z
type: feat
scope: workflow
---

## Enable and document claude-code auto-accept mode

Created a global `settings.json` for claude-code to enable `acceptEdits` by default, allowing for fully autonomous operation. Added a `CLAUDE_SETUP.md` file to document this essential configuration step for new developers or fresh installations.

---
task_id: create-eck-commit-command-v1
date: 2025-09-14T23:29:38Z
type: feat
scope: workflow
---

## Create custom /eck:commit claude-code command

Added a custom slash command to automate the new structured journaling and conventional commit process. This command takes structured input (type, scope, summary, details) and uses it to update JOURNAL.md and create a git commit, enforcing our new workflow.

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure

### Environment Overrides

The following environment settings override auto-detected values:

- **project_type**: nodejs
- **project_name**: @xelth/eck-snapshot
- **project_version**: 3.0.0
- **has_typescript**: false
- **framework**: node.js
- **is_monorepo**: false

**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.

---



### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (`gemini_wsl`), who has a detailed (`_ja.md`) snapshot and the ability to command a **Coder** agent (`claude`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (`gemini_wsl`):** Receives strategic tasks, analyzes the `_ja.md` snapshot, breaks the task down, and commands the Coder.
  - **Coder (`claude`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while `gemini_wsl` can use its own tools for analysis, validation, and running shell commands.

### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`) - LOW-LEVEL EXECUTION:**
Use `apply_code_changes` for simple, direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**2. For Junior Architects (`gemini_wsl`) - HIGH-LEVEL DELEGATION:**
Use `execute_strategic_task` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

```json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to `routes/api.js`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:


### Local Development Agent (AGENT_LOCAL_DEV) (ID: "local_dev")
- **Description:** Cross-platform local development with SQLite
- **GUI Support:** Yes
- **Capabilities:** npm install, npm run dev, npm run dev:backend, git operations, sqlite3 commands, electron debug, file editing, testing commands, browser automation
- **Restrictions:** no PM2 commands, no PostgreSQL production operations, no systemctl, no production deployments

### Production Server Agent (AGENT_PROD_SERVER) (ID: "production_server")
- **Description:** Linux production server with PostgreSQL and PM2, with development capabilities
- **GUI Support:** No (Headless)
- **Capabilities:** pm2 restart/reload/stop/start, postgresql queries, knex migrations --env production, systemctl, log analysis, nginx operations, deployment scripts, file editing, npm install, git operations
- **Restrictions:** no electron, no GUI apps, no direct DB schema changes without migrations, always backup before migrations

### Android WSL Development Agent (AGENT_ANDROID_WSL) (ID: "android_wsl_dev")
- **Description:** Android development in Windows Subsystem for Linux with Gradle and ADB support
- **GUI Support:** No (Headless)
- **Capabilities:** ./gradlew build, ./gradlew assembleDebug, ./gradlew assembleRelease, ./gradlew clean, ./gradlew lint, ./gradlew test, adb devices, adb install, adb logcat, adb shell, git operations, file editing, gradle tasks, gradle wrapper operations
- **Restrictions:** no direct GUI access (Android Studio), requires /mnt/c/ path for Windows file system access, no Android emulator control (emulator runs on Windows host), limited USB device access through WSL, no hardware debugging interfaces

### Gemini WSL Agent (Junior Architect) (ID: "gemini_wsl")
- **Description:** Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.
- **GUI Support:** No (Headless)
- **Capabilities:** git operations, npm install, file editing, gemini (self), claude (delegate)
- **Restrictions:** Runs only inside the WSL environment

### Gemini Windows Agent (Standalone) (ID: "gemini_windows")
- **Description:** Gemini, running in native Windows (PowerShell). Can only access Windows tools.
- **GUI Support:** Yes
- **Capabilities:** git operations, npm install, file editing, gemini (self)
- **Restrictions:** Runs only in native Windows, Cannot access WSL-only tools like claude



## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .gemini/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îî‚îÄ‚îÄ gpt.toml
‚îú‚îÄ‚îÄ create/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ commands/
‚îÇ               ‚îî‚îÄ‚îÄ extensions/
‚îÇ                   ‚îî‚îÄ‚îÄ sample-extension.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ askGpt.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoDocs.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProfiles.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruneSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queryProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.test.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ segmenter.js
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresConnector.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_simple.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysisService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.test.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcherService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddingService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gptService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gptService.test.js
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architect-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ envScanRequest.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitWorkflow.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgent.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorMode.md
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ test_prune.md/
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ codex_delegation_snapshot.md
‚îú‚îÄ‚îÄ COMMANDS_REFERENCE.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ knexfile.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ setup.json
‚îú‚îÄ‚îÄ setup.json.bak
‚îú‚îÄ‚îÄ test_knexfile.js
‚îú‚îÄ‚îÄ test_snapshot.md
‚îî‚îÄ‚îÄ vitest.config.js
```

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs": [
      0,
      0.3091314011805184,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs-monorepo": [
      0,
      0.30927098145345017,
      0,
      0
    ]
  },
  "trainingPoints": {
    "nodejs": [
      {
        "fileSizeInBytes": 229899,
        "estimatedTokens": 45980,
        "actualTokens": 71069,
        "timestamp": "2025-09-21T23:52:36.862Z"
      }
    ],
    "nodejs-monorepo": [
      {
        "fileSizeInBytes": 261612,
        "estimatedTokens": 65403,
        "actualTokens": 80909,
        "timestamp": "2025-09-22T01:50:24.392Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Google Gemini API Key - Required for the 'index' and 'query' commands
# Get your key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_API_KEY_HERE"

# PostgreSQL Connection Details
DB_HOST=127.0.0.1
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_DATABASE=eck_snapshot_db

--- File: /.gemini/tools/gpt.toml ---

description = 'Ask GPT (ChatGPT subscription) for coding tasks.'
prompt = '!{node $(pwd)/index.js ask-gpt "{{args}}"}'  # Dynamic path


--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# EckSnapshot Index
/.ecksnapshot_index/

# Generated snapshot files
*_vectors.json
*_rag_*.md

# Environment variables
.env
.env.local
.env.production



# Added by eck-snapshot to ignore metadata directory
.eck/

# Test snapshot directory
create-snapshot/

# Claude Code settings (exclude all except commands/eck)
.claude/
!.claude/commands/
!.claude/commands/eck/
!.claude/commands/eck/**

# Third-party managed components
/managed_components/


--- File: /COMMANDS_REFERENCE.md ---

# Commands Reference

This document contains essential commands and setup instructions for the project's multi-agent architecture.

## Junior Architect (`gemini_wsl`) Setup Guide

This section explains how the `gemini_wsl` agent (Junior Architect) is configured to delegate coding tasks to the `claude` agent (Coder).

This architecture relies on `gemini-cli`'s custom tool feature.

## 1. Agent Definition

The Junior Architect (JA) is the `gemini_wsl` agent, which is an instance of `gemini-cli` running in WSL. It is defined in `setup.json`.

## 2. Custom Command (`claude.toml`)

The JA's ability to delegate to the Coder (`claude`) is defined by a custom `gemini-cli` command.

This command must be defined in a file named `claude.toml` and placed in the `gemini-cli` configuration directory (e.g., `~/.gemini/tools/claude.toml`).

### `claude.toml` Content

```toml
description = "Ask Claude (from the eckSnapshot project) to help with a task."
prompt = """!{node /mnt/c/Users/xelth/eckSnapshot/index.js ask-claude "{{args}}"}"""
```

## 3. JA Prompt (`agent-prompt.template.md`)

The JA *knows* how to *use* this command because its main system prompt (loaded from `src/templates/agent-prompt.template.md`) instructs it to.

This prompt *mandates* that the `{{args}}` it passes to the `/claude` command must be a single-line **JSON string** in the `apply_code_changes` format.

## 4. Execution Flow

1.  **Senior Architect (Gemini)** gives a high-level `execute_strategic_task` to `gemini_wsl`.
2.  **`gemini_wsl` (JA)** analyzes the task and formulates a low-level `apply_code_changes` JSON payload.
3.  **`gemini_wsl`** executes `/claude` with the JSON payload as a single string argument (`{{args}}`).
4.  **`claude.toml`** executes the `eck-snapshot ask-claude "{...}"` shell command.
5.  **`eck-snapshot`** (specifically `claudeCliService.js`) receives the JSON string as a 'prompt'.
6.  It forwards this prompt to the `claude-cli` binary (`local_dev`), which is smart enough to parse the JSON and execute the `apply_code_changes` task.

## Claude Code Commands

### Commit Command (`.claude/commands/eck/commit.md`)

A custom command for structured commits with automatic journaling. Place this file in `.claude/commands/eck/commit.md` to enable it in Claude Code.

**Usage:** `/commit <type> <scope> <summary> <details>`

**Function:**
- Stages all current changes
- Creates YAML frontmatter for journal entry with task_id, date, type, scope
- Creates markdown body with summary and details
- Prepends complete journal entry to `.eck/JOURNAL.md`
- Creates conventional commit message: `{type}({scope}): {summary}`
- Executes the commit

**Example:** `/commit feat api "Add user authentication" "Implemented JWT-based auth with login/logout endpoints"`

**Important:** This command should be preserved in git (via `.gitignore` rules) so it can be recreated if lost. The command integrates with the project's `.eck` manifest system for structured development journaling.

## Setup Gemini Command

### Dynamic Path Configuration

The `setup-gemini` command automates the creation of `claude.toml` with dynamic path resolution, eliminating the need for hardcoded paths like `/mnt/c/...`.

**Usage:** `eck-snapshot setup-gemini [options]`

**Options:**
- `-v, --verbose` - Show detailed output and error information

**Function:**
- Detects `gemini-cli` installation using `which()`
- Resolves current project path using `process.cwd()`
- Creates `~/.gemini/tools/claude.toml` with dynamic paths
- Integrates with `setup.json` for environment variable configuration
- Handles cross-platform compatibility (WSL/Windows/macOS/Linux)

**Example Generated `claude.toml`:**
```toml
[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["/home/user/projects/eckSnapshot/index.js", "ask-claude"]

[claude.metadata]
platform = "linux"
working_directory = "/home/user/projects/eckSnapshot"
```

**Benefits:**
- ‚úÖ No hardcoded `/mnt/c/` paths
- ‚úÖ Works across all platforms automatically
- ‚úÖ Resolves paths dynamically based on current working directory
- ‚úÖ Integrates with existing `setup.json` configuration
- ‚úÖ Validates `gemini-cli` and `index.js` availability before setup

## MCP Integration with Feedback Support

### MCP Feedback Object

The `mcp_feedback` object provides enhanced error reporting and status tracking for MCP (Message Control Protocol) integration with gemini-cli delegation.

**Structure:**
```json
{
  "mcp_feedback": {
    "success": true,
    "errors": [],
    "mcp_version": "1.0"
  }
}
```

**Properties:**
- `success` (boolean): Indicates if the MCP operation completed successfully
- `errors` (array): List of error messages or issues encountered during execution
- `mcp_version` (string): Version of MCP protocol used

**Usage in Payloads:**

Add `mcp_feedback` to the `post_execution_steps` object in your JSON command payloads:

```json
{
  "target_agent": "local_dev",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "Implement user authentication",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "auth",
        "summary": "Add JWT authentication"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**Implementation:**

The `claudeCliService.js` automatically parses `mcp_feedback` from incoming prompts and includes it in the response object. When errors are present, they are logged to the console for debugging purposes.

**Benefits:**
- ‚úÖ Enhanced error reporting for MCP operations
- ‚úÖ Version tracking for protocol compatibility
- ‚úÖ Automatic logging of MCP errors
- ‚úÖ Improved debugging for gemini-cli delegation issues

## Auto-Generated Gemini Extensions

*This section is automatically generated. Run `npm run docs:auto` to update.*

### sample-extension

Sample Gemini extension for demonstration

**Commands:**

- **sample-command**: A sample command for testing auto-docs
  - Usage: `sample-command [options]`
  - Examples: `sample-command --help`

**Tools:**

- **sample-tool**: A sample tool for testing auto-docs
  - Usage: Use this tool for sample operations



--- File: /codex_delegation_snapshot.md ---

üîç WSL detected, using configured host: 172.29.16.1


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /knexfile.js ---

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
const isWSL = process.platform === 'linux' && 
  (process.env.WSL_DISTRO_NAME || 
   fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));

if (isWSL && (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1')) {
  // For WSL, always use the standard WSL2 gateway IP
  process.env.DB_HOST = '172.29.16.1';
}

export default {
  development: {
    client: 'pg',
    connection: {
      host: process.env.DB_HOST || '127.0.0.1',
      port: process.env.DB_PORT || 5432,
      user: process.env.DB_USER || 'myuser',
      password: process.env.DB_PASSWORD || 'mypassword',
      database: process.env.DB_DATABASE || 'eck_snapshot_db',
    },
    pool: {
      min: 2,
      max: 10
    }
  }
};

--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-c": "^0.21.4",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /packages/cli/src/commands/extensions/sample-extension.json ---

{
  "name": "sample-extension",
  "description": "Sample Gemini extension for demonstration",
  "commands": [
    {
      "name": "sample-command",
      "description": "A sample command for testing auto-docs",
      "usage": "sample-command [options]",
      "examples": [
        "sample-command --help"
      ]
    }
  ],
  "tools": [
    {
      "name": "sample-tool",
      "description": "A sample tool for testing auto-docs",
      "usage": "Use this tool for sample operations"
    }
  ]
}

--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": [
          "build.gradle",
          "build.gradle.kts",
          "settings.gradle",
          "settings.gradle.kts"
        ],
        "directories": [
          "app/src/main",
          "app/src/androidTest"
        ],
        "manifestFiles": [
          "AndroidManifest.xml"
        ],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": [
          "package.json"
        ],
        "directories": [
          "packages",
          "apps",
          "libs"
        ],
        "patterns": [
          "workspaces",
          "lerna",
          "nx",
          "rush"
        ],
        "priority": 7
      },
      "nodejs": {
        "files": [
          "package.json"
        ],
        "directories": [
          "node_modules"
        ],
        "priority": 6
      },
      "python-poetry": {
        "files": [
          "pyproject.toml"
        ],
        "patterns": [
          "tool.poetry"
        ],
        "priority": 9
      },
      "python-pip": {
        "files": [
          "requirements.txt",
          "setup.py",
          "setup.cfg"
        ],
        "directories": [
          "__pycache__",
          "venv",
          ".venv"
        ],
        "priority": 7
      },
      "python-conda": {
        "files": [
          "environment.yml",
          "environment.yaml",
          "conda.yml"
        ],
        "priority": 8
      },
      "django": {
        "files": [
          "manage.py"
        ],
        "patterns": [
          "django",
          "Django"
        ],
        "priority": 9
      },
      "flask": {
        "files": [
          "app.py",
          "application.py"
        ],
        "patterns": [
          "flask",
          "Flask"
        ],
        "priority": 8
      },
      "flutter": {
        "files": [
          "pubspec.yaml"
        ],
        "directories": [
          "lib",
          "android",
          "ios"
        ],
        "priority": 8
      },
      "react-native": {
        "files": [
          "package.json"
        ],
        "directories": [
          "android",
          "ios"
        ],
        "patterns": [
          "react-native"
        ],
        "priority": 8
      },
      "rust": {
        "files": [
          "Cargo.toml"
        ],
        "directories": [
          "src",
          "target"
        ],
        "priority": 9
      },
      "go": {
        "files": [
          "go.mod",
          "go.sum"
        ],
        "directories": [
          "cmd",
          "pkg",
          "internal"
        ],
        "priority": 7
      },
      "dotnet": {
        "files": [
          "*.csproj",
          "*.sln",
          "*.fsproj",
          "*.vbproj"
        ],
        "directories": [
          "bin",
          "obj"
        ],
        "priority": 7
      },
      "c": {
        "files": [
          "Makefile",
          "CMakeLists.txt",
          "*.c",
          "*.h"
        ],
        "directories": [
          "src",
          "include",
          "lib"
        ],
        "priority": 6
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "smartMode": {
    "tokenThreshold": 7000000,
    "_comment": "Projects over this token count will use vector indexing instead of single file snapshots"
  },
  "vectorIndex": {
    "autoExportOnIndex": true,
    "_comment": "Automatically export the vector index to a file after every successful 'index' command."
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "npm-shrinkwrap.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*",
      "LICENSE*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "coverage/",
      "create-snapshot/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      },
      "c": {
        "filesToIgnore": [
          "*.o",
          "*.a",
          "*.so",
          "*.out",
          "*.exe"
        ],
        "dirsToIgnore": [
          "build/",
          "cmake-build-debug/",
          "cmake-build-release/",
          ".cmake/"
        ],
        "extensionsToIgnore": [
          ".o",
          ".a",
          ".so",
          ".out"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./.eck/snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      },
      "gemini_wsl": {
        "active": true,
        "name": "Gemini WSL Agent (Junior Architect)",
        "description": "Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "gemini",
            "claude"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)",
          "claude (delegate)"
        ],
        "restrictions": [
          "Runs only inside the WSL environment"
        ]
      },
      "gemini_windows": {
        "active": true,
        "name": "Gemini Windows Agent (Standalone)",
        "description": "Gemini, running in native Windows (PowerShell). Can only access Windows tools.",
        "guiSupport": true,
        "identification": {
          "markers": [
            "Windows",
            "gemini",
            "!WSL"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)"
        ],
        "restrictions": [
          "Runs only in native Windows",
          "Cannot access WSL-only tools like claude"
        ]
      }
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "src/templates/envScanRequest.md",
      "gitWorkflow": "src/templates/gitWorkflow.md",
      "multiAgent": "src/templates/multiAgent.md",
      "vectorMode": "src/templates/vectorMode.md",
      "agent": "src/templates/agent-prompt.template.md"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject, viewIndex } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .action(createRepoSnapshot)
    .addHelpText('after', `
Examples for --profile:
  --profile backend                      (Uses the 'backend' profile)
  --profile "backend,-**/tests/**"         (Uses 'backend' profile, excludes all test files)
  --profile "src/**/*.js,-**/*.test.js"  (Includes all JS files in src, excludes tests)

  Combine predefined profiles (from .eck/profiles.json) with ad-hoc glob patterns.
  Prefix a profile name or glob pattern with '-' to exclude it.
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Index view command
  program
    .command('index-view')
    .description('View the contents of the code chunks database')
    .option('--limit <number>', 'Number of records to display', (val) => parseInt(val), 10)
    .option('--offset <number>', 'Number of records to skip', (val) => parseInt(val), 0)
    .option('--file <path>', 'Filter by file path')
    .action(viewIndex);

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/cli/commands/askGpt.js ---

import { ask } from '../../services/gptService.js';

/**
 * CLI entry point for ask-gpt command.
 * @param {string} payload - JSON payload string.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} options - CLI options.
 */
export async function askGpt(payload, options = {}) {
  const verbose = Boolean(options.verbose);
  const model = options.model || 'gpt-5-codex';
  const reasoning = options.reasoning || 'high';

  if (!payload) {
    console.error('ask-gpt requires a JSON payload argument.');
    process.exitCode = 1;
    return;
  }

  try {
    const result = await ask(payload, { verbose, model, reasoning });
    console.log(JSON.stringify(result, null, 2));
  } catch (error) {
    console.error(error.message);
    if (verbose && error?.stack) {
      console.error(error.stack);
    }
    process.exitCode = 1;
  }
}


--- File: /src/cli/commands/autoDocs.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Auto-generate documentation from gemini-extension.json files
 */
export async function generateAutoDocs() {
  try {
    const projectRoot = path.resolve(__dirname, '../../../');
    const extensionsDir = path.join(projectRoot, 'packages/cli/src/commands/extensions');
    const referenceFile = path.join(projectRoot, 'COMMANDS_REFERENCE.md');

    // Check if extensions directory exists
    try {
      await fs.access(extensionsDir);
    } catch (error) {
      console.log(`Extensions directory not found at: ${extensionsDir}`);
      console.log('Creating example structure...');

      // Create the directory structure
      await fs.mkdir(extensionsDir, { recursive: true });

      // Create a sample gemini-extension.json file for demonstration
      const sampleExtension = {
        name: "sample-extension",
        description: "Sample Gemini extension for demonstration",
        commands: [
          {
            name: "sample-command",
            description: "A sample command for testing auto-docs",
            usage: "sample-command [options]",
            examples: ["sample-command --help"]
          }
        ],
        tools: [
          {
            name: "sample-tool",
            description: "A sample tool for testing auto-docs",
            usage: "Use this tool for sample operations"
          }
        ]
      };

      await fs.writeFile(
        path.join(extensionsDir, 'sample-extension.json'),
        JSON.stringify(sampleExtension, null, 2)
      );

      console.log('Created sample extension at:', path.join(extensionsDir, 'sample-extension.json'));
    }

    // Read all JSON files in the extensions directory
    const files = await fs.readdir(extensionsDir);
    const jsonFiles = files.filter(file => file.endsWith('.json'));

    if (jsonFiles.length === 0) {
      console.log('No JSON files found in extensions directory');
      return;
    }

    console.log(`Found ${jsonFiles.length} extension file(s): ${jsonFiles.join(', ')}`);

    // Parse each JSON file and extract command/tool information
    const extensions = [];

    for (const file of jsonFiles) {
      try {
        const filePath = path.join(extensionsDir, file);
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = JSON.parse(content);
        extensions.push({ filename: file, ...extension });
        console.log(`Parsed extension: ${extension.name || file}`);
      } catch (error) {
        console.warn(`Failed to parse ${file}:`, error.message);
      }
    }

    if (extensions.length === 0) {
      console.log('No valid extension files found');
      return;
    }

    // Generate markdown content
    let markdownContent = '\n## Auto-Generated Gemini Extensions\n\n';
    markdownContent += '*This section is automatically generated. Run `npm run docs:auto` to update.*\n\n';

    for (const extension of extensions) {
      markdownContent += `### ${extension.name || extension.filename}\n\n`;

      if (extension.description) {
        markdownContent += `${extension.description}\n\n`;
      }

      // Add commands section
      if (extension.commands && extension.commands.length > 0) {
        markdownContent += '**Commands:**\n\n';
        for (const command of extension.commands) {
          markdownContent += `- **${command.name}**: ${command.description || 'No description'}\n`;
          if (command.usage) {
            markdownContent += `  - Usage: \`${command.usage}\`\n`;
          }
          if (command.examples && command.examples.length > 0) {
            markdownContent += `  - Examples: ${command.examples.map(ex => `\`${ex}\``).join(', ')}\n`;
          }
        }
        markdownContent += '\n';
      }

      // Add tools section
      if (extension.tools && extension.tools.length > 0) {
        markdownContent += '**Tools:**\n\n';
        for (const tool of extension.tools) {
          markdownContent += `- **${tool.name}**: ${tool.description || 'No description'}\n`;
          if (tool.usage) {
            markdownContent += `  - Usage: ${tool.usage}\n`;
          }
        }
        markdownContent += '\n';
      }
    }

    // Read the current COMMANDS_REFERENCE.md
    let currentContent;
    try {
      currentContent = await fs.readFile(referenceFile, 'utf-8');
    } catch (error) {
      console.warn('COMMANDS_REFERENCE.md not found, creating new file');
      currentContent = '# Commands Reference\n\n';
    }

    // Remove existing auto-generated section if it exists
    const autoGenRegex = /\n## Auto-Generated Gemini Extensions[\s\S]*?(?=\n## |\n# |$)/;
    const updatedContent = currentContent.replace(autoGenRegex, '') + markdownContent;

    // Write the updated content back to the file
    await fs.writeFile(referenceFile, updatedContent);

    console.log('\n‚úÖ Auto-documentation generated successfully!');
    console.log(`üìù Updated: ${referenceFile}`);
    console.log(`üì¶ Processed ${extensions.length} extension(s)`);

  } catch (error) {
    console.error('Failed to generate auto-docs:', error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return {
          content: `--- File: /${normalizedPath} ---\n\n${content}\n\n`,
          path: normalizedPath,
          size: fileStats.size
        };
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const successfulFileObjects = results.filter(Boolean);
    const contentArray = successfulFileObjects.map(f => f.content);

    // Return all processed data instead of writing file
    return {
      stats,
      contentArray,
      successfulFileObjects,
      allFiles,
      originalCwd,
      repoPath
    };
    
  } finally {
    process.chdir(originalCwd); // Ensure we always change back
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      smartModeTokenThreshold: setupConfig.smartMode.tokenThreshold,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    if (estimation.estimatedTokens > config.smartModeTokenThreshold) {
      spinner.succeed('Project is large. Switching to vector indexing mode.');
      await indexProject(repoPath, options);
    } else {
      spinner.succeed('Project is small. Creating dual snapshots...');
      
      // Step 1: Process all files ONCE
      const { 
        stats, 
        contentArray, 
        successfulFileObjects, 
        allFiles, 
        originalCwd: processingOriginalCwd, // We get originalCwd from the processing function
        repoPath: processedRepoPath 
      } = await processProjectFiles(repoPath, options, config, projectDetection.type);

      const originalCwd = process.cwd(); // Get CWD *before* chdir
      process.chdir(processedRepoPath); // Go back to repo path for git hash and tree

      try {
        // --- Common Data --- 
        const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
        const repoName = path.basename(processedRepoPath);
        const gitHash = await getGitCommitHash(processedRepoPath);
        const fileExtension = options.format || config.defaultFormat || 'md';
        const outputPath = options.output || path.resolve(originalCwd, config.output);
        await fs.mkdir(outputPath, { recursive: true });

        const shouldIncludeTree = config.tree && !options.noTree;
        let directoryTree = '';
        if (shouldIncludeTree) {
          console.log('üå≥ Generating directory tree...');
          directoryTree = await generateDirectoryTree(processedRepoPath, '', allFiles, 0, config.maxDepth || 10, config);
        }

        // Calculate included file stats by extension
        const includedFilesByType = new Map();
        for (const fileObj of successfulFileObjects) {
            try {
                let ext = path.extname(fileObj.path);
                if (ext === '') ext = '.no-extension';
                includedFilesByType.set(ext, (includedFilesByType.get(ext) || 0) + 1);
            } catch (e) { /* Silently ignore */ }
        }
        const sortedIncludedStats = [...includedFilesByType.entries()].sort((a, b) => b[1] - a[1]);

        // Calculate Top 10 Largest Files
        const largestFiles = [...successfulFileObjects].sort((a, b) => b.size - a.size).slice(0, 10);

        const fileBody = (directoryTree ? `\n## Directory Structure\n\n\`\`\`\n${directoryTree}\`\`\`\n\n` : '') + contentArray.join('');

        // --- File 1: Architect Snapshot --- 
        const architectOptions = { ...options, agent: false };
        // Load manifest for headers
        const eckManifest = await loadProjectEckManifest(processedRepoPath);
        const isGitRepo = await checkGitRepository(processedRepoPath);

        const architectHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: architectOptions, repoPath: processedRepoPath }, isGitRepo);
        const architectBaseFilename = `${repoName}_snapshot_${timestamp}${gitHash ? `_${gitHash}` : ''}`;
        const architectFilename = `${architectBaseFilename}.${fileExtension}`;
        const architectFilePath = path.join(outputPath, architectFilename);
        await fs.writeFile(architectFilePath, architectHeader + fileBody);

        // --- File 2: Junior Architect Snapshot --- 
        let jaFilePath = null;
        if (!options.profile && !options.agent && fileExtension === 'md') { // Only create JA snapshot if main is MD
          console.log('üñãÔ∏è Generating Junior Architect (_ja) snapshot...');
          const jaOptions = { ...options, agent: true, noTree: false, noAiHeader: false };
          const jaHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: jaOptions, repoPath: processedRepoPath }, isGitRepo);
          const jaFilename = `${architectBaseFilename}_ja.${fileExtension}`;
          jaFilePath = path.join(outputPath, jaFilename);
          await fs.writeFile(jaFilePath, jaHeader + fileBody);
        }

        // --- Combined Report --- 
        console.log('\n‚úÖ Snapshot generation complete!');
        console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        console.log(`üìÑ Architect File: ${architectFilePath}`);
        if (jaFilePath) {
          console.log(`üìÑ Junior Arch File: ${jaFilePath}`);
        }
        console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
        console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
        console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
        console.log(`üìã Format: ${fileExtension.toUpperCase()}`);

        if (sortedIncludedStats.length > 0) {
          console.log('\nüì¶ Included File Types:');
          console.log('---------------------------------');
          for (const [ext, count] of sortedIncludedStats.slice(0, 10)) {
              console.log(`   - ${String(ext).padEnd(15)} ${String(count).padStart(5)} files`);
          }
          if (sortedIncludedStats.length > 10) {
              console.log(`   ... and ${sortedIncludedStats.length - 10} other types.`);
          }
        }

        if (largestFiles.length > 0) {
          console.log('\nüêò Top 10 Largest Files (Included):');
          console.log('---------------------------------');
          for (const fileObj of largestFiles) {
              console.log(`   - ${String(formatSize(fileObj.size)).padEnd(15)} ${fileObj.path}`);
          }
        }
        
        // Excluded/Skipped Files Section
        const hasExcludedContent = stats.excludedFiles > 0 || stats.binaryFiles > 0 || stats.oversizedFiles > 0 || stats.ignoredFiles > 0 || stats.errors.length > 0;
        if (hasExcludedContent) {
          console.log('\nüö´ Excluded/Skipped Files:');
          console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        }
        
        if (stats.excludedFiles > 0) {
          console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
        }
        if (stats.binaryFiles > 0) {
          console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
        }
        if (stats.oversizedFiles > 0) {
          console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
        }
        if (stats.ignoredFiles > 0) {
          console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
        }
        if (stats.errors.length > 0) {
          console.log(`‚ùå Errors: ${stats.errors.length}`);
          if (options.verbose) {
            stats.errors.forEach(err => console.log(`   ${err}`));
          }
        }
        
        // Print detailed skip reasons report
        if (stats.skippedFilesDetails.size > 0) {
          console.log('\nüìã Skip Reasons:');
          console.log('---------------------------------');
          
          for (const [reason, files] of stats.skippedFilesDetails.entries()) {
            console.log(`\nüî∏ ${reason} (${files.length} files):`);
            files.forEach(file => {
              console.log(`   ‚Ä¢ ${file}`);
            });
          }
          console.log('---------------------------------');
        } else {
          console.log('---------------------------------');
        }
        
        // Generate training command string if estimation data is available
        if (estimation && projectDetection.type && !options.profile) {
          const trainingCommand = generateTrainingCommand(projectDetection.type, estimation.estimatedTokens, estimation.totalSize, repoPath);
          console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
          console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
          console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
        }

      } finally {
        process.chdir(originalCwd); // Final reset back to original CWD
      }
    }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}

--- File: /src/cli/commands/detectProfiles.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Extracts a JSON object from a string that might contain markdown wrappers or log output.
 * Finds the first opening brace '{' and the last closing brace '}' to extract the JSON.
 */
function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }

  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1).trim();
  }

  return text.trim();
}

/**
 * Scans the project structure, saves the directory tree to a file, and asks an AI to generate
 * context profiles, saving them to .eck/profiles.json.
 */
export async function detectProfiles(repoPath, options) {
  const spinner = ora('Initializing and scanning project structure...').start();
  try {
    await initializeEckManifest(repoPath);

    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const config = {
        ...userConfig,
        ...setupConfig.fileFiltering,
        ...setupConfig.performance
    };

    const allFiles = await scanDirectoryRecursively(repoPath, config, repoPath);
    spinner.text = 'Generating directory tree...';
    const dirTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth, config);

    if (!dirTree) {
        throw new Error('Failed to generate directory tree or project is empty.');
    }

    spinner.text = 'Saving directory tree to file...';
    const treeFilePath = path.join(repoPath, '.eck', 'directory_tree_for_profiling.md');
    await fs.writeFile(treeFilePath, dirTree);

    const prompt = `You are a code architect. Based on the file directory tree found in the file at './.eck/directory_tree_for_profiling.md', please identify logical 'context profiles' for splitting the project.
Your output MUST be ONLY a valid JSON object.
The keys of the object MUST be the profile names (e.g., 'frontend', 'backend', 'core-logic', 'docs').
The values MUST be an object containing 'include' and 'exclude' arrays of glob patterns.
Example: {"frontend": {"include": ["packages/ui/**"], "exclude": []}, "docs": {"include": ["docs/**"], "exclude": []}}.
DO NOT add any conversational text, introductory sentences, or explanations. Your entire response must be ONLY the JSON object.`;

    spinner.text = 'Asking AI to analyze directory tree and detect profiles...';
    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;

    if (!rawText || typeof rawText.replace !== 'function') {
      throw new Error(`AI returned invalid content type: ${typeof rawText}`);
    }

    spinner.text = 'Saving generated profiles...';
    const cleanedJson = extractJson(rawText);
    let parsedProfiles;
    try {
        parsedProfiles = JSON.parse(cleanedJson);
    } catch (e) {
        console.error('\nInvalid JSON received from AI:', cleanedJson);
        throw new Error(`AI returned invalid JSON: ${e.message}`);
    }

    const outputPath = path.join(repoPath, '.eck', 'profiles.json');
    await fs.writeFile(outputPath, JSON.stringify(parsedProfiles, null, 2));

    const profileKeys = Object.keys(parsedProfiles);
    spinner.succeed(`Successfully detected and saved ${profileKeys.length} profiles to ${outputPath}`);

    console.log('\n‚ú® Detected Profiles:');
    console.log('---------------------------');
    for (const profileName of profileKeys) {
        console.log(`  - ${profileName}`);
    }
    console.log('\nYou can now use these profile names with the --profile flag.');

  } catch (error) {
    spinner.fail(`Failed to detect profiles: ${error.message}`);
  }
}


--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));
    
    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
      console.log('   eck-snapshot index                              # For large projects');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
      console.log('   eck-snapshot index                              # For large projects');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
      console.log('   eck-snapshot index                              # For semantic search');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    let files = await getProjectFiles(projectPath);
    
    // --- Apply Advanced Profile Filtering ---
    const defaultProfile = await getProfile('default', projectPath);
    if (options.profile) {
        mainSpinner.text = `Applying profile filter: '${options.profile}'...`;
        files = await applyProfileFilter(files, options.profile, projectPath);
        mainSpinner.info(`Filtered down to ${files.length} files using profile: '${options.profile}'.`);
    } else if (defaultProfile) {
        mainSpinner.text = "Applying detected 'default' profile...";
        files = micromatch(files, defaultProfile.include, { ignore: defaultProfile.exclude });
        mainSpinner.info(`Filtered down to ${files.length} files using detected 'default' profile.`);
    }
    if (files.length === 0) {
        throw new Error(`Profile filter resulted in 0 files. Aborting.`);
    }
    // --- End Profile Filtering ---

    const profileName = options.profile || 'default';
    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await releaseAnalysisModel();
    await releaseEmbeddingModel();
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}


--- File: /src/cli/commands/pruneSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { parseSnapshotContent, parseSize, formatSize } from '../../utils/fileUtils.js';

function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }
  const firstBracket = text.indexOf('[');
  const lastBracket = text.lastIndexOf(']');
  if (firstBracket !== -1 && lastBracket !== -1 && lastBracket > firstBracket) {
    return text.substring(firstBracket, lastBracket + 1).trim();
  }
  return text.trim();
}

export async function pruneSnapshot(snapshotFile, options) {
  const spinner = ora('Starting snapshot pruning process...').start();
  try {
    const targetSize = parseSize(options.targetSize);
    spinner.text = `Reading snapshot file: ${snapshotFile}`;
    const snapshotContent = await fs.readFile(snapshotFile, 'utf-8');
    const snapshotHeader = snapshotContent.split('--- File: /')[0];
    const files = parseSnapshotContent(snapshotContent);

    if (files.length === 0) {
      spinner.warn('No files found in the snapshot.');
      return;
    }

    const currentSize = Buffer.byteLength(snapshotContent, 'utf-8');
    if (currentSize <= targetSize) {
      spinner.succeed(`Snapshot is already smaller than the target size. (${formatSize(currentSize)} < ${formatSize(targetSize)})`);
      return;
    }

    spinner.text = 'Asking AI to rank files by importance...';
    const filePaths = files.map(f => f.path);
    const prompt = `You are a software architect. Given the following list of file paths from a project snapshot, rank them by importance for understanding the project's core functionality. The most critical files (e.g., entry points, core logic, configurations) should be first. Your output MUST be ONLY a JSON array of strings, with the file paths in ranked order. Do not add any other text.\n\nFILE LIST:\n${JSON.stringify(filePaths, null, 2)}`;

    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;
    const cleanedJson = extractJson(rawText);

    let rankedFiles;
    try {
      rankedFiles = JSON.parse(cleanedJson);
      if (!Array.isArray(rankedFiles) || rankedFiles.some(item => typeof item !== 'string')) {
        throw new Error('AI response is not an array of strings.');
      }
    } catch (e) {
      spinner.fail(`Failed to parse AI's file ranking: ${e.message}`);
      console.error('Received from AI:', cleanedJson);
      return;
    }

    spinner.text = 'Building pruned snapshot...';
    const fileMap = new Map(files.map(f => [f.path, f.content]));
    let newSnapshotContent = snapshotHeader;
    let newSize = Buffer.byteLength(newSnapshotContent, 'utf-8');
    let filesIncluded = 0;

    for (const filePath of rankedFiles) {
      if (fileMap.has(filePath)) {
        const fileContent = fileMap.get(filePath);
        const fileEntry = `--- File: /${filePath} ---\n\n${fileContent}\n\n`;
        const entrySize = Buffer.byteLength(fileEntry, 'utf-8');

        if (newSize + entrySize > targetSize) {
          break;
        }

        newSnapshotContent += fileEntry;
        newSize += entrySize;
        filesIncluded++;
      }
    }

    const outputFilename = `${path.basename(snapshotFile, path.extname(snapshotFile))}_pruned_${options.targetSize}${path.extname(snapshotFile)}`;
    const outputPath = path.join(path.dirname(snapshotFile), outputFilename);

    await fs.writeFile(outputPath, newSnapshotContent);

    spinner.succeed('Snapshot pruning complete!');
    console.log(`- Original Size: ${formatSize(currentSize)}`);
    console.log(`- New Size: ${formatSize(newSize)}`);
    console.log(`- Files Included: ${filesIncluded} / ${files.length}`);
    console.log(`- Pruned snapshot saved to: ${outputPath}`);

  } catch (error) {
    spinner.fail(`An error occurred during pruning: ${error.message}`);
  }
}


--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-—Å–Ω–∞–ø—à–æ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

export async function viewIndex(options) {
  const spinner = ora('Connecting to database...').start();
  const knex = getKnex();

  try {
    await initDb();

    spinner.text = 'Fetching code chunks from database...';

    // Build query with optional filters
    let query = knex('code_chunks')
      .select('id', 'file_path', 'chunk_type', 'chunk_name', 'profile')
      .orderBy('id', 'asc');

    // Apply file filter if specified
    if (options.file) {
      query = query.where('file_path', 'like', `%${options.file}%`);
      spinner.info(`Filtering by file path: ${options.file}`);
    }

    // Apply pagination
    if (options.limit) {
      query = query.limit(options.limit);
    }
    if (options.offset) {
      query = query.offset(options.offset);
    }

    const chunks = await query;

    if (chunks.length === 0) {
      spinner.warn('No code chunks found in the database.');
      return;
    }

    spinner.succeed(`Found ${chunks.length} code chunks`);

    // Display results in a formatted table
    console.log('\nüìä Code Chunks Index:');
    console.log('‚ïê'.repeat(100));
    console.table(chunks.map(chunk => ({
      ID: chunk.id,
      'File Path': chunk.file_path.replace(process.cwd(), '.'),
      Type: chunk.chunk_type,
      Name: chunk.chunk_name,
      Profile: chunk.profile || 'default'
    })));

    // Show summary
    const totalCount = await knex('code_chunks').count('* as count').first();
    console.log(`\nShowing ${chunks.length} of ${totalCount.count} total chunks`);

    if (options.limit && chunks.length === options.limit) {
      console.log(`\nüí° Use --offset ${(options.offset || 0) + options.limit} to view the next page`);
    }

  } catch (error) {
    spinner.fail(`Failed to view index: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/setupGemini.js ---

import which from 'which';
import path from 'path';
import fs from 'fs/promises';
import os from 'os';
import chalk from 'chalk';

/**
 * Sets up claude.toml configuration for gemini-cli integration with dynamic paths
 * @param {Object} options - Command options
 */
export async function setupGemini(options = {}) {
  try {
    console.log(chalk.blue('üîß Setting up gemini-cli integration with dynamic paths...'));

    // Check if gemini-cli is installed
    let geminiCliPath;
    try {
      geminiCliPath = await which('gemini-cli');
      console.log(chalk.green(`‚úÖ Found gemini-cli at: ${geminiCliPath}`));
    } catch (error) {
      console.error(chalk.red('‚ùå gemini-cli not found in PATH'));
      console.log(chalk.yellow('üí° Please install gemini-cli first:'));
      console.log(chalk.cyan('   npm install -g gemini-cli'));
      process.exit(1);
    }

    // Get current working directory for dynamic path resolution
    const currentDir = process.cwd();
    const indexJsPath = path.join(currentDir, 'index.js');

    // Verify index.js exists
    try {
      await fs.access(indexJsPath);
      console.log(chalk.green(`‚úÖ Found eck-snapshot index.js at: ${indexJsPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Could not find index.js at: ${indexJsPath}`));
      console.log(chalk.yellow('üí° Make sure you are running this command from the eck-snapshot project directory'));
      process.exit(1);
    }

    // Create gemini tools directory
    const homeDir = os.homedir();
    const geminiToolsDir = path.join(homeDir, '.gemini', 'tools');

    try {
      await fs.mkdir(geminiToolsDir, { recursive: true });
      console.log(chalk.green(`‚úÖ Created/verified gemini tools directory: ${geminiToolsDir}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to create gemini tools directory: ${error.message}`));
      process.exit(1);
    }

    // Read environment variables from setup.json if available
    let envVars = {};
    try {
      const setupJsonPath = path.join(currentDir, 'setup.json');
      const setupContent = await fs.readFile(setupJsonPath, 'utf-8');
      const setupData = JSON.parse(setupContent);

      // Extract relevant environment variables
      if (setupData.environmentDetection) {
        envVars.ECK_SNAPSHOT_PATH = currentDir;
        console.log(chalk.blue(`üìã Using project context from setup.json`));
      }
    } catch (error) {
      console.log(chalk.yellow('‚ö†Ô∏è  setup.json not found or invalid, using defaults'));
    }

    // Generate claude.toml content with dynamic paths
    const claudeTomlContent = generateClaudeToml(indexJsPath, envVars);

    // Write claude.toml file
    const claudeTomlPath = path.join(geminiToolsDir, 'claude.toml');
    try {
      await fs.writeFile(claudeTomlPath, claudeTomlContent, 'utf-8');
      console.log(chalk.green(`‚úÖ Generated claude.toml at: ${claudeTomlPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to write claude.toml: ${error.message}`));
      process.exit(1);
    }

    // Success summary
    console.log(chalk.green('\nüéâ Setup completed successfully!'));
    console.log(chalk.blue('\nüìã Configuration summary:'));
    console.log(chalk.cyan(`   ‚Ä¢ gemini-cli: ${geminiCliPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ eck-snapshot: ${indexJsPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ claude.toml: ${claudeTomlPath}`));

    if (Object.keys(envVars).length > 0) {
      console.log(chalk.cyan(`   ‚Ä¢ Environment variables: ${Object.keys(envVars).join(', ')}`));
    }

    console.log(chalk.blue('\nüöÄ You can now use:'));
    console.log(chalk.cyan('   gemini-cli claude "Your prompt here"'));
    console.log(chalk.green('\n‚ú® Cross-platform path resolution is automatically handled!'));

  } catch (error) {
    console.error(chalk.red(`‚ùå Setup failed: ${error.message}`));
    if (options.verbose) {
      console.error(chalk.red('Stack trace:'), error.stack);
    }
    process.exit(1);
  }
}

/**
 * Generates claude.toml content with dynamic paths
 * @param {string} indexJsPath - Path to eck-snapshot index.js
 * @param {Object} envVars - Environment variables to include
 * @returns {string} - Generated TOML content
 */
function generateClaudeToml(indexJsPath, envVars = {}) {
  const envSection = Object.keys(envVars).length > 0
    ? `# Environment variables from setup.json
${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}

`
    : '';

  return `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'
# This file uses dynamic paths to work across WSL/Windows environments

${envSection}[claude]
# eck-snapshot integration for AI-powered repository analysis
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

# Command examples:
# gemini-cli claude "Create a snapshot of the current project"
# gemini-cli claude "Analyze the database structure"
# gemini-cli claude "Generate a project overview"

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
generated_at = "${new Date().toISOString()}"
platform = "${process.platform}"
node_version = "${process.version}"
working_directory = "${path.dirname(indexJsPath)}"

# Cross-platform compatibility notes:
# - Paths are automatically resolved using process.cwd()
# - Works in WSL, Windows, macOS, and Linux
# - No hardcoded /mnt/c/ paths required
`;
}

--- File: /src/cli/commands/setupGemini.test.js ---

import { describe, it, expect } from 'vitest';

describe('setupGemini integration', () => {
  it('should validate path resolution logic', () => {
    // Test path join functionality that setupGemini uses
    const currentDir = '/test/project';
    const indexJsPath = `${currentDir}/index.js`;

    expect(indexJsPath).toBe('/test/project/index.js');
    expect(indexJsPath).toContain('index.js');
  });

  it('should validate gemini tools directory structure', () => {
    const homeDir = '/home/user';
    const geminiToolsDir = `${homeDir}/.gemini/tools`;
    const claudeTomlPath = `${geminiToolsDir}/claude.toml`;

    expect(geminiToolsDir).toBe('/home/user/.gemini/tools');
    expect(claudeTomlPath).toBe('/home/user/.gemini/tools/claude.toml');
  });

  it('should validate TOML content structure', () => {
    const indexJsPath = '/test/project/index.js';
    const envVars = { ECK_SNAPSHOT_PATH: '/test/project' };

    // Test environment section generation
    const envSection = Object.keys(envVars).length > 0
      ? `# Environment variables from setup.json\n${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}\n\n`
      : '';

    // Test main TOML structure
    const tomlContent = `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'

${envSection}[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
platform = "${process.platform}"
working_directory = "${indexJsPath.replace('/index.js', '')}"`;

    expect(tomlContent).toContain('[claude]');
    expect(tomlContent).toContain('name = "eck-snapshot"');
    expect(tomlContent).toContain(`args = ["${indexJsPath}", "ask-claude"]`);
    expect(tomlContent).toContain('[claude.metadata]');
    expect(tomlContent).toContain('ECK_SNAPSHOT_PATH = "/test/project"');
  });

  it('should handle cross-platform paths correctly', () => {
    const testPaths = [
      { platform: 'windows', path: 'C:\\Users\\test\\project\\index.js' },
      { platform: 'unix', path: '/home/user/project/index.js' },
      { platform: 'wsl', path: '/mnt/c/Users/test/project/index.js' }
    ];

    testPaths.forEach(({ platform, path }) => {
      expect(path).toContain('index.js');
      expect(path.length).toBeGreaterThan(0);

      // Test that the path is absolute (platform-appropriate)
      if (platform === 'windows') {
        expect(path).toMatch(/^[A-Z]:\\/);
      } else {
        expect(path).toMatch(/^\//);
      }
    });
  });

  it('should validate error handling patterns', () => {
    // Test error message patterns that setupGemini should handle
    const errorPatterns = [
      'gemini-cli not found in PATH',
      'Could not find index.js',
      'Failed to create gemini tools directory',
      'Failed to write claude.toml'
    ];

    errorPatterns.forEach(pattern => {
      expect(pattern).toBeDefined();
      expect(typeof pattern).toBe('string');
      expect(pattern.length).toBeGreaterThan(0);
    });
  });

  it('should test JSON parsing for setup.json', () => {
    const validSetupData = {
      environmentDetection: {
        detected: true
      }
    };

    const jsonString = JSON.stringify(validSetupData);
    const parsed = JSON.parse(jsonString);

    expect(parsed.environmentDetection).toBeDefined();
    expect(parsed.environmentDetection.detected).toBe(true);

    // Test invalid JSON handling pattern
    const invalidJson = 'invalid json {';
    let parseError = null;
    try {
      JSON.parse(invalidJson);
    } catch (e) {
      parseError = e;
    }

    expect(parseError).toBeDefined();
    expect(parseError.message).toContain('JSON');
  });
});

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/core/segmenter.js ---

import { parse } from '@babel/parser';
import _traverse from '@babel/traverse';
const traverse = _traverse.default;
import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import Parser from 'tree-sitter';
import Python from 'tree-sitter-python';
import Java from 'tree-sitter-java';
import Kotlin from 'tree-sitter-kotlin';
import C from 'tree-sitter-c';

function generateHash(content) {
  return crypto.createHash('sha256').update(content).digest('hex');
}

const tsParser = new Parser();
const languageParsers = {
    '.py': Python,
    '.java': Java,
    '.kt': Kotlin,
    '.c': C,
    '.h': C,
};

async function _segmentWithTreeSitter(content, filePath, language) {
    tsParser.setLanguage(language);
    const tree = tsParser.parse(content);
    const chunks = [];
    // Graph relations for tree-sitter are not implemented in this step.
    const relations = [];

    function walk(node) {
        const nodeTypeMap = {
            'function_definition': 'function', 'class_definition': 'class', // Python
            'function_declaration': 'function', 'class_declaration': 'class', // Kotlin/Java
            'method_declaration': 'function', // Java
            'struct_specifier': 'struct', 'enum_specifier': 'enum', 'union_specifier': 'union', 'type_definition': 'typedef', // C
        };

        if (nodeTypeMap[node.type]) {
            const nameNode = node.childForFieldName('name') || node.child(1);
            const chunkName = nameNode ? nameNode.text : 'anonymous';
            const chunkCode = node.text;
            chunks.push({
                filePath,
                chunk_type: nodeTypeMap[node.type],
                chunk_name: chunkName,
                code: chunkCode,
                contentHash: generateHash(chunkCode)
            });
        }
        node.children.forEach(walk);
    }
    walk(tree.rootNode);
    return { chunks, relations };
}

async function _segmentJavaScript(content, filePath) {
    const chunks = [];
    const relations = [];

    try {
        const ast = parse(content, { sourceType: 'module', plugins: ['typescript', 'jsx'], errorRecovery: true });

        const getChunkData = (node) => {
            const chunkName = node.id ? node.id.name : 'anonymous';
            const chunkCode = content.substring(node.start, node.end);
            return { filePath, chunk_name: chunkName, code: chunkCode, contentHash: generateHash(chunkCode) };
        };

        traverse(ast, {
            enter(path) {
                let currentScopeName = 'file';
                const parentFunction = path.findParent((p) => p.isFunctionDeclaration() || p.isClassDeclaration());
                if (parentFunction && parentFunction.node.id) {
                    currentScopeName = parentFunction.node.id.name;
                }

                if (path.isFunctionDeclaration() || path.isClassDeclaration()) {
                    chunks.push({ ...getChunkData(path.node), chunk_type: path.isClassDeclaration() ? 'class' : 'function' });
                }

                if (path.isImportDeclaration()) {
                    const sourceFile = path.node.source.value;
                    relations.push({ from: filePath, to: sourceFile, type: 'IMPORTS' });
                }

                if (path.isCallExpression()) {
                    const calleeName = path.get('callee').toString();
                    relations.push({ from: currentScopeName, to: calleeName, type: 'CALLS' });
                }
            }
        });
    } catch (e) {
        console.error(`Babel parsing error in ${filePath}: ${e.message}`);
    }
    return { chunks, relations };
}

export async function segmentFile(filePath) {
    try {
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = path.extname(filePath);
        let result = { chunks: [], relations: [] };

        if (['.js', '.jsx', '.ts', '.tsx'].includes(extension)) {
            result = await _segmentJavaScript(content, filePath);
        } else if (languageParsers[extension]) {
            result = await _segmentWithTreeSitter(content, filePath, languageParsers[extension]);
        }
        
        // Fallback: if no specific chunks, treat the whole file as one
        if (result.chunks.length === 0) {
            const code = content;
            result.chunks.push({ filePath, chunk_type: 'file', chunk_name: path.basename(filePath), code, contentHash: generateHash(code) });
        }

        return result;
    } catch (error) {
        console.error(`Failed to segment file ${filePath}: ${error.message}`);
        return { chunks: [], relations: [] };
    }
}

--- File: /src/database/postgresConnector.js ---

import knex from 'knex';
import fs from 'fs/promises';
import path from 'path';
import config from '../../knexfile.js';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));

let knexInstance = null;

function getKnex() {
  if (!knexInstance) {
    console.log('Initializing Knex connection...');
    knexInstance = knex(config.development);
  }
  return knexInstance;
}

async function initDb() {
  const db = getKnex();
  try {
    console.log('Checking database connection...');
    await db.raw('SELECT 1+1 AS result');
    console.log('Connection successful.');

    console.log('Applying database schema...');
    // Try full schema first, fallback to simple schema
    try {
      const schemaPath = path.join(__dirname, 'schema.sql');
      const schemaSQL = await fs.readFile(schemaPath, 'utf-8');
      await db.raw(schemaSQL);
      console.log('Full schema with vector extensions applied successfully.');
    } catch (error) {
      console.log('Vector extensions not available, using simplified schema...');
      const simpleSchemaPath = path.join(__dirname, 'schema_simple.sql');
      const simpleSchemaSQL = await fs.readFile(simpleSchemaPath, 'utf-8');
      await db.raw(simpleSchemaSQL);
      console.log('Simplified schema applied successfully.');
    }
  } catch (error) {
    console.error('Error initializing database:', error.message);
    throw error;
  }
}

async function destroyDb() {
  if (knexInstance) {
    console.log('Destroying Knex connection pool...');
    await knexInstance.destroy();
    knexInstance = null;
  }
}

export {
  getKnex,
  initDb,
  destroyDb,
};

--- File: /src/database/schema.sql ---

-- –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
-- (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ Apache AGE —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ PG)
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç)
SELECT create_graph('eck_snapshot_graph');

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞ (—É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding VECTOR(768), -- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è Jina Code v2
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π (—Ä–µ–±–µ—Ä –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

-- –°–æ–∑–¥–∞–µ–º HNSW-–∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX IF NOT EXISTS code_chunks_embedding_idx ON code_chunks USING HNSW (embedding vector_cosine_ops);

--- File: /src/database/schema_simple.sql ---

-- Simplified schema without vector and graph extensions for testing

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding TEXT, -- JSON string representation for now
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

--- File: /src/services/analysisService.js ---

import { pipeline } from '@xenova/transformers';

class AnalysisService {
    static instance = null;
    static modelName = 'Xenova/distilgpt2'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = await pipeline('text-generation', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = null;
        }
    }
}

export async function getCodeSummary(codeChunk) {
    const generator = await AnalysisService.getInstance();

    const prompt = `This code:\n${codeChunk.substring(0, 150)}\nSummary:`;

    const output = await generator(prompt, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true
    });

    const generatedText = output[0].generated_text;
    const summary = generatedText.replace(prompt, '').trim() || 'Auto-generated description';
    return summary.substring(0, 200); // Limit summary length
}

export const releaseModel = AnalysisService.releaseModel;

--- File: /src/services/authService.js ---

import ora from 'ora';
import { execa } from 'execa';

/**
 * Initiates the interactive login flow by spawning 'codex login'.
 * This will open a browser and wait for the user to complete authentication.
 * @returns {Promise<void>}
 */
export async function initiateLogin() {
  const spinner = ora('Authentication required. Please follow the browser instructions.').start();
  try {
    // Run `codex login` interactively, inheriting stdio to show user instructions.
    await execa('codex', ['login'], { stdio: 'inherit' });
    spinner.succeed('Login successful. Retrying original command...');
  } catch (e) {
    spinner.fail('Login process failed or was cancelled.');
    // Re-throw to notify p-retry that the attempt failed.
    throw new Error(`Login failed: ${e.message}`);
  }
}

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';
import pRetry from 'p-retry';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {boolean} continueConversation Whether to continue the last conversation with -c flag.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, continueConversation = false) {
  try {
    // Ensure the log directory exists
    try {
      await import('fs/promises').then(fs => fs.mkdir('./.eck/logs', { recursive: true }));
    } catch (e) {
      console.error(`Failed to create log directory: ${e.message}`);
      // Do not block execution if log dir creation fails, just warn
    }
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }

    return await attemptClaudeExecution(prompt, sessionId);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./.eck/logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./.eck/logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null) {
  const timestamp = new Date().toISOString();
  const logFile = `./.eck/logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./.eck/logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./.eck/logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./.eck/logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null) {
  return new Promise((resolve, reject) => {
    
    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }

    // Always add the skip permissions flag for automation reliability
    args.push('--dangerously-skip-permissions');

    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');
    
    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });
    
    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;
    
    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const ACTIVITY_TIMEOUT = 60000; // 1 minute of inactivity allowed
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log('üíÄ No activity detected for 1 minute, killing process');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${ACTIVITY_TIMEOUT/1000} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

/**
 * Executes a prompt using gemini-cli delegation with retry logic for transient errors.
 * @param {string} prompt The prompt to send to Claude via gemini-cli.
 * @returns {Promise<object>} A promise that resolves with the response from Claude.
 */
export async function askClaude(prompt) {
  return pRetry(async () => {
    try {
      const result = await execa('gemini-cli', ['claude', prompt], {
        timeout: 120000 // 2 minute timeout
      });

      // Parse mcp_feedback if present in prompt
      let mcpFeedback = null;
      try {
        const promptObj = JSON.parse(prompt);
        if (promptObj.payload && promptObj.payload.post_execution_steps && promptObj.payload.post_execution_steps.mcp_feedback) {
          mcpFeedback = promptObj.payload.post_execution_steps.mcp_feedback;

          // Log if errors array is non-empty
          if (mcpFeedback.errors && Array.isArray(mcpFeedback.errors) && mcpFeedback.errors.length > 0) {
            console.warn('MCP feedback contains errors:', mcpFeedback.errors);
          }
        }
      } catch (parseError) {
        // If prompt is not valid JSON or doesn't contain mcp_feedback, continue normally
      }

      return {
        stdout: result.stdout,
        stderr: result.stderr,
        success: true,
        mcp_feedback: mcpFeedback
      };
    } catch (error) {
      // Check if this is a transient error that should be retried
      if (isTransientError(error)) {
        console.log(`Transient error detected, retrying: ${error.message}`);
        throw error; // This will trigger a retry
      }

      // Non-transient errors should not be retried
      console.error(`Non-transient error in askClaude: ${error.message}`);
      return {
        stdout: error.stdout || '',
        stderr: error.stderr || error.message,
        success: false,
        error: error.message
      };
    }
  }, {
    retries: 3,
    minTimeout: 1000,
    maxTimeout: 5000,
    onFailedAttempt: (error) => {
      console.log(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`);
    }
  });
}

/**
 * Checks if an error is transient and should be retried.
 * @param {Error} error The error to check.
 * @returns {boolean} True if the error is transient.
 */
export function isTransientError(error) {
  const errorMessage = (error.message || '').toLowerCase();
  const stderr = (error.stderr || '').toLowerCase();
  const stdout = (error.stdout || '').toLowerCase();
  const allOutput = `${errorMessage} ${stderr} ${stdout}`;

  // Network-related errors
  const networkErrors = [
    'network',
    'timeout',
    'connection',
    'econnreset',
    'enotfound',
    'econnrefused',
    'socket hang up'
  ];

  // Quota/rate limit errors
  const quotaErrors = [
    'quota exceeded',
    'rate limit',
    'too many requests',
    'service unavailable',
    'temporarily unavailable',
    '429',
    '500',
    '502',
    '503',
    '504'
  ];

  const transientPatterns = [...networkErrors, ...quotaErrors];

  return transientPatterns.some(pattern => allOutput.includes(pattern));
}

--- File: /src/services/claudeCliService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { askClaude } from './claudeCliService.js';

// Mock execa
vi.mock('execa', () => ({
  execa: vi.fn()
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', () => ({
  default: async (fn, options) => {
    // For tests, we'll execute the function directly without retries
    return await fn();
  }
}));

describe('claudeCliService', () => {
  let mockExeca;

  beforeEach(async () => {
    const { execa } = await import('execa');
    mockExeca = execa;
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.resetAllMocks();
  });

  describe('askClaude', () => {
    it('should successfully execute gemini-cli claude command', async () => {
      const mockResponse = {
        stdout: '{"result": "test response", "success": true}',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith('gemini-cli', ['claude', 'test prompt'], {
        timeout: 120000
      });
      expect(result).toEqual({
        stdout: mockResponse.stdout,
        stderr: mockResponse.stderr,
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle non-transient errors without retry', async () => {
      const mockError = new Error('EACCES: permission denied');
      mockError.code = 'EACCES';
      mockError.stdout = '';
      mockError.stderr = 'permission denied';

      mockExeca.mockRejectedValue(mockError);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: 'permission denied',
        success: false,
        error: 'EACCES: permission denied'
      });
    });

    it('should identify transient network errors', async () => {
      const mockError = new Error('Connection timeout');
      mockError.stdout = '';
      mockError.stderr = 'network timeout occurred';

      const { isTransientError } = await import('./claudeCliService.js');

      expect(isTransientError(mockError)).toBe(true);
    });


    it('should handle JSON parsing in gemini-cli response', async () => {
      const complexJsonResponse = {
        stdout: JSON.stringify({
          type: 'result',
          data: {
            analysis: 'test analysis',
            metrics: { tokens: 100, cost: 0.05 }
          },
          timestamp: new Date().toISOString()
        }),
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(complexJsonResponse);

      const result = await askClaude('analyze this code');

      expect(result.success).toBe(true);
      expect(result.stdout).toContain('test analysis');
      expect(result.stdout).toContain('tokens');
    });

    it('should handle empty responses gracefully', async () => {
      const mockResponse = {
        stdout: '',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: '',
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle stderr warnings without failing', async () => {
      const mockResponse = {
        stdout: '{"result": "success"}',
        stderr: 'Warning: deprecated feature used',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result.success).toBe(true);
      expect(result.stderr).toContain('deprecated feature');
    });

    it('should respect timeout configuration', async () => {
      mockExeca.mockResolvedValue({
        stdout: 'response',
        stderr: '',
        exitCode: 0
      });

      await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith(
        'gemini-cli',
        ['claude', 'test prompt'],
        { timeout: 120000 }
      );
    });

    it('should parse mcp_feedback from JSON prompt', async () => {
      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithFeedback = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: true,
              errors: [],
              mcp_version: '1.0'
            }
          }
        }
      });

      const result = await askClaude(promptWithFeedback);

      expect(result.mcp_feedback).toEqual({
        success: true,
        errors: [],
        mcp_version: '1.0'
      });
    });

    it('should log warning when mcp_feedback contains errors', async () => {
      const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithErrors = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: false,
              errors: ['Error 1', 'Error 2'],
              mcp_version: '1.0'
            }
          }
        }
      });

      await askClaude(promptWithErrors);

      expect(consoleSpy).toHaveBeenCalledWith('MCP feedback contains errors:', ['Error 1', 'Error 2']);

      consoleSpy.mockRestore();
    });
  });

  describe('transient error detection', () => {
    it('should treat network errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const networkErrors = [
        'network error',
        'timeout',
        'connection refused',
        'ECONNRESET',
        'ENOTFOUND',
        'socket hang up'
      ];

      networkErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should treat quota errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const quotaErrors = [
        'quota exceeded',
        'rate limit',
        'too many requests',
        '429',
        '503'
      ];

      quotaErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should not treat permission errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const permanentErrors = [
        'EACCES: permission denied',
        'Invalid API key',
        'Authentication failed'
      ];

      permanentErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(false);
      });
    });
  });
});

--- File: /src/services/dispatcherService.js ---

import { ask as askGpt } from './gptService.js';
import { executePrompt as askClaude } from './claudeCliService.js';

/**
 * Dispatches an analytical task to the most efficient AI model with a fallback.
 * Priority 1: Codex (GPT) with low reasoning for speed and cost.
 * Priority 2: Claude as a reliable fallback.
 * @param {string} prompt The JSON payload or prompt string for the task.
 * @returns {Promise<object>} The result from the successful AI agent.
 */
export async function dispatchAnalysisTask(prompt) {
  try {
    console.log('üß† Dispatcher: Attempting analysis with Codex (low reasoning)...');
    const gptOptions = {
      model: 'gpt-5-codex',
      reasoning: 'low'
    };
    // The 'ask' function expects payload as first arg, and options as second.
    // Since prompt is a string here, we wrap it in an object for consistency if needed,
    // but for simple prompts it can often be passed directly.
    const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
    return await askGpt(payload, { verbose: false, ...gptOptions });
  } catch (gptError) {
    console.warn(`‚ö†Ô∏è Codex (low reasoning) failed: ${gptError.message}`);
    console.log('üîÑ Failing over to Claude for analysis...');
    try {
      return await askClaude(prompt);
    } catch (claudeError) {
      console.error(`‚ùå Critical Failure: Both Codex and Claude failed for analysis task.`);
      throw new Error(`Primary (Codex) Error: ${gptError.message}\nFallback (Claude) Error: ${claudeError.message}`);
    }
  }
}

--- File: /src/services/embedding.js ---

import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';
import pLimit from 'p-limit';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "embedding-001" });

async function generateEmbedding(text, taskType = 'RETRIEVAL_DOCUMENT') {
  try {
    const result = await model.embedContent({ 
      content: { parts: [{ text }] },
      taskType
    });
    return result.embedding.values;
  } catch (error) {
    console.error('‚ùå Gemini Embedding Error:', error.message);
    throw error;
  }
}

async function generateBatchEmbeddings(segments, taskType = 'RETRIEVAL_DOCUMENT') {
    if (segments.length === 0) return [];

    const BATCH_COUNT_LIMIT = 100;
    const BATCH_SIZE_LIMIT = 3000000; // 3MB for safety
    const allBatches = [];
    let currentBatch = [];
    let currentBatchSize = 0;

    for (const segment of segments) {
        const segmentSize = Buffer.byteLength(segment.content, 'utf8');
        if (segmentSize > BATCH_SIZE_LIMIT) { // Handle single oversized segments
            console.log(chalk.yellow(`  -> Warning: Segment '${segment.name}' in '${segment.filePath}' is oversized and will be truncated.`));
            segment.content = segment.content.substring(0, 20000) + '... [truncated]'; // Truncate oversized segment
        }
        if (currentBatch.length > 0 && (currentBatch.length >= BATCH_COUNT_LIMIT || currentBatchSize + Buffer.byteLength(segment.content, 'utf8') > BATCH_SIZE_LIMIT)) {
            allBatches.push(currentBatch);
            currentBatch = [];
            currentBatchSize = 0;
        }
        currentBatch.push(segment);
        currentBatchSize += Buffer.byteLength(segment.content, 'utf8');
    }
    if (currentBatch.length > 0) {
        allBatches.push(currentBatch);
    }

    console.log(chalk.cyan(`‚è≥ Generating embeddings for ${segments.length} segments, divided into ${allBatches.length} safe chunks...`));

    const limit = pLimit(5); // Set concurrency to 5 parallel requests
    let processedCount = 0;

    const promises = allBatches.map((batch, i) => {
        return limit(async () => {
            const batchSizeKB = (Buffer.byteLength(batch.map(s => s.content).join(''), 'utf8') / 1024).toFixed(2);
            console.log(chalk.blue(`  -> Sending chunk ${i + 1}/${allBatches.length} (${batch.length} segments, ${batchSizeKB} KB)...`));
            try {
                const contents = batch.map(s => ({ parts: [{ text: s.content }] }));
                const result = await model.batchEmbedContents({ 
                    requests: contents.map(content => ({ content, taskType }))
                });
                processedCount++;
                console.log(chalk.green(`  <- Chunk ${i + 1}/${allBatches.length} processed successfully.`));
                return result.embeddings.map(e => e.values);
            } catch (error) {
                console.error(chalk.red(`‚ùå Gemini Batch Embedding Error (Chunk ${i + 1}/${allBatches.length}):`), error.message);
                throw error;
            }
        });
    });

    const chunkResults = await Promise.all(promises);
    const allEmbeddings = chunkResults.flat();

    console.log(chalk.green.bold('‚úÖ Batch embeddings generated successfully.'));
    return allEmbeddings;
}

export const embeddingService = {
  generateEmbedding,
  generateBatchEmbeddings
};

--- File: /src/services/embeddingService.js ---

import { pipeline } from '@xenova/transformers';

class EmbeddingService {
    static instance = null;
    static modelName = 'Xenova/jina-embeddings-v2-base-en'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = await pipeline('feature-extraction', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = null;
            // In Node.js, there's no explicit GPU memory release, 
            // relying on the garbage collector is the standard way.
        }
    }
}

export async function generateEmbedding(code) {
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(code, { pooling: 'mean', normalize: true });
    return Array.from(result.data);
}

export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) return [];
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(texts, { pooling: 'mean', normalize: true });
    
    // Convert tensor to array of arrays
    const embeddings = [];
    for (let i = 0; i < result.dims[0]; i++) {
        const start = i * result.dims[1];
        const end = start + result.dims[1];
        embeddings.push(Array.from(result.data.slice(start, end)));
    }
    return embeddings;
}

export const releaseModel = EmbeddingService.releaseModel;

--- File: /src/services/gptService.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import pRetry from 'p-retry';
import ora from 'ora';
import { loadProjectEckManifest } from '../utils/fileUtils.js';
import { initiateLogin } from './authService.js';
import which from 'which';

const SYSTEM_PROMPT = 'You are a Coder agent. Apply code changes per JSON spec. Respond only in JSON: {success: bool, changes: array, errors: array, post_steps: object}';

class AuthError extends Error {
  constructor(message) {
    super(message);
    this.name = 'AuthError';
  }
}

/**
 * Checks if the codex CLI tool is available in the system's PATH.
 * Throws an error if not found.
 */
async function ensureCodexCliExists() {
  try {
    await which('codex');
  } catch (error) {
    throw new Error('The `codex` CLI tool is not installed or not in your PATH. Please install it from https://github.com/openai/codex to use this command.');
  }
}

/**
 * Delegates an apply_code_changes payload to the codex CLI with auto-login.
 * @param {string|object} payload - JSON string or object payload to forward to the agent.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} [options]
 * @returns {Promise<object>}
 */
export async function ask(payload, options = {}) {
  const { verbose = false, model = 'gpt-5-codex', reasoning = 'high' } = options;
  await ensureCodexCliExists();

  const run = async () => {
    const spinner = verbose ? null : ora('Sending payload to Codex agent...').start();
    try {
      const payloadObject = await parsePayload(payload);
      const manifest = await loadProjectEckManifest(process.cwd());
      const userPrompt = buildUserPrompt(payloadObject, manifest);
      const promptInput = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const args = [
        'exec',
        // Use full-auto mode to prevent interactive prompts from the agent,
        // as this service is designed for non-interactive delegation.
        '--full-auto',
        '--model', model,
        '-c', `model_reasoning_effort=${reasoning}`
      ];

      debug(verbose, `Executing: codex ${args.join(' ')} <stdin>`);

      const cliResult = await execa('codex', args, {
        cwd: process.cwd(),
        timeout: 300000, // 5-minute timeout
        input: promptInput // Stream large prompts via stdin to avoid argv limits
      });

      const output = cliResult?.stdout?.trim();
      if (!output) {
        throw new Error('codex CLI returned empty response');
      }

      const parsed = extractFinalJson(output);
      if (parsed) {
        if (parsed.post_steps || parsed.post_execution_steps) {
          const postSteps = parsed.post_steps || parsed.post_execution_steps;
          await handlePostExecutionSteps(postSteps, payloadObject);
          parsed.mcp_feedback = postSteps?.mcp_feedback || null;
        }
        spinner?.succeed('Codex agent completed the task.');
        return parsed;
      }

      // If parsing fails, surface the raw response text for upstream handling.
      spinner?.succeed('Codex agent completed the task.');
      return { success: true, changes: [], errors: [], response_text: output };

    } catch (error) {
        spinner?.fail('Codex execution failed.');
        handleCliError(error); // This will throw a specific error type
    }
  };

  return pRetry(run, {
    retries: 1, // Only retry once after a successful login
    minTimeout: 0,
    onFailedAttempt: async (error) => {
      if (error.name === 'AuthError') {
        await initiateLogin();
      } else {
        throw error; // Don't retry for other errors, fail immediately
      }
    }
  });
}


async function parsePayload(payload) {
  if (typeof payload === 'string') {
    try {
      return JSON.parse(payload);
    } catch (error) {
      throw new Error(`Failed to parse payload JSON: ${error.message}`);
    }
  }
  if (typeof payload === 'object' && payload !== null) {
    return payload;
  }
  throw new Error('Invalid payload type. Expected JSON string or object.');
}

function buildUserPrompt(payloadObject, manifest) {
  const payloadString = JSON.stringify(payloadObject);
  if (!manifest) {
    return payloadString;
  }

  const sections = [];
  if (manifest.context) {
    sections.push('## .eck Context\n' + manifest.context);
  }
  if (manifest.operations) {
    sections.push('## .eck Operations\n' + manifest.operations);
  }
  if (manifest.journal) {
    sections.push('## .eck Journal\n' + manifest.journal);
  }
  if (manifest.environment && Object.keys(manifest.environment).length > 0) {
    sections.push('## .eck Environment\n' + JSON.stringify(manifest.environment, null, 2));
  }

  if (sections.length === 0) {
    return payloadString;
  }

  return `${payloadString}\n\n# Project Context\n${sections.join('\n\n')}`;
}

function debug(verbose, message) {
  if (verbose) {
    console.log(`[ask-gpt] ${message}`);
  }
}

function extractFinalJson(text) {
  const trimmed = text?.trim();
  if (!trimmed) {
    return null;
  }

  try {
    return JSON.parse(trimmed);
  } catch (error) {
    // Continue with fallback parsing when logs precede the JSON payload.
  }

  const fencedMatch = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fencedContent = fencedMatch[1].trim();
    try {
      return JSON.parse(fencedContent);
    } catch (error) {
      // Ignore and fall through to final brace search.
    }
  }

  const lastBraceIndex = trimmed.lastIndexOf('{');
  if (lastBraceIndex === -1) {
    return null;
  }

  const jsonCandidate = trimmed.slice(lastBraceIndex);
  try {
    return JSON.parse(jsonCandidate);
  } catch (error) {
    return null;
  }
}

function handleCliError(error) {
  const combined = `${error?.message || ''} ${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
  // Check for text that `codex` outputs when auth is missing.
  if (combined.includes('authentication is required') || combined.includes('please run `codex login`')) {
    const authError = new Error('Codex authentication is required. Attempting to log in.');
    authError.name = 'AuthError';
    throw authError;
  }

  throw new Error(`codex CLI failed: ${error.stderr || error.message}`);
}

async function handlePostExecutionSteps(postSteps, payloadObject) {
  if (!postSteps || typeof postSteps !== 'object') {
    return;
  }

  if (postSteps.journal_entry) {
    await applyJournalEntry(postSteps.journal_entry, payloadObject);
  }

  if (postSteps.mcp_feedback) {
    logMcpFeedback(postSteps.mcp_feedback);
  }
}

async function applyJournalEntry(entry, payloadObject) {
  const journalEntry = normalizeJournalEntry(entry);
  const journalPath = path.join(process.cwd(), '.eck', 'JOURNAL.md');

  await fs.mkdir(path.dirname(journalPath), { recursive: true });

  let existing = '';
  try {
    existing = await fs.readFile(journalPath, 'utf-8');
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw new Error(`Failed to read JOURNAL.md: ${error.message}`);
    }
  }

  const taskId = payloadObject?.task_id || payloadObject?.payload?.task_id || journalEntry.task_id || 'ask-gpt';
  const isoDate = new Date().toISOString();

  const frontmatter = [
    '---',
    `task_id: ${taskId}`,
    `date: ${isoDate}`,
    `type: ${journalEntry.type}`,
    `scope: ${journalEntry.scope}`,
    '---',
    ''
  ].join('\n');

  const summary = journalEntry.summary ? `## ${journalEntry.summary}\n` : '';
  const details = journalEntry.details ? `${journalEntry.details}\n` : '';

  const entryBlock = `${frontmatter}${summary ? `${summary}\n` : ''}${details}\n`;

  const existingTrimmed = existing ? existing.replace(/^\n+/, '') : '';
  const newContent = `${entryBlock}${existingTrimmed}`.replace(/\n{3,}/g, '\n\n');

  await fs.writeFile(journalPath, newContent.trimEnd() + '\n');

  await stageJournal(journalPath);
  await commitJournal(journalEntry);
}

function normalizeJournalEntry(entry) {
  return {
    type: entry.type || 'chore',
    scope: entry.scope || 'journal',
    summary: entry.summary || 'Update journal entry',
    details: entry.details || ''
  };
}

async function stageJournal(journalPath) {
  const relativePath = path.relative(process.cwd(), journalPath);
  try {
    await execa('git', ['add', relativePath], { cwd: process.cwd() });
  } catch (error) {
    throw new Error(`Failed to stage journal entry: ${error.message}`);
  }
}

async function commitJournal(entry) {
  const scopePart = entry.scope ? `(${entry.scope})` : '';
  const summary = (entry.summary || 'Update journal entry').replace(/\s+/g, ' ').trim();
  const commitMessage = `${entry.type}${scopePart}: ${summary}`;

  try {
    await execa('git', ['commit', '-m', commitMessage], { cwd: process.cwd() });
  } catch (error) {
    const text = `${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
    if (text.includes('nothing to commit')) {
      console.warn('Journal entry already committed or no changes to commit.');
      return;
    }
    throw new Error(`Failed to commit journal entry: ${error.message}`);
  }
}

function logMcpFeedback(feedback) {
  if (!feedback) {
    return;
  }

  const errors = Array.isArray(feedback.errors) ? feedback.errors : [];
  if (!feedback.success || errors.length > 0) {
    console.warn('MCP feedback indicates issues:', errors.length > 0 ? errors : feedback);
  } else {
    console.log('MCP feedback:', feedback);
  }
}


--- File: /src/services/gptService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

// Mock execa and which
vi.mock('execa', () => ({ execa: vi.fn() }));
vi.mock('which', () => ({ default: vi.fn() }));

// Mock fs/promises for journal entries
const mkdirMock = vi.fn();
const readFileMock = vi.fn();
const writeFileMock = vi.fn();
const loadProjectEckManifestMock = vi.fn();
vi.mock('fs/promises', () => ({
  mkdir: mkdirMock,
  readFile: readFileMock,
  writeFile: writeFileMock
}));
vi.mock('../utils/fileUtils.js', () => ({
  loadProjectEckManifest: loadProjectEckManifestMock
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', async (importOriginal) => {
  const actual = await importOriginal();
  return {
    ...actual,
    default: vi.fn(async (fn, options) => {
      try {
        return await fn();
      } catch (error) {
        if (options.onFailedAttempt) {
          await options.onFailedAttempt(error);
          // In a real scenario, p-retry would re-run fn. For testing, we simulate one retry.
          if (error.name === 'AuthError') {
             return await fn();
          }
        }
        throw error;
      }
    })
  };
});

// Mock the authService
vi.mock('./authService.js', () => ({
  initiateLogin: vi.fn()
}));

describe('gptService with codex CLI', () => {
  let ask;
  let execaMock;
  let whichMock;
  let initiateLoginMock;

  beforeEach(async () => {
    vi.clearAllMocks();

    ({ execa: execaMock } = await import('execa'));
    const which = (await import('which')).default;
    whichMock = which;
    ({ initiateLogin: initiateLoginMock } = await import('./authService.js'));
    ({ ask } = await import('./gptService.js'));

    whichMock.mockResolvedValue('/usr/bin/codex');
    loadProjectEckManifestMock.mockResolvedValue(null);
  });

  it('should call codex CLI with correct arguments and parse final JSON from noisy output', async () => {
    const codexLogs = '[2025-10-06 20:04:22] OpenAI Codex v0.42.0\nSome setup log...\n\n{"success": true, "changes": ["change1"], "errors": []}';
    execaMock.mockResolvedValue({ stdout: codexLogs });

    const payload = { objective: 'Test' };
    const result = await ask(payload);

    expect(result).toEqual({ success: true, changes: ['change1'], errors: [] });
    expect(execaMock).toHaveBeenCalledWith('codex', expect.arrayContaining(['exec', '--full-auto', '--model']), expect.any(Object));
    const [, , options] = execaMock.mock.calls[0];
    expect(options.input).toContain(JSON.stringify(payload));
  });

  it('should trigger login flow on authentication error and retry', async () => {
    const authError = new Error('Authentication is required. Please run `codex login`.');
    authError.name = 'AuthError'; // Custom error name to trigger retry
    authError.stderr = 'Authentication is required. Please run `codex login`.';

    const successResponse = {
      id: 'task2',
      msg: {
        type: 'task_complete',
        last_agent_message: '{"success": true}'
      }
    };

    // First call fails, second call (retry) succeeds
    execaMock
      .mockRejectedValueOnce(authError)
      .mockResolvedValueOnce({ stdout: JSON.stringify(successResponse) });

    initiateLoginMock.mockResolvedValue();

    const result = await ask({ objective: 'Retry test' });

    expect(result).toEqual({ success: true });
    expect(initiateLoginMock).toHaveBeenCalledTimes(1);
    expect(execaMock).toHaveBeenCalledTimes(2); // Initial call + retry
  });

  it('should throw if codex CLI is not found', async () => {
    whichMock.mockRejectedValue(new Error('not found'));
    await expect(ask({})).rejects.toThrow('The `codex` CLI tool is not installed');
  });

  it('should throw non-auth errors immediately without retry', async () => {
    const otherError = new Error('Some other CLI error');
    otherError.stderr = 'Something else went wrong';
    execaMock.mockRejectedValueOnce(otherError);

    await expect(ask({})).rejects.toThrow('codex CLI failed: Something else went wrong');
    expect(initiateLoginMock).not.toHaveBeenCalled();
  });
});


--- File: /src/templates/agent-prompt.template.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

--- File: /src/templates/architect-prompt.template.md ---

# AI Architect Instructions

You are an autonomous AI Architect. Your primary goal is to develop and evolve a software project by planning high-level architecture and delegating implementation tasks to an execution agent named Claude.

## Core Workflow: The Thought-Tool-Observation Loop

Your entire operational process follows a strict loop:
1.  **Thought:** Analyze the user's request, the current state of the project, and previous observations. Formulate a plan and decide on the next immediate action. You must explain your reasoning and your chosen action in plain text.
2.  **Tool:** Immediately after your thought process, you MUST issue a command to either the local `eck-snapshot` environment or the `claude_code_agent`.
3.  **Observation:** After issuing a command, you MUST STOP and wait for an `Observation:` message from the system, which will contain the result of your command. Do not proceed until you receive it.

## Commanding the Execution Agent (Claude)

To delegate any coding task (writing, editing, testing, refactoring), you MUST generate a JSON command block for the `claude_code_agent`. This is your primary method of modifying the codebase.

**JSON Command Format:**
```json
{
  "target_agent": "claude_code_agent",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "A brief, clear task description for Claude.",
    "context": "Explain why this change is needed and any relevant architectural context.",
    "files_to_modify": [
      {
        "path": "exact/path/to/file.js",
        "action": "add | modify | replace | delete",
        "location": "line numbers, function name, or a unique search pattern",
        "details": "Precise, step-by-step instructions for Claude to implement."
      }
    ]
  }
}
```

## Interacting with the Local Environment

To understand the project state, you can command the `eck-snapshot` tool directly. Use this for discovery, analysis, and managing project context.

**Tool Command Format:** `[tool_code: eck-snapshot <command> <options>]`

**Available Commands:**
- `eck-snapshot snapshot`: To create a new snapshot of the current state.
- `eck-snapshot query "<question>"`: To search the codebase.
- `eck-snapshot detect`: To analyze the project structure.
- `eck-snapshot restore <snapshot_file> --include ...`: To view specific files from a snapshot.

## Final Mandate

Your existence is defined by this loop. Think, act by issuing a tool command, and then wait for the observation. This is the only way you can make progress.

--- File: /src/templates/envScanRequest.md ---

Run this command and return the output:
```
node -e "console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))"
```

--- File: /src/templates/gitWorkflow.md ---

## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

--- File: /src/templates/multiAgent.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** {{stats.includedFiles}}
- **Total Files in Repo:** {{stats.totalFiles}}

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

{{projectOverview}}

{{eckManifestSection}}

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (`gemini_wsl`), who has a detailed (`_ja.md`) snapshot and the ability to command a **Coder** agent (`claude`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (`gemini_wsl`):** Receives strategic tasks, analyzes the `_ja.md` snapshot, breaks the task down, and commands the Coder.
  - **Coder (`claude`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while `gemini_wsl` can use its own tools for analysis, validation, and running shell commands.

### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`) - LOW-LEVEL EXECUTION:**
Use `apply_code_changes` for simple, direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**2. For Junior Architects (`gemini_wsl`) - HIGH-LEVEL DELEGATION:**
Use `execute_strategic_task` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

```json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to `routes/api.js`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

{{agentDefinitions}}



--- File: /src/templates/vectorMode.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: "{{userQuery}}"

* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.
* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** {{repoName}}
- **User Query:** "{{userQuery}}"
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

{{multiAgentSection}}

---

--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig, getAllProfiles } from '../config.js';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Helper function to read a template file or return the string if it's not a path
    const loadTemplate = async (templatePathOrString) => {
      if (templatePathOrString && (templatePathOrString.endsWith('.md') || templatePathOrString.endsWith('.txt'))) {
        try {
          // Resolve path relative to the project root. __dirname is src/utils.
          const resolvedPath = path.join(__dirname, '..', '..', templatePathOrString);
          return await fs.readFile(resolvedPath, 'utf-8');
        } catch (e) {
          return `ERROR: FAILED TO LOAD TEMPLATE ${templatePathOrString}: ${e.message}`;
        }
      }
      return templatePathOrString; // Fallback for old-style inline strings or errors
    };

    // P1 Bug Fix: Normalize manifest structure as per Consilium report
    function normalizeManifest(raw) {
      if (!raw) return null;
      const out = {};
      // Handle `setup.json` structure (e.g., `projectContext.name`)
      if (raw.projectContext) {
        out.context = raw.projectContext.description || JSON.stringify(raw.projectContext, null, 2);
        out.operations = raw.operations || raw.projectContext.operations || ''; // Assuming .eck/OPERATIONS.md is separate
        out.journal = raw.journal || raw.projectContext.journal || ''; // Assuming .eck/JOURNAL.md is separate
        out.environment = raw.environment || raw.projectContext.environment || {}; // Assuming .eck/ENVIRONMENT.md is separate
      } else {
        // Handle direct .eck file structure (e.g., raw.context from CONTEXT.md)
        out.context = raw.context || '';
        out.operations = raw.operations || '';
        out.journal = raw.journal || '';
        out.environment = raw.environment || {};
      }
      // Add fallback text if still empty
      if (!out.context) out.context = 'No project context provided.';
      if (!out.operations) out.operations = 'No operations guide provided.';
      if (!out.journal) out.journal = 'No journal entries found.';

      return out;
    }

    // --- Build common context sections --- 
    const projectOverview = `### PROJECT OVERVIEW
- **Project:** ${context.repoName || 'Unknown'}
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.
`;
    const normalizedEck = normalizeManifest(context.eckManifest);
    let eckManifestSection = '';
    if (normalizedEck) {
      eckManifestSection = buildEckManifestSection(normalizedEck);
    } else {
      eckManifestSection = '### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nWARNING: .eck manifest was not found or was empty.\n';
    }
    // --- End context building ---


    // Check if agent mode is enabled
    if (context.options && context.options.agent) {
      const agentPromptTemplate = await loadTemplate(promptTemplates.agent);

      const agentHeader = `${agentPromptTemplate}

${projectOverview}
${eckManifestSection}
---

## Project Snapshot Information

- **Project**: ${context.repoName || 'Unknown'}
- **Timestamp**: ${new Date().toISOString()}
- **Files Included**: ${context.stats ? context.stats.includedFiles : 'Unknown'}
- **Total Files in Repo**: ${context.stats ? context.stats.totalFiles : 'Unknown'}

---

`;
      return agentHeader;
    }

    // --- This is the main/Senior Architect prompt logic --- 
    let template;
    if (context.mode === 'vector') {
      template = await loadTemplate(promptTemplates.vectorMode);
      // Inject context for vector mode
      template = template.replace('{{multiAgentSection}}', `
${projectOverview}
${eckManifestSection}
`); 
    } else {
      template = await loadTemplate(promptTemplates.multiAgent);
      // --- INJECT DYNAMIC CONTEXT --- 
      template = template.replace('{{projectOverview}}', projectOverview);
      template = template.replace('{{eckManifestSection}}', eckManifestSection);
      // --- END INJECT --- 
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions
    };

    let renderedTemplate = render(template, data);
    
    // Inject dynamic profile context if a profile is active
    if (context.options && context.options.profile && context.repoPath) {
      let metadataHeader = '\n\n## Partial Snapshot Context\n';
      metadataHeader += `- **Profile(s) Active:** ${context.options.profile}\n`;
      try {
          const allProfiles = await getAllProfiles(context.repoPath);
          const activeProfileNames = context.options.profile.split(',').map(p => p.trim().replace(/^-/, ''));
          const allProfileNames = Object.keys(allProfiles).filter(p => !activeProfileNames.includes(p));
          if (allProfileNames.length > 0) {
               metadataHeader += `- **Other Available Profiles:** ${allProfileNames.join(', ')}\n`;
          }
      } catch (e) { /* fail silently on metadata generation */ }
      
      const insertMarker = "### HIERARCHICAL AGENT WORKFLOW"; // Use our new marker
      renderedTemplate = renderedTemplate.replace(insertMarker, metadataHeader + '\n' + insertMarker);
    }

    return renderedTemplate;

  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header', error.message);
    return `# Snapshot for ${context.repoName || 'Project'}

Generated: ${new Date().toISOString()}

---

`;
  }
}

--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { dispatchAnalysisTask } from '../services/dispatcherService.js';
import { getProfile } from '../config.js';
import micromatch from 'micromatch';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = '.eck/';
  const comment = '# Added by eck-snapshot to ignore metadata directory';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

// Helper function to determine if a string is a glob pattern
function isGlob(str) {
  return str.includes('*') || str.includes('?') || str.includes('{');
}

/**
 * Applies advanced profile filtering (multi-profile, exclusion, and ad-hoc globs) to a file list.
 */
export async function applyProfileFilter(allFiles, profileString, repoPath) {
  const profileParts = profileString.split(',').map(p => p.trim()).filter(Boolean);
  
  const includeGlobs = [];
  const excludeGlobs = [];
  const includeNames = [];
  const excludeNames = [];

  // Step 1: Differentiate between profile names and ad-hoc glob patterns
  for (const part of profileParts) {
    const isNegative = part.startsWith('-');
    const pattern = isNegative ? part.substring(1) : part;

    if (isGlob(pattern)) {
      if (isNegative) {
        excludeGlobs.push(pattern);
      } else {
        includeGlobs.push(pattern);
      }
    } else {
      if (isNegative) {
        excludeNames.push(pattern);
      } else {
        includeNames.push(pattern);
      }
    }
  }

  let workingFiles = [];
  let finalIncludes = [...includeGlobs];
  let finalExcludes = [...excludeGlobs];

  // Step 2: Load patterns from specified profile names
  const allProfileNames = [...new Set([...includeNames, ...excludeNames])];
  const profiles = new Map();
  for (const name of allProfileNames) {
    const profile = await getProfile(name, repoPath);
    if (profile) {
      profiles.set(name, profile);
    } else {
      // This is an ad-hoc glob, not a profile, so no warning is needed.
      if (!isGlob(name)) {
        console.warn(`‚ö†Ô∏è Warning: Profile '${name}' not found and will be skipped.`);
      }
    }
  }

  for (const name of includeNames) {
    if (profiles.has(name)) {
      finalIncludes.push(...(profiles.get(name).include || []));
      finalExcludes.push(...(profiles.get(name).exclude || []));
    }
  }
  for (const name of excludeNames) {
    if (profiles.has(name)) {
      finalExcludes.push(...(profiles.get(name).include || []));
    }
  }
  
  // Step 3: Apply the filtering logic
  if (finalIncludes.length > 0) {
    workingFiles = micromatch(allFiles, finalIncludes);
  } else if (includeNames.length > 0 && includeGlobs.length === 0) {
    workingFiles = [];
  } else {
    workingFiles = allFiles;
  }

  if (finalExcludes.length > 0) {
    workingFiles = micromatch.not(workingFiles, finalExcludes);
  }

  return workingFiles;
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');
  
  try {
    // Check if .eck directory already exists and has all required files
    let needsInitialization = false;
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory exists, check if all required files are present
        const requiredFiles = ['ENVIRONMENT.md', 'CONTEXT.md', 'OPERATIONS.md', 'JOURNAL.md'];
        for (const fileName of requiredFiles) {
          try {
            await fs.stat(path.join(eckDir, fileName));
          } catch (error) {
            console.log(`   ‚ÑπÔ∏è Missing ${fileName}, initialization needed`);
            needsInitialization = true;
            break;
          }
        }
        if (!needsInitialization) {
          // All files exist, no need to initialize
          return;
        }
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
      needsInitialization = true;
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');

    // --- NEW HYBRID LOGIC --- 
    // 1. Run static analysis first to gather facts.
    let staticFacts = {};
    try {
      staticFacts = await detectProjectType(projectPath);
      console.log(`   üîç Static analysis complete. Detected type: ${staticFacts.type}`);
    } catch (e) {
      console.warn(`   ‚ö†Ô∏è Static project detection failed: ${e.message}. Proceeding with generic prompts.`);
    }

    // Prevent AI hallucination by removing low-confidence "other possibilities"
    if (staticFacts && staticFacts.allDetections) {
      delete staticFacts.allDetections;
    }
    
    const staticFactsJson = JSON.stringify(staticFacts, null, 2);
    // --- END NEW LOGIC ---
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'ENVIRONMENT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw YAML key-value content for an .eck/ENVIRONMENT.md file. Only include detected facts. DO NOT add any keys that are not present in the facts. DO NOT add conversational text or markdown wrappers. Your response MUST start directly with a YAML key (e.g., 'project_type: ...').`,
        content: `# This file is for environment overrides. Add agent-specific settings here.\nagent_id: local_dev\n` // Simple static fallback
      },
      {
        name: 'CONTEXT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/CONTEXT.md file. Use the facts to write ## Description, ## Architecture, and ## Key Technologies. DO NOT add conversational text (like "Here is the file..."). Your response MUST start *directly* with the '# Project Overview' heading.`,
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: `Given these static project analysis facts (especially package.json scripts):
${staticFactsJson}

Generate the raw Markdown content ONLY for a .eck/OPERATIONS.md file. DO NOT add conversational text. Your response MUST start *directly* with the '# Common Operations' heading. List commands for ## Development Setup, ## Running the Project, and ## Testing.`,
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/ROADMAP.md file. DO NOT add conversational text. Start *directly* with '# Project Roadmap'. Propose 1-2 *plausible* placeholder items for ## Current Sprint/Phase and ## Next Phase based on the project type.`,
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        prompt: `Generate the raw Markdown content ONLY for a .eck/TECH_DEBT.md file. DO NOT add conversational text. Start *directly* with '# Technical Debt'. Propose 1-2 *common* placeholder items for ## Code Quality Issues and ## Refactoring Opportunities.`,
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file (only if it doesn't exist)
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      
      // Skip if file already exists
      try {
        await fs.stat(filePath);
        console.log(`   ‚úÖ ${file.name} already exists, skipping`);
        continue;
      } catch (error) {
        // File doesn't exist, create it
      }
      
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For files with a prompt, try to dynamically generate
      if (file.prompt) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponseObject = await dispatchAnalysisTask(file.prompt); // Use the prompt
          const rawText = aiResponseObject.result || aiResponseObject.response_text; // Handle both Claude and GPT responses
          
          if (!rawText || typeof rawText.replace !== 'function') {
             throw new Error(`AI returned invalid content type: ${typeof rawText}`);
          }

          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = rawText.replace(/^```(markdown|yaml)?\n|```$/g, '').trim();
          
          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}


--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));

  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }

  const best = detections[0];

  // Special handling for mixed monorepos
  const isLikelyMonorepo = detections.length > 1 && detections.some(d => d.score >= 40);

  if (isLikelyMonorepo) {
    // If we have multiple strong detections, prefer the highest priority with substantial evidence
    const strongDetections = detections.filter(d => d.score >= 40);
    if (strongDetections.length > 1) {
      const primaryType = strongDetections[0].type;
      return {
        type: primaryType,
        confidence: Math.min(strongDetections[0].score / 100, 1.0),
        details: {
          ...strongDetections[0].details,
          isMonorepo: true,
          additionalTypes: strongDetections.slice(1).map(d => d.type)
        },
        allDetections: detections
      };
    }
  }

  // Boost confidence for strong workspace indicators
  if (best.details && (best.details.isWorkspace || best.details.workspaceSize)) {
    const boostedScore = best.score + 20; // Bonus for workspace structure
    return {
      type: best.type,
      confidence: Math.min(boostedScore / 100, 1.0),
      details: best.details,
      allDetections: detections
    };
  }

  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;

  // Check for required files (check both root and common subdirectories)
  if (pattern.files) {
    for (const file of pattern.files) {
      // Check in root directory first
      const rootExists = await fileExists(path.join(projectPath, file));
      if (rootExists) {
        score += 25; // Each required file adds points
      } else {
        // For Cargo.toml and other project files, also check common subdirectory patterns
        const commonSubdirs = ['src', 'lib', 'app', 'core', 'backend', 'frontend'];
        // Add project-type specific subdirectories
        if (file === 'Cargo.toml') {
          commonSubdirs.push('codex-rs', 'rust', 'server', 'api');
        }
        if (file === 'package.json') {
          commonSubdirs.push('codex-cli', 'cli', 'client', 'web', 'ui');
        }

        for (const subdir of commonSubdirs) {
          const subdirExists = await fileExists(path.join(projectPath, subdir, file));
          if (subdirExists) {
            score += 20; // Slightly lower score for subdirectory finds
            break; // Only count once per file type
          }
        }
      }
    }
  }

  // Check for required directories (check both root and one level deep)
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const rootExists = await directoryExists(path.join(projectPath, dir));
      if (rootExists) {
        score += 20; // Each required directory adds points
      } else {
        // Check in common project subdirectories
        const projectSubdirs = ['codex-rs', 'codex-cli', 'src', 'lib', 'app'];
        for (const projDir of projectSubdirs) {
          const subdirExists = await directoryExists(path.join(projectPath, projDir, dir));
          if (subdirExists) {
            score += 15; // Lower score for nested directory finds
            break;
          }
        }
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names (more precise matching)
        const foundInDeps = Object.keys(allDeps).some(dep => dep === patternText || dep.startsWith(patternText + '/'));
        // Only check for exact matches in keywords array, not description (too broad)
        const foundInKeywords = packageJson.keywords && Array.isArray(packageJson.keywords)
          ? packageJson.keywords.some(keyword => keyword.toLowerCase() === patternText.toLowerCase())
          : false;
        
        if (foundInDeps || foundInKeywords) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };

  try {
    // Check both root and common subdirectories for Cargo.toml
    let cargoPath = path.join(projectPath, 'Cargo.toml');
    let cargoContent = null;

    if (await fileExists(cargoPath)) {
      cargoContent = await fs.readFile(cargoPath, 'utf-8');
    } else {
      // Check common Rust project subdirectories
      const rustSubdirs = ['codex-rs', 'rust', 'src', 'core', 'server'];
      for (const subdir of rustSubdirs) {
        const subdirCargoPath = path.join(projectPath, subdir, 'Cargo.toml');
        if (await fileExists(subdirCargoPath)) {
          cargoPath = subdirCargoPath;
          cargoContent = await fs.readFile(subdirCargoPath, 'utf-8');
          details.primaryLocation = subdir;
          break;
        }
      }
    }

    if (!cargoContent) {
      return details;
    }

    const nameMatch = cargoContent.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = cargoContent.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = cargoContent.match(/edition\s*=\s*"([^"]+)"/);

    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];

    // Check if it's a workspace
    if (cargoContent.includes('[workspace]')) {
      details.isWorkspace = true;

      // Count workspace members
      const workspaceMatch = cargoContent.match(/members\s*=\s*\[([\s\S]*?)\]/);
      if (workspaceMatch) {
        const members = workspaceMatch[1].split(',').map(m => m.trim().replace(/"/g, '')).filter(m => m);
        details.workspaceMembers = members.length;
      }
    }

    // Check for multiple Cargo.toml files (indicates workspace structure)
    if (details.primaryLocation) {
      const subdirPath = path.join(projectPath, details.primaryLocation);
      try {
        const subdirs = await fs.readdir(subdirPath, { withFileTypes: true });
        let cargoCount = 0;
        for (const entry of subdirs) {
          if (entry.isDirectory()) {
            const memberCargoPath = path.join(subdirPath, entry.name, 'Cargo.toml');
            if (await fileExists(memberCargoPath)) {
              cargoCount++;
            }
          }
        }
        if (cargoCount > 3) { // If many workspace members, this is definitely a Rust project
          details.workspaceSize = 'large';
        }
      } catch (error) {
        // Ignore
      }
    }

  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }

  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'c': [0, 0.23, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    const parsedData = JSON.parse(data);
    // Ensure the structure is complete by merging with defaults
    return {
        coefficients: { ...DEFAULT_COEFFICIENTS, ...parsedData.coefficients },
        trainingPoints: parsedData.trainingPoints || {}
    };
  } catch (error) {
    // If file doesn't exist or is malformed, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];

  if (!points || points.length === 0) {
    // No points, nothing to do.
    return;
  }

  if (points.length === 1) {
    // With one point, use a direct ratio for the linear coefficient.
    const point = points[0];
    if (point.fileSizeInBytes > 0) { // Avoid division by zero
        const ratio = point.actualTokens / point.fileSizeInBytes;
        data.coefficients[projectType] = [
            0, // intercept
            Math.max(0, ratio), // linear term (slope)
            0, 0 // quadratic, cubic
        ];
    }
    return;
  }

  // Use linear regression for 2 or more points.
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;

  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;

    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }

  const denominator = (n * sumX2 - sumX * sumX);
  if (denominator === 0) return; // Avoid division by zero, can't calculate slope

  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / denominator;
  const intercept = (sumY - slope * sumX) / n;

  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

--- File: /test_knexfile.js ---

module.exports = {
  development: {
    client: 'sqlite3',
    connection: {
      filename: './dev.sqlite3'
    }
  },
  
  staging: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  },
  
  production: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  }
};


--- File: /test_snapshot.md ---

# Test Snapshot

## Project Overview
This is a test project for demonstrating the Gemini session API integration.

## Files
- `index.js` - Main entry point
- `src/` - Source code directory

## Instructions
You are an AI assistant helping with this test project. Please respond to user queries about the codebase.

--- File: /vitest.config.js ---

import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    testMatch: ['**/*.{test,spec}.js'],
    coverage: {
      reporter: ['text', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        '**/*.config.js',
        '**/*.test.js',
        '**/*.spec.js'
      ]
    }
  }
});



--- File: /test_prune.md/eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_ja.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

### PROJECT OVERVIEW
- **Project:** eckSnapshot
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.


## Project-Specific Manifest (.eck Directory)

This project includes a `.eck` directory with specific context and configuration:

### Project Context

# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.

### Operations Guide

# Common Operations

## Development Setup
```bash
# Setup commands
npm install
# or yarn install
```

## Running the Project
```bash
# Development mode
npm run dev

# Production build
npm run build
```

## Testing
```bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
```

## Deployment
```bash
# Deployment commands
npm run deploy
```

## Troubleshooting
Common issues and their solutions.

### Development Journal

---
task_id: fix-profile-detect-and-add-index-viewer-v1
date: 2025-10-10
type: fix
scope: cli
---

# Fix JSON parsing in profile-detect

- Modified extractJson to be more robust against AI log wrappers
- Finds first '{' and last '}' to extract JSON from surrounding text
- Added /managed_components/ to .gitignore
- Implemented new index-view command to inspect code chunks database
- This resolves the crash when running the profile-detect command

---
task_id: feat-c-language-support-v1
date: 2025-10-10
type: feat
scope: core
---

# Add support for C language projects

- Added C project detection (Makefile, CMakeLists.txt, *.c, *.h)
- Added C-specific file filtering for compiled objects (.o, .a, .so)
- Installed tree-sitter-c@0.21.4 parser
- Integrated C parser into segmenter for functions, structs, enums, unions
- Added token estimation coefficients for C (0.23 ratio)

---
task_id: refactor-claude-skip-permissions-default-v1
date: 2025-09-28T14:30:00.000Z
type: refactor
scope: claude-cli
---
## Force --dangerously-skip-permissions for all Claude calls

- Removed the user-facing '--dangerously-skip-permissions' option from 'ask-claude' and 'ask-claude-session' commands.
- Hardcoded the flag in the service layer to ensure all calls to 'claude-code' are non-interactive by default.
- This change improves the reliability of the tool in automated workflows by preventing it from hanging on permission prompts.

---
task_id: refactor-gpt-service-to-codex-cli-v1
date: 2025-09-28T14:00:00.000Z
type: refactor
scope: services
---
## Switch ask-gpt from chatgpt-cli to official codex CLI

- Replaced `npx chatgpt` calls with the official `codex` CLI, using the `exec --full-auto` command for machine-readable output.
- Implemented an automatic login flow that detects authentication errors and triggers the interactive `codex login` command.
- Created a new `authService.js` to handle the login initiation.
- Removed the `open` package dependency as it is no longer needed.
- Updated tests in `gptService.test.js` to mock the new `codex` command flow.
- Added comprehensive documentation in README.md for both ChatGPT and Claude Code integration.
- Enhanced CLI help with detailed examples and authentication instructions.

---
task_id: gpt-test-1
date: 2025-09-28T09:24:01.314Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:41.532Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:23:33.203Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:43.135Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:27.678Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: gpt-test-1
date: 2025-09-28T09:22:04.054Z
type: feat
scope: test
---
## GPT integration test

- Delegated simple change
- Committed journal

---
task_id: configure-claude-auto-accept-v1
date: 2025-09-14T23:34:21Z
type: feat
scope: workflow
---

## Enable and document claude-code auto-accept mode

Created a global `settings.json` for claude-code to enable `acceptEdits` by default, allowing for fully autonomous operation. Added a `CLAUDE_SETUP.md` file to document this essential configuration step for new developers or fresh installations.

---
task_id: create-eck-commit-command-v1
date: 2025-09-14T23:29:38Z
type: feat
scope: workflow
---

## Create custom /eck:commit claude-code command

Added a custom slash command to automate the new structured journaling and conventional commit process. This command takes structured input (type, scope, summary, details) and uses it to update JOURNAL.md and create a git commit, enforcing our new workflow.

# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure

### Environment Overrides

The following environment settings override auto-detected values:

- **project_type**: nodejs
- **project_name**: @xelth/eck-snapshot
- **project_version**: 3.0.0
- **has_typescript**: false
- **framework**: node.js
- **is_monorepo**: false

**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.

---


---

## Project Snapshot Information

- **Project**: eckSnapshot
- **Timestamp**: 2025-10-11T12:17:51.284Z
- **Files Included**: 52
- **Total Files in Repo**: 65

---


## Directory Structure

```
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ eck/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ commit.md
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .gemini/
‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ       ‚îî‚îÄ‚îÄ gpt.toml
‚îú‚îÄ‚îÄ create/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îî‚îÄ‚îÄ commands/
‚îÇ               ‚îî‚îÄ‚îÄ extensions/
‚îÇ                   ‚îî‚îÄ‚îÄ sample-extension.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ cli/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ askGpt.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ autoDocs.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ consilium.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ createSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProfiles.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detectProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pruneSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ queryProject.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ restoreSnapshot.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ setupGemini.test.js
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trainTokens.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cli.js
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ segmenter.js
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postgresConnector.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema_simple.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysisService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claudeCliService.test.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dispatcherService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embedding.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddingService.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gptService.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gptService.test.js
‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architect-prompt.template.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ envScanRequest.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gitWorkflow.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multiAgent.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorMode.md
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aiHeader.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fileUtils.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ projectDetector.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tokenEstimator.js
‚îÇ   ‚îî‚îÄ‚îÄ config.js
‚îú‚îÄ‚îÄ test_prune.md/
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ codex_delegation_snapshot.md
‚îú‚îÄ‚îÄ COMMANDS_REFERENCE.md
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ knexfile.js
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ setup.json
‚îú‚îÄ‚îÄ setup.json.bak
‚îú‚îÄ‚îÄ test_knexfile.js
‚îú‚îÄ‚îÄ test_snapshot.md
‚îî‚îÄ‚îÄ vitest.config.js
```

--- File: /.eck-token-training.json ---

{
  "coefficients": {
    "android": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs": [
      0,
      0.3091314011805184,
      0,
      0
    ],
    "python": [
      0,
      0.22,
      0,
      0
    ],
    "rust": [
      0,
      0.18,
      0,
      0
    ],
    "go": [
      0,
      0.19,
      0,
      0
    ],
    "unknown": [
      0,
      0.25,
      0,
      0
    ],
    "nodejs-monorepo": [
      0,
      0.30927098145345017,
      0,
      0
    ]
  },
  "trainingPoints": {
    "nodejs": [
      {
        "fileSizeInBytes": 229899,
        "estimatedTokens": 45980,
        "actualTokens": 71069,
        "timestamp": "2025-09-21T23:52:36.862Z"
      }
    ],
    "nodejs-monorepo": [
      {
        "fileSizeInBytes": 261612,
        "estimatedTokens": 65403,
        "actualTokens": 80909,
        "timestamp": "2025-09-22T01:50:24.392Z"
      }
    ]
  }
}

--- File: /.env.example ---

# Google Gemini API Key - Required for the 'index' and 'query' commands
# Get your key from https://aistudio.google.com/app/apikey
GEMINI_API_KEY="YOUR_API_KEY_HERE"

# PostgreSQL Connection Details
DB_HOST=127.0.0.1
DB_PORT=5432
DB_USER=myuser
DB_PASSWORD=mypassword
DB_DATABASE=eck_snapshot_db

--- File: /.gemini/tools/gpt.toml ---

description = 'Ask GPT (ChatGPT subscription) for coding tasks.'
prompt = '!{node $(pwd)/index.js ask-gpt "{{args}}"}'  # Dynamic path


--- File: /.gitignore ---

# Dependencies
/node_modules

# Production
/build
/dist


# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# IDEs
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# EckSnapshot Index
/.ecksnapshot_index/

# Generated snapshot files
*_vectors.json
*_rag_*.md

# Environment variables
.env
.env.local
.env.production



# Added by eck-snapshot to ignore metadata directory
.eck/

# Test snapshot directory
create-snapshot/

# Claude Code settings (exclude all except commands/eck)
.claude/
!.claude/commands/
!.claude/commands/eck/
!.claude/commands/eck/**

# Third-party managed components
/managed_components/


--- File: /COMMANDS_REFERENCE.md ---

# Commands Reference

This document contains essential commands and setup instructions for the project's multi-agent architecture.

## Junior Architect (`gemini_wsl`) Setup Guide

This section explains how the `gemini_wsl` agent (Junior Architect) is configured to delegate coding tasks to the `claude` agent (Coder).

This architecture relies on `gemini-cli`'s custom tool feature.

## 1. Agent Definition

The Junior Architect (JA) is the `gemini_wsl` agent, which is an instance of `gemini-cli` running in WSL. It is defined in `setup.json`.

## 2. Custom Command (`claude.toml`)

The JA's ability to delegate to the Coder (`claude`) is defined by a custom `gemini-cli` command.

This command must be defined in a file named `claude.toml` and placed in the `gemini-cli` configuration directory (e.g., `~/.gemini/tools/claude.toml`).

### `claude.toml` Content

```toml
description = "Ask Claude (from the eckSnapshot project) to help with a task."
prompt = """!{node /mnt/c/Users/xelth/eckSnapshot/index.js ask-claude "{{args}}"}"""
```

## 3. JA Prompt (`agent-prompt.template.md`)

The JA *knows* how to *use* this command because its main system prompt (loaded from `src/templates/agent-prompt.template.md`) instructs it to.

This prompt *mandates* that the `{{args}}` it passes to the `/claude` command must be a single-line **JSON string** in the `apply_code_changes` format.

## 4. Execution Flow

1.  **Senior Architect (Gemini)** gives a high-level `execute_strategic_task` to `gemini_wsl`.
2.  **`gemini_wsl` (JA)** analyzes the task and formulates a low-level `apply_code_changes` JSON payload.
3.  **`gemini_wsl`** executes `/claude` with the JSON payload as a single string argument (`{{args}}`).
4.  **`claude.toml`** executes the `eck-snapshot ask-claude "{...}"` shell command.
5.  **`eck-snapshot`** (specifically `claudeCliService.js`) receives the JSON string as a 'prompt'.
6.  It forwards this prompt to the `claude-cli` binary (`local_dev`), which is smart enough to parse the JSON and execute the `apply_code_changes` task.

## Claude Code Commands

### Commit Command (`.claude/commands/eck/commit.md`)

A custom command for structured commits with automatic journaling. Place this file in `.claude/commands/eck/commit.md` to enable it in Claude Code.

**Usage:** `/commit <type> <scope> <summary> <details>`

**Function:**
- Stages all current changes
- Creates YAML frontmatter for journal entry with task_id, date, type, scope
- Creates markdown body with summary and details
- Prepends complete journal entry to `.eck/JOURNAL.md`
- Creates conventional commit message: `{type}({scope}): {summary}`
- Executes the commit

**Example:** `/commit feat api "Add user authentication" "Implemented JWT-based auth with login/logout endpoints"`

**Important:** This command should be preserved in git (via `.gitignore` rules) so it can be recreated if lost. The command integrates with the project's `.eck` manifest system for structured development journaling.

## Setup Gemini Command

### Dynamic Path Configuration

The `setup-gemini` command automates the creation of `claude.toml` with dynamic path resolution, eliminating the need for hardcoded paths like `/mnt/c/...`.

**Usage:** `eck-snapshot setup-gemini [options]`

**Options:**
- `-v, --verbose` - Show detailed output and error information

**Function:**
- Detects `gemini-cli` installation using `which()`
- Resolves current project path using `process.cwd()`
- Creates `~/.gemini/tools/claude.toml` with dynamic paths
- Integrates with `setup.json` for environment variable configuration
- Handles cross-platform compatibility (WSL/Windows/macOS/Linux)

**Example Generated `claude.toml`:**
```toml
[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["/home/user/projects/eckSnapshot/index.js", "ask-claude"]

[claude.metadata]
platform = "linux"
working_directory = "/home/user/projects/eckSnapshot"
```

**Benefits:**
- ‚úÖ No hardcoded `/mnt/c/` paths
- ‚úÖ Works across all platforms automatically
- ‚úÖ Resolves paths dynamically based on current working directory
- ‚úÖ Integrates with existing `setup.json` configuration
- ‚úÖ Validates `gemini-cli` and `index.js` availability before setup

## MCP Integration with Feedback Support

### MCP Feedback Object

The `mcp_feedback` object provides enhanced error reporting and status tracking for MCP (Message Control Protocol) integration with gemini-cli delegation.

**Structure:**
```json
{
  "mcp_feedback": {
    "success": true,
    "errors": [],
    "mcp_version": "1.0"
  }
}
```

**Properties:**
- `success` (boolean): Indicates if the MCP operation completed successfully
- `errors` (array): List of error messages or issues encountered during execution
- `mcp_version` (string): Version of MCP protocol used

**Usage in Payloads:**

Add `mcp_feedback` to the `post_execution_steps` object in your JSON command payloads:

```json
{
  "target_agent": "local_dev",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "Implement user authentication",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "auth",
        "summary": "Add JWT authentication"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**Implementation:**

The `claudeCliService.js` automatically parses `mcp_feedback` from incoming prompts and includes it in the response object. When errors are present, they are logged to the console for debugging purposes.

**Benefits:**
- ‚úÖ Enhanced error reporting for MCP operations
- ‚úÖ Version tracking for protocol compatibility
- ‚úÖ Automatic logging of MCP errors
- ‚úÖ Improved debugging for gemini-cli delegation issues

## Auto-Generated Gemini Extensions

*This section is automatically generated. Run `npm run docs:auto` to update.*

### sample-extension

Sample Gemini extension for demonstration

**Commands:**

- **sample-command**: A sample command for testing auto-docs
  - Usage: `sample-command [options]`
  - Examples: `sample-command --help`

**Tools:**

- **sample-tool**: A sample tool for testing auto-docs
  - Usage: Use this tool for sample operations



--- File: /codex_delegation_snapshot.md ---

üîç WSL detected, using configured host: 172.29.16.1


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /knexfile.js ---

import dotenv from 'dotenv';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
const isWSL = process.platform === 'linux' && 
  (process.env.WSL_DISTRO_NAME || 
   fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));

if (isWSL && (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1')) {
  // For WSL, always use the standard WSL2 gateway IP
  process.env.DB_HOST = '172.29.16.1';
}

export default {
  development: {
    client: 'pg',
    connection: {
      host: process.env.DB_HOST || '127.0.0.1',
      port: process.env.DB_PORT || 5432,
      user: process.env.DB_USER || 'myuser',
      password: process.env.DB_PASSWORD || 'mypassword',
      database: process.env.DB_DATABASE || 'eck_snapshot_db',
    },
    pool: {
      min: 2,
      max: 10
    }
  }
};

--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-c": "^0.21.4",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /packages/cli/src/commands/extensions/sample-extension.json ---

{
  "name": "sample-extension",
  "description": "Sample Gemini extension for demonstration",
  "commands": [
    {
      "name": "sample-command",
      "description": "A sample command for testing auto-docs",
      "usage": "sample-command [options]",
      "examples": [
        "sample-command --help"
      ]
    }
  ],
  "tools": [
    {
      "name": "sample-tool",
      "description": "A sample tool for testing auto-docs",
      "usage": "Use this tool for sample operations"
    }
  ]
}

--- File: /setup.json ---

{
  "_comment": "Central configuration file for eck-snapshot. ALL settings are configured here.",
  "projectContext": {
    "name": "eckasse",
    "type": "monorepo",
    "architecture": {
      "workspaces": [
        "@eckasse/core",
        "@eckasse/desktop",
        "@eckasse/adapters",
        "@eckasse/shared-frontend"
      ],
      "stack": [
        "Node.js",
        "Express",
        "Electron",
        "PostgreSQL",
        "WebSocket"
      ],
      "aiIntegration": "Google Gemini for POS natural language control"
    }
  },
  "projectDetection": {
    "_comment": "Automatic project type detection based on file structure",
    "patterns": {
      "android": {
        "files": [
          "build.gradle",
          "build.gradle.kts",
          "settings.gradle",
          "settings.gradle.kts"
        ],
        "directories": [
          "app/src/main",
          "app/src/androidTest"
        ],
        "manifestFiles": [
          "AndroidManifest.xml"
        ],
        "priority": 10
      },
      "nodejs-monorepo": {
        "files": [
          "package.json"
        ],
        "directories": [
          "packages",
          "apps",
          "libs"
        ],
        "patterns": [
          "workspaces",
          "lerna",
          "nx",
          "rush"
        ],
        "priority": 7
      },
      "nodejs": {
        "files": [
          "package.json"
        ],
        "directories": [
          "node_modules"
        ],
        "priority": 6
      },
      "python-poetry": {
        "files": [
          "pyproject.toml"
        ],
        "patterns": [
          "tool.poetry"
        ],
        "priority": 9
      },
      "python-pip": {
        "files": [
          "requirements.txt",
          "setup.py",
          "setup.cfg"
        ],
        "directories": [
          "__pycache__",
          "venv",
          ".venv"
        ],
        "priority": 7
      },
      "python-conda": {
        "files": [
          "environment.yml",
          "environment.yaml",
          "conda.yml"
        ],
        "priority": 8
      },
      "django": {
        "files": [
          "manage.py"
        ],
        "patterns": [
          "django",
          "Django"
        ],
        "priority": 9
      },
      "flask": {
        "files": [
          "app.py",
          "application.py"
        ],
        "patterns": [
          "flask",
          "Flask"
        ],
        "priority": 8
      },
      "flutter": {
        "files": [
          "pubspec.yaml"
        ],
        "directories": [
          "lib",
          "android",
          "ios"
        ],
        "priority": 8
      },
      "react-native": {
        "files": [
          "package.json"
        ],
        "directories": [
          "android",
          "ios"
        ],
        "patterns": [
          "react-native"
        ],
        "priority": 8
      },
      "rust": {
        "files": [
          "Cargo.toml"
        ],
        "directories": [
          "src",
          "target"
        ],
        "priority": 9
      },
      "go": {
        "files": [
          "go.mod",
          "go.sum"
        ],
        "directories": [
          "cmd",
          "pkg",
          "internal"
        ],
        "priority": 7
      },
      "dotnet": {
        "files": [
          "*.csproj",
          "*.sln",
          "*.fsproj",
          "*.vbproj"
        ],
        "directories": [
          "bin",
          "obj"
        ],
        "priority": 7
      },
      "c": {
        "files": [
          "Makefile",
          "CMakeLists.txt",
          "*.c",
          "*.h"
        ],
        "directories": [
          "src",
          "include",
          "lib"
        ],
        "priority": 6
      }
    }
  },
  "environmentDetection": {
    "_comment": "Cross-platform environment detection",
    "scanCommand": "node -e \"console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').existsSync('*.sqlite*')?'SQLite':'none', require('path').basename(process.cwd())].join('|'))\"",
    "responseFormat": "ENV|OS/arch|NodeVersion|Database|ProjectFolder",
    "platformMarkers": {
      "development": {
        "paths": [
          "/home/",
          "/Users/",
          "\\Users\\",
          "WSL"
        ],
        "database": [
          "SQLite",
          "*.sqlite*"
        ],
        "process": [
          "npm",
          "node"
        ]
      },
      "production": {
        "paths": [
          "/var/www/",
          "/opt/",
          "/srv/"
        ],
        "database": [
          "PostgreSQL",
          "eckwms"
        ],
        "process": [
          "PM2",
          "systemd"
        ]
      }
    }
  },
  "smartMode": {
    "tokenThreshold": 7000000,
    "_comment": "Projects over this token count will use vector indexing instead of single file snapshots"
  },
  "vectorIndex": {
    "autoExportOnIndex": true,
    "_comment": "Automatically export the vector index to a file after every successful 'index' command."
  },
  "contextProfiles": {
    "backend": {
      "description": "Backend API, database, business logic",
      "include": [
        "packages/backend/**",
        "packages/core/**",
        "knexfile.js",
        "migrations/**"
      ],
      "exclude": [
        "**/*.test.*",
        "**/*.sqlite*",
        "node_modules/**"
      ]
    },
    "frontend": {
      "description": "Electron app and frontend components",
      "include": [
        "packages/desktop/**",
        "packages/shared-frontend/**",
        "packages/adapters/**"
      ],
      "exclude": [
        "**/dist/**",
        "**/build/**",
        "**/node_modules/**"
      ]
    },
    "android-core": {
      "description": "Android app source code and resources",
      "include": [
        "app/src/main/java/**",
        "app/src/main/kotlin/**",
        "app/src/main/res/**",
        "app/src/main/AndroidManifest.xml",
        "build.gradle*",
        "settings.gradle*"
      ],
      "exclude": [
        "**/build/**",
        "**/.gradle/**",
        "**/generated/**",
        "app/src/androidTest/**",
        "app/src/test/**"
      ]
    },
    "android-tests": {
      "description": "Android test code and configurations",
      "include": [
        "app/src/test/**",
        "app/src/androidTest/**",
        "**/test/**"
      ],
      "exclude": [
        "**/build/**"
      ]
    },
    "android-config": {
      "description": "Android build configuration and dependencies",
      "include": [
        "build.gradle*",
        "settings.gradle*",
        "gradle.properties",
        "local.properties",
        "proguard-rules.pro",
        "gradle/libs.versions.toml"
      ]
    },
    "database": {
      "description": "Database schema and migrations only",
      "include": [
        "**/migrations/**",
        "**/knexfile.js",
        "**/schema.sql"
      ]
    },
    "deployment": {
      "description": "Deployment and PM2 configuration",
      "include": [
        "ecosystem.config.js",
        "pm2.json",
        ".github/workflows/**"
      ]
    },
    "test-frontend": {
      "description": "Test frontend profile",
      "include": [
        "frontend/**"
      ],
      "exclude": []
    },
    "test-backend": {
      "description": "Test backend profile",
      "include": [
        "backend/**"
      ],
      "exclude": []
    }
  },
  "fileFiltering": {
    "filesToIgnore": [
      "package-lock.json",
      "npm-shrinkwrap.json",
      "yarn.lock",
      "pnpm-lock.yaml",
      "*.log",
      "*.tmp",
      ".env",
      ".env.local",
      ".env.production",
      "eckasse_*.sqlite*",
      "README*",
      "readme*",
      "LICENSE*"
    ],
    "extensionsToIgnore": [
      ".sqlite3",
      ".sqlite",
      ".db",
      ".DS_Store",
      ".env",
      ".pyc",
      ".class",
      ".jar",
      ".aar",
      ".apk",
      ".aab",
      ".dex",
      ".o",
      ".so",
      ".dylib",
      ".log",
      ".tmp",
      ".bak",
      ".swp",
      ".ico",
      ".png",
      ".jpg",
      ".jpeg",
      ".gif",
      ".svg"
    ],
    "dirsToIgnore": [
      "node_modules/",
      ".git/",
      ".eck/",
      "dist/",
      "build/",
      ".next/",
      ".nuxt/",
      "target/",
      "bin/",
      "obj/",
      ".idea/",
      "coverage/",
      "create-snapshot/"
    ],
    "includeHidden": false,
    "projectSpecific": {
      "android": {
        "filesToIgnore": [
          "gradle-wrapper.jar",
          "local.properties",
          "*.iml",
          "*.apk",
          "*.aab",
          "*.aar",
          "*.jar",
          "*.dex",
          "R.java",
          "BuildConfig.java"
        ],
        "dirsToIgnore": [
          "build/",
          ".gradle/",
          ".idea/",
          "app/build/",
          "app/.cxx/",
          "**/generated/",
          "**/intermediates/",
          "**/outputs/",
          "**/tmp/"
        ],
        "extensionsToIgnore": [
          ".apk",
          ".aab",
          ".aar",
          ".jar",
          ".dex",
          ".pro",
          ".ap_",
          ".aidl"
        ]
      },
      "nodejs": {
        "filesToIgnore": [
          "package-lock.json",
          "yarn.lock",
          "pnpm-lock.yaml"
        ],
        "dirsToIgnore": [
          "node_modules/",
          "dist/",
          ".next/"
        ]
      },
      "python": {
        "filesToIgnore": [
          "*.pyc",
          "*.pyo",
          "*.pyd",
          "__pycache__",
          "*.egg-info",
          ".coverage"
        ],
        "dirsToIgnore": [
          "__pycache__/",
          ".pytest_cache/",
          ".coverage/",
          "venv/",
          ".venv/",
          "env/",
          ".env/",
          "dist/",
          "build/",
          "*.egg-info/",
          ".tox/",
          ".mypy_cache/"
        ],
        "extensionsToIgnore": [
          ".pyc",
          ".pyo",
          ".pyd"
        ]
      },
      "rust": {
        "filesToIgnore": [
          "Cargo.lock"
        ],
        "dirsToIgnore": [
          "target/",
          "debug/",
          "release/"
        ]
      },
      "go": {
        "filesToIgnore": [
          "go.sum"
        ],
        "dirsToIgnore": [
          "vendor/"
        ]
      },
      "dotnet": {
        "filesToIgnore": [
          "*.user",
          "*.suo",
          "*.cache"
        ],
        "dirsToIgnore": [
          "bin/",
          "obj/",
          ".vs/",
          "packages/"
        ]
      },
      "c": {
        "filesToIgnore": [
          "*.o",
          "*.a",
          "*.so",
          "*.out",
          "*.exe"
        ],
        "dirsToIgnore": [
          "build/",
          "cmake-build-debug/",
          "cmake-build-release/",
          ".cmake/"
        ],
        "extensionsToIgnore": [
          ".o",
          ".a",
          ".so",
          ".out"
        ]
      }
    }
  },
  "performance": {
    "maxFileSize": "10MB",
    "maxTotalSize": "100MB",
    "maxDepth": 10,
    "concurrency": 10
  },
  "output": {
    "defaultFormat": "md",
    "defaultPath": "./.eck/snapshots",
    "includeTree": true,
    "_comment": "Default output format for snapshots: md, json, or txt. defaultPath is the output directory. includeTree controls whether to include directory structure."
  },
  "aiInstructions": {
    "architectPersona": {
      "role": "Project Manager and Solution Architect AI",
      "goal": "Translate user requests into technical plans and generate precise commands for execution agents",
      "contextRequirement": "ALWAYS check environment context before generating commands",
      "workflow": [
        "Request ENV scan from agent",
        "Analyze User Request in their native language",
        "Formulate environment-appropriate technical plan",
        "Propose the plan and await user confirmation",
        "Generate environment-specific JSON command block",
        "Communicate with user in their language, commands in ENGLISH"
      ]
    },
    "executionAgents": {
      "local_dev": {
        "active": true,
        "name": "Local Development Agent (AGENT_LOCAL_DEV)",
        "description": "Cross-platform local development with SQLite",
        "guiSupport": true,
        "identification": {
          "markers": [
            "WSL",
            "/home/",
            "/Users/",
            "\\Users\\",
            "SQLite"
          ]
        },
        "capabilities": [
          "npm install",
          "npm run dev",
          "npm run dev:backend",
          "git operations",
          "sqlite3 commands",
          "electron debug",
          "file editing",
          "testing commands",
          "browser automation"
        ],
        "restrictions": [
          "no PM2 commands",
          "no PostgreSQL production operations",
          "no systemctl",
          "no production deployments"
        ]
      },
      "production_server": {
        "active": true,
        "name": "Production Server Agent (AGENT_PROD_SERVER)",
        "description": "Linux production server with PostgreSQL and PM2, with development capabilities",
        "guiSupport": false,
        "identification": {
          "markers": [
            "/var/www/",
            "PostgreSQL",
            "PM2",
            "eckwms"
          ]
        },
        "capabilities": [
          "pm2 restart/reload/stop/start",
          "postgresql queries",
          "knex migrations --env production",
          "systemctl",
          "log analysis",
          "nginx operations",
          "deployment scripts",
          "file editing",
          "npm install",
          "git operations"
        ],
        "restrictions": [
          "no electron",
          "no GUI apps",
          "no direct DB schema changes without migrations",
          "always backup before migrations"
        ]
      },
      "android_wsl_dev": {
        "active": true,
        "name": "Android WSL Development Agent (AGENT_ANDROID_WSL)",
        "description": "Android development in Windows Subsystem for Linux with Gradle and ADB support",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "build.gradle.kts",
            "/mnt/c/Users",
            "app/src/main",
            "AndroidManifest.xml"
          ]
        },
        "capabilities": [
          "./gradlew build",
          "./gradlew assembleDebug",
          "./gradlew assembleRelease",
          "./gradlew clean",
          "./gradlew lint",
          "./gradlew test",
          "adb devices",
          "adb install",
          "adb logcat",
          "adb shell",
          "git operations",
          "file editing",
          "gradle tasks",
          "gradle wrapper operations"
        ],
        "restrictions": [
          "no direct GUI access (Android Studio)",
          "requires /mnt/c/ path for Windows file system access",
          "no Android emulator control (emulator runs on Windows host)",
          "limited USB device access through WSL",
          "no hardware debugging interfaces"
        ]
      },
      "ci_cd": {
        "active": false,
        "name": "CI/CD Pipeline Agent (AGENT_CI_CD)",
        "description": "Automated testing and deployment pipeline",
        "guiSupport": false,
        "capabilities": [
          "npm ci",
          "npm test",
          "npm run build",
          "docker build",
          "artifact generation"
        ],
        "restrictions": [
          "no interactive commands",
          "no GUI applications",
          "no watch modes",
          "no development servers"
        ]
      },
      "gemini_wsl": {
        "active": true,
        "name": "Gemini WSL Agent (Junior Architect)",
        "description": "Gemini, running in WSL. Can access all WSL tools and delegate to other agents like claude.",
        "guiSupport": false,
        "identification": {
          "markers": [
            "WSL",
            "gemini",
            "claude"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)",
          "claude (delegate)"
        ],
        "restrictions": [
          "Runs only inside the WSL environment"
        ]
      },
      "gemini_windows": {
        "active": true,
        "name": "Gemini Windows Agent (Standalone)",
        "description": "Gemini, running in native Windows (PowerShell). Can only access Windows tools.",
        "guiSupport": true,
        "identification": {
          "markers": [
            "Windows",
            "gemini",
            "!WSL"
          ]
        },
        "capabilities": [
          "git operations",
          "npm install",
          "file editing",
          "gemini (self)"
        ],
        "restrictions": [
          "Runs only in native Windows",
          "Cannot access WSL-only tools like claude"
        ]
      }
    },
    "header": {
      "defaultEnabled": true,
      "_comment": "Controls whether AI instruction headers are included by default in snapshots"
    },
    "promptTemplates": {
      "envScanRequest": "src/templates/envScanRequest.md",
      "gitWorkflow": "src/templates/gitWorkflow.md",
      "multiAgent": "src/templates/multiAgent.md",
      "vectorMode": "src/templates/vectorMode.md",
      "agent": "src/templates/agent-prompt.template.md"
    }
  },
  "consilium": {
    "enabled": true,
    "votingMode": "blind-first",
    "autoTrigger": {
      "conditions": [
        "database migration",
        "production deployment",
        "architecture changes",
        "security updates"
      ]
    },
    "phases": {
      "blind": {
        "enabled": true,
        "timeout": "3m"
      },
      "debate": {
        "enabled": true,
        "triggerDivergence": 0.3,
        "rounds": 2
      }
    },
    "defaultMembers": {
      "architect": {
        "active": true,
        "modelName": "Claude/Gemini",
        "role": "System Architecture",
        "strengths": [
          "monorepo structure",
          "workspace dependencies",
          "API design"
        ]
      },
      "database_expert": {
        "active": true,
        "modelName": "GPT-4/Claude",
        "role": "Database Specialist",
        "strengths": [
          "PostgreSQL optimization",
          "Knex migrations",
          "data integrity"
        ]
      },
      "devops": {
        "active": true,
        "modelName": "Any Available",
        "role": "Deployment & Operations",
        "strengths": [
          "PM2 configuration",
          "environment management",
          "monitoring"
        ]
      }
    },
    "complexityThresholds": {
      "low": [
        "bug fixes",
        "simple features",
        "documentation"
      ],
      "medium": [
        "feature implementation",
        "refactoring",
        "integration"
      ],
      "high": [
        "architecture changes",
        "system redesign",
        "performance optimization",
        "security implementation"
      ]
    },
    "prompts": {
      "requestTemplate": "\n# Consilium Request\n\n## Task Overview\n- **Complexity:** {{complexity}}\n- **Domain:** {{domain}}\n- **Requesting Agent:** {{requestingAgent}}\n- **Environment:** {{envContext}}\n\n## Problem Description\n{{problemDescription}}\n\n## Questions for Consilium\n{{#each questions}}\n- {{this}}\n{{/each}}\n\n## Available Context\n{{contextDescription}}\n\n---\n",
      "responseTemplate": "\n# Consilium Response - {{modelName}}\n\n## Analysis\n{{analysis}}\n\n## Recommendations\n{{recommendations}}\n\n## Implementation Steps\n{{implementationSteps}}\n\n## Risks and Considerations\n{{risks}}\n\n## Success Metrics\n{{successMetrics}}\n\n---\n"
    }
  }
}

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject, viewIndex } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .action(createRepoSnapshot)
    .addHelpText('after', `
Examples for --profile:
  --profile backend                      (Uses the 'backend' profile)
  --profile "backend,-**/tests/**"         (Uses 'backend' profile, excludes all test files)
  --profile "src/**/*.js,-**/*.test.js"  (Includes all JS files in src, excludes tests)

  Combine predefined profiles (from .eck/profiles.json) with ad-hoc glob patterns.
  Prefix a profile name or glob pattern with '-' to exclude it.
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Index view command
  program
    .command('index-view')
    .description('View the contents of the code chunks database')
    .option('--limit <number>', 'Number of records to display', (val) => parseInt(val), 10)
    .option('--offset <number>', 'Number of records to skip', (val) => parseInt(val), 0)
    .option('--file <path>', 'Filter by file path')
    .action(viewIndex);

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/cli/commands/askGpt.js ---

import { ask } from '../../services/gptService.js';

/**
 * CLI entry point for ask-gpt command.
 * @param {string} payload - JSON payload string.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} options - CLI options.
 */
export async function askGpt(payload, options = {}) {
  const verbose = Boolean(options.verbose);
  const model = options.model || 'gpt-5-codex';
  const reasoning = options.reasoning || 'high';

  if (!payload) {
    console.error('ask-gpt requires a JSON payload argument.');
    process.exitCode = 1;
    return;
  }

  try {
    const result = await ask(payload, { verbose, model, reasoning });
    console.log(JSON.stringify(result, null, 2));
  } catch (error) {
    console.error(error.message);
    if (verbose && error?.stack) {
      console.error(error.stack);
    }
    process.exitCode = 1;
  }
}


--- File: /src/cli/commands/autoDocs.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

/**
 * Auto-generate documentation from gemini-extension.json files
 */
export async function generateAutoDocs() {
  try {
    const projectRoot = path.resolve(__dirname, '../../../');
    const extensionsDir = path.join(projectRoot, 'packages/cli/src/commands/extensions');
    const referenceFile = path.join(projectRoot, 'COMMANDS_REFERENCE.md');

    // Check if extensions directory exists
    try {
      await fs.access(extensionsDir);
    } catch (error) {
      console.log(`Extensions directory not found at: ${extensionsDir}`);
      console.log('Creating example structure...');

      // Create the directory structure
      await fs.mkdir(extensionsDir, { recursive: true });

      // Create a sample gemini-extension.json file for demonstration
      const sampleExtension = {
        name: "sample-extension",
        description: "Sample Gemini extension for demonstration",
        commands: [
          {
            name: "sample-command",
            description: "A sample command for testing auto-docs",
            usage: "sample-command [options]",
            examples: ["sample-command --help"]
          }
        ],
        tools: [
          {
            name: "sample-tool",
            description: "A sample tool for testing auto-docs",
            usage: "Use this tool for sample operations"
          }
        ]
      };

      await fs.writeFile(
        path.join(extensionsDir, 'sample-extension.json'),
        JSON.stringify(sampleExtension, null, 2)
      );

      console.log('Created sample extension at:', path.join(extensionsDir, 'sample-extension.json'));
    }

    // Read all JSON files in the extensions directory
    const files = await fs.readdir(extensionsDir);
    const jsonFiles = files.filter(file => file.endsWith('.json'));

    if (jsonFiles.length === 0) {
      console.log('No JSON files found in extensions directory');
      return;
    }

    console.log(`Found ${jsonFiles.length} extension file(s): ${jsonFiles.join(', ')}`);

    // Parse each JSON file and extract command/tool information
    const extensions = [];

    for (const file of jsonFiles) {
      try {
        const filePath = path.join(extensionsDir, file);
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = JSON.parse(content);
        extensions.push({ filename: file, ...extension });
        console.log(`Parsed extension: ${extension.name || file}`);
      } catch (error) {
        console.warn(`Failed to parse ${file}:`, error.message);
      }
    }

    if (extensions.length === 0) {
      console.log('No valid extension files found');
      return;
    }

    // Generate markdown content
    let markdownContent = '\n## Auto-Generated Gemini Extensions\n\n';
    markdownContent += '*This section is automatically generated. Run `npm run docs:auto` to update.*\n\n';

    for (const extension of extensions) {
      markdownContent += `### ${extension.name || extension.filename}\n\n`;

      if (extension.description) {
        markdownContent += `${extension.description}\n\n`;
      }

      // Add commands section
      if (extension.commands && extension.commands.length > 0) {
        markdownContent += '**Commands:**\n\n';
        for (const command of extension.commands) {
          markdownContent += `- **${command.name}**: ${command.description || 'No description'}\n`;
          if (command.usage) {
            markdownContent += `  - Usage: \`${command.usage}\`\n`;
          }
          if (command.examples && command.examples.length > 0) {
            markdownContent += `  - Examples: ${command.examples.map(ex => `\`${ex}\``).join(', ')}\n`;
          }
        }
        markdownContent += '\n';
      }

      // Add tools section
      if (extension.tools && extension.tools.length > 0) {
        markdownContent += '**Tools:**\n\n';
        for (const tool of extension.tools) {
          markdownContent += `- **${tool.name}**: ${tool.description || 'No description'}\n`;
          if (tool.usage) {
            markdownContent += `  - Usage: ${tool.usage}\n`;
          }
        }
        markdownContent += '\n';
      }
    }

    // Read the current COMMANDS_REFERENCE.md
    let currentContent;
    try {
      currentContent = await fs.readFile(referenceFile, 'utf-8');
    } catch (error) {
      console.warn('COMMANDS_REFERENCE.md not found, creating new file');
      currentContent = '# Commands Reference\n\n';
    }

    // Remove existing auto-generated section if it exists
    const autoGenRegex = /\n## Auto-Generated Gemini Extensions[\s\S]*?(?=\n## |\n# |$)/;
    const updatedContent = currentContent.replace(autoGenRegex, '') + markdownContent;

    // Write the updated content back to the file
    await fs.writeFile(referenceFile, updatedContent);

    console.log('\n‚úÖ Auto-documentation generated successfully!');
    console.log(`üìù Updated: ${referenceFile}`);
    console.log(`üì¶ Processed ${extensions.length} extension(s)`);

  } catch (error) {
    console.error('Failed to generate auto-docs:', error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/consilium.js ---

import fs from 'fs/promises';

/**
 * Generate a consilium request for complex decisions
 */
async function generateConsiliumRequest(task, complexity, agentId) {
  const request = {
    consilium_request: {
      request_id: `cons-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      requesting_agent: agentId,
      complexity_score: complexity,
      
      task: {
        type: task.type || "technical_decision",
        title: task.title,
        description: task.description,
        current_implementation: task.currentCode || "N/A",
        proposed_solution: task.proposedSolution || "To be determined",
        constraints: task.constraints || [],
        success_criteria: task.criteria || []
      },
      
      consilium_instructions: `
        You are a technical expert participating in a consilium decision.
        
        RESPOND WITH:
        1. Your expert opinion on the best approach
        2. Specific technical recommendations
        3. Potential risks and mitigation strategies
        4. Your confidence level (0-100%)
        
        FORMAT YOUR RESPONSE AS JSON:
        {
          "expert": "[Your Model Name]",
          "role": "[Your assigned role]",
          "recommendation": {
            "approach": "Detailed technical solution",
            "implementation_steps": ["step1", "step2"],
            "key_benefits": ["benefit1", "benefit2"],
            "risks": ["risk1", "risk2"],
            "mitigation": ["strategy1", "strategy2"]
          },
          "alternatives_considered": ["alt1", "alt2"],
          "confidence": 85,
          "critical_warnings": []
        }
      `,
      
      aggregation_rules: {
        minimum_confidence_required: 60,
        consensus_threshold: 0.66,
        veto_roles: ["security_auditor"],
        conflict_resolution: "weighted_average_with_discussion"
      }
    }
  };
  
  return request;
}

export async function generateConsilium(options) {
  console.log('üß† Generating Consilium Request...');
  
  const task = {
    type: options.type || 'technical_decision',
    title: options.title || 'Technical Decision Required',
    description: options.description || 'Please provide a description',
    constraints: options.constraints ? options.constraints.split(',') : [],
    currentCode: options.snapshot || null
  };
  
  const complexity = options.complexity || 7;
  const agentId = options.agent || 'AGENT_ORCHESTRATOR';
  
  const request = await generateConsiliumRequest(task, complexity, agentId);
  
  const outputFile = options.output || 'consilium_request.json';
  await fs.writeFile(outputFile, JSON.stringify(request, null, 2));
  
  console.log(`‚úÖ Consilium request saved to: ${outputFile}`);
  console.log('\nüìã Next steps:');
  console.log('1. Send this request to multiple LLM experts');
  console.log('2. Collect their responses');
  console.log('3. Run: eck-snapshot process-consilium <responses.json>');
}

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return {
          content: `--- File: /${normalizedPath} ---\n\n${content}\n\n`,
          path: normalizedPath,
          size: fileStats.size
        };
      } catch (error) {
        stats.errors.push(`${normalizedPath}: ${error.message}`);
        trackSkippedFile(normalizedPath, `Error: ${error.message}`);
        return null;
      }
    };

    const results = await Promise.all(allFiles.map((fp, index) => limit(() => processFile(fp, index))));
    progressBar.stop();
    
    const successfulFileObjects = results.filter(Boolean);
    const contentArray = successfulFileObjects.map(f => f.content);

    // Return all processed data instead of writing file
    return {
      stats,
      contentArray,
      successfulFileObjects,
      allFiles,
      originalCwd,
      repoPath
    };
    
  } finally {
    process.chdir(originalCwd); // Ensure we always change back
  }
}

export async function createRepoSnapshot(repoPath, options) {
  const spinner = ora('Analyzing project...').start();
  try {
    // Ensure snapshots/ is in .gitignore to prevent accidental commits
    await ensureSnapshotsInGitignore(repoPath);
    
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(repoPath);

    // Auto-commit unstaged changes if in a git repo
    const isGitRepo = await checkGitRepository(repoPath);
    if (isGitRepo) {
      spinner.text = 'Checking for unstaged changes...';
      try {
        const { stdout: status } = await execa('git', ['status', '--porcelain'], { cwd: repoPath });
        if (status) {
          spinner.text = 'Unstaged changes detected. Auto-committing...';
          await execa('git', ['add', '.'], { cwd: repoPath });
          const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
          await execa('git', ['commit', '-m', `chore(snapshot): Auto-commit before snapshot [${timestamp}]`], { cwd: repoPath });
          spinner.info('Auto-commit complete.');
        } else {
          // No changes, do nothing. Logging this would be too verbose.
        }
      } catch (e) {
        spinner.warn(`Auto-commit failed: ${e.message}`);
      }
    }
    spinner.text = 'Analyzing project...'; // Reset spinner text
    
    // Detect project type first
    const projectDetection = await detectProjectType(repoPath);
    spinner.stop();
    displayProjectInfo(projectDetection);
    
    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    
    // Update project context based on detection
    if (projectDetection.type !== 'unknown' && projectDetection.details) {
      setupConfig.projectContext = createDynamicProjectContext(projectDetection);
    }
    
    // Merge configs: setup.json base, user overrides, command options
    const config = {
      ...userConfig, // Start with old defaults
      ...setupConfig.fileFiltering, // Overwrite with setup.json values
      ...setupConfig.performance,
      smartModeTokenThreshold: setupConfig.smartMode.tokenThreshold,
      defaultFormat: setupConfig.output?.defaultFormat || 'md',
      aiHeaderEnabled: setupConfig.aiInstructions?.header?.defaultEnabled ?? true,
      ...options // Command-line options have the final say
    };
    
    // Apply defaults for options that may not be provided via command line
    if (!config.output) {
      config.output = setupConfig.output?.defaultPath || './snapshots';
    }
    // For tree option, we need to check if --no-tree was explicitly passed
    // Commander.js sets tree to false when --no-tree is passed, true otherwise
    // We only want to use the config default if the user didn't specify --no-tree
    if (!('noTree' in options)) {
      // User didn't pass --no-tree, so we can use the config default
      config.tree = setupConfig.output?.includeTree ?? true;
    }
    if (config.includeHidden === undefined) {
      config.includeHidden = setupConfig.fileFiltering?.includeHidden ?? false;
    }

    const estimation = await estimateProjectTokens(repoPath, config, projectDetection.type);
    spinner.info(`Estimated project size: ~${Math.round(estimation.estimatedTokens).toLocaleString()} tokens.`);

    if (estimation.estimatedTokens > config.smartModeTokenThreshold) {
      spinner.succeed('Project is large. Switching to vector indexing mode.');
      await indexProject(repoPath, options);
    } else {
      spinner.succeed('Project is small. Creating dual snapshots...');
      
      // Step 1: Process all files ONCE
      const { 
        stats, 
        contentArray, 
        successfulFileObjects, 
        allFiles, 
        originalCwd: processingOriginalCwd, // We get originalCwd from the processing function
        repoPath: processedRepoPath 
      } = await processProjectFiles(repoPath, options, config, projectDetection.type);

      const originalCwd = process.cwd(); // Get CWD *before* chdir
      process.chdir(processedRepoPath); // Go back to repo path for git hash and tree

      try {
        // --- Common Data --- 
        const timestamp = new Date().toISOString().slice(0, 19).replace('T', '_').replace(/:/g, '-');
        const repoName = path.basename(processedRepoPath);
        const gitHash = await getGitCommitHash(processedRepoPath);
        const fileExtension = options.format || config.defaultFormat || 'md';
        const outputPath = options.output || path.resolve(originalCwd, config.output);
        await fs.mkdir(outputPath, { recursive: true });

        const shouldIncludeTree = config.tree && !options.noTree;
        let directoryTree = '';
        if (shouldIncludeTree) {
          console.log('üå≥ Generating directory tree...');
          directoryTree = await generateDirectoryTree(processedRepoPath, '', allFiles, 0, config.maxDepth || 10, config);
        }

        // Calculate included file stats by extension
        const includedFilesByType = new Map();
        for (const fileObj of successfulFileObjects) {
            try {
                let ext = path.extname(fileObj.path);
                if (ext === '') ext = '.no-extension';
                includedFilesByType.set(ext, (includedFilesByType.get(ext) || 0) + 1);
            } catch (e) { /* Silently ignore */ }
        }
        const sortedIncludedStats = [...includedFilesByType.entries()].sort((a, b) => b[1] - a[1]);

        // Calculate Top 10 Largest Files
        const largestFiles = [...successfulFileObjects].sort((a, b) => b.size - a.size).slice(0, 10);

        const fileBody = (directoryTree ? `\n## Directory Structure\n\n\`\`\`\n${directoryTree}\`\`\`\n\n` : '') + contentArray.join('');

        // --- File 1: Architect Snapshot --- 
        const architectOptions = { ...options, agent: false };
        // Load manifest for headers
        const eckManifest = await loadProjectEckManifest(processedRepoPath);
        const isGitRepo = await checkGitRepository(processedRepoPath);

        const architectHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: architectOptions, repoPath: processedRepoPath }, isGitRepo);
        const architectBaseFilename = `${repoName}_snapshot_${timestamp}${gitHash ? `_${gitHash}` : ''}`;
        const architectFilename = `${architectBaseFilename}.${fileExtension}`;
        const architectFilePath = path.join(outputPath, architectFilename);
        await fs.writeFile(architectFilePath, architectHeader + fileBody);

        // --- File 2: Junior Architect Snapshot --- 
        let jaFilePath = null;
        if (!options.profile && !options.agent && fileExtension === 'md') { // Only create JA snapshot if main is MD
          console.log('üñãÔ∏è Generating Junior Architect (_ja) snapshot...');
          const jaOptions = { ...options, agent: true, noTree: false, noAiHeader: false };
          const jaHeader = await generateEnhancedAIHeader({ stats, repoName, mode: 'file', eckManifest, options: jaOptions, repoPath: processedRepoPath }, isGitRepo);
          const jaFilename = `${architectBaseFilename}_ja.${fileExtension}`;
          jaFilePath = path.join(outputPath, jaFilename);
          await fs.writeFile(jaFilePath, jaHeader + fileBody);
        }

        // --- Combined Report --- 
        console.log('\n‚úÖ Snapshot generation complete!');
        console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        console.log(`üìÑ Architect File: ${architectFilePath}`);
        if (jaFilePath) {
          console.log(`üìÑ Junior Arch File: ${jaFilePath}`);
        }
        console.log(`üìä Files processed: ${stats.includedFiles}/${stats.totalFiles}`);
        console.log(`üìè Total size: ${formatSize(stats.totalSize)}`);
        console.log(`üì¶ Processed size: ${formatSize(stats.processedSize)}`);
        console.log(`üìã Format: ${fileExtension.toUpperCase()}`);

        if (sortedIncludedStats.length > 0) {
          console.log('\nüì¶ Included File Types:');
          console.log('---------------------------------');
          for (const [ext, count] of sortedIncludedStats.slice(0, 10)) {
              console.log(`   - ${String(ext).padEnd(15)} ${String(count).padStart(5)} files`);
          }
          if (sortedIncludedStats.length > 10) {
              console.log(`   ... and ${sortedIncludedStats.length - 10} other types.`);
          }
        }

        if (largestFiles.length > 0) {
          console.log('\nüêò Top 10 Largest Files (Included):');
          console.log('---------------------------------');
          for (const fileObj of largestFiles) {
              console.log(`   - ${String(formatSize(fileObj.size)).padEnd(15)} ${fileObj.path}`);
          }
        }
        
        // Excluded/Skipped Files Section
        const hasExcludedContent = stats.excludedFiles > 0 || stats.binaryFiles > 0 || stats.oversizedFiles > 0 || stats.ignoredFiles > 0 || stats.errors.length > 0;
        if (hasExcludedContent) {
          console.log('\nüö´ Excluded/Skipped Files:');
          console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
        }
        
        if (stats.excludedFiles > 0) {
          console.log(`üö´ Excluded files: ${stats.excludedFiles}`);
        }
        if (stats.binaryFiles > 0) {
          console.log(`üì± Binary files skipped: ${stats.binaryFiles}`);
        }
        if (stats.oversizedFiles > 0) {
          console.log(`üìè Oversized files skipped: ${stats.oversizedFiles}`);
        }
        if (stats.ignoredFiles > 0) {
          console.log(`üôà Ignored files: ${stats.ignoredFiles}`);
        }
        if (stats.errors.length > 0) {
          console.log(`‚ùå Errors: ${stats.errors.length}`);
          if (options.verbose) {
            stats.errors.forEach(err => console.log(`   ${err}`));
          }
        }
        
        // Print detailed skip reasons report
        if (stats.skippedFilesDetails.size > 0) {
          console.log('\nüìã Skip Reasons:');
          console.log('---------------------------------');
          
          for (const [reason, files] of stats.skippedFilesDetails.entries()) {
            console.log(`\nüî∏ ${reason} (${files.length} files):`);
            files.forEach(file => {
              console.log(`   ‚Ä¢ ${file}`);
            });
          }
          console.log('---------------------------------');
        } else {
          console.log('---------------------------------');
        }
        
        // Generate training command string if estimation data is available
        if (estimation && projectDetection.type && !options.profile) {
          const trainingCommand = generateTrainingCommand(projectDetection.type, estimation.estimatedTokens, estimation.totalSize, repoPath);
          console.log('\nüéØ To improve token estimation accuracy, run this command after checking actual tokens:');
          console.log(`${trainingCommand}[ACTUAL_TOKENS_HERE]`);
          console.log('   Replace [ACTUAL_TOKENS_HERE] with the real token count from your LLM');
        }

      } finally {
        process.chdir(originalCwd); // Final reset back to original CWD
      }
    }
  } catch (error) {
    spinner.fail(`Operation failed: ${error.message}`);
    process.exit(1);
  }
}

--- File: /src/cli/commands/detectProfiles.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { scanDirectoryRecursively, generateDirectoryTree, initializeEckManifest, loadConfig } from '../../utils/fileUtils.js';
import { loadSetupConfig } from '../../config.js';

/**
 * Extracts a JSON object from a string that might contain markdown wrappers or log output.
 * Finds the first opening brace '{' and the last closing brace '}' to extract the JSON.
 */
function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }

  const firstBrace = text.indexOf('{');
  const lastBrace = text.lastIndexOf('}');

  if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
    return text.substring(firstBrace, lastBrace + 1).trim();
  }

  return text.trim();
}

/**
 * Scans the project structure, saves the directory tree to a file, and asks an AI to generate
 * context profiles, saving them to .eck/profiles.json.
 */
export async function detectProfiles(repoPath, options) {
  const spinner = ora('Initializing and scanning project structure...').start();
  try {
    await initializeEckManifest(repoPath);

    const setupConfig = await loadSetupConfig();
    const userConfig = await loadConfig(options.config);
    const config = {
        ...userConfig,
        ...setupConfig.fileFiltering,
        ...setupConfig.performance
    };

    const allFiles = await scanDirectoryRecursively(repoPath, config, repoPath);
    spinner.text = 'Generating directory tree...';
    const dirTree = await generateDirectoryTree(repoPath, '', allFiles, 0, config.maxDepth, config);

    if (!dirTree) {
        throw new Error('Failed to generate directory tree or project is empty.');
    }

    spinner.text = 'Saving directory tree to file...';
    const treeFilePath = path.join(repoPath, '.eck', 'directory_tree_for_profiling.md');
    await fs.writeFile(treeFilePath, dirTree);

    const prompt = `You are a code architect. Based on the file directory tree found in the file at './.eck/directory_tree_for_profiling.md', please identify logical 'context profiles' for splitting the project.
Your output MUST be ONLY a valid JSON object.
The keys of the object MUST be the profile names (e.g., 'frontend', 'backend', 'core-logic', 'docs').
The values MUST be an object containing 'include' and 'exclude' arrays of glob patterns.
Example: {"frontend": {"include": ["packages/ui/**"], "exclude": []}, "docs": {"include": ["docs/**"], "exclude": []}}.
DO NOT add any conversational text, introductory sentences, or explanations. Your entire response must be ONLY the JSON object.`;

    spinner.text = 'Asking AI to analyze directory tree and detect profiles...';
    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;

    if (!rawText || typeof rawText.replace !== 'function') {
      throw new Error(`AI returned invalid content type: ${typeof rawText}`);
    }

    spinner.text = 'Saving generated profiles...';
    const cleanedJson = extractJson(rawText);
    let parsedProfiles;
    try {
        parsedProfiles = JSON.parse(cleanedJson);
    } catch (e) {
        console.error('\nInvalid JSON received from AI:', cleanedJson);
        throw new Error(`AI returned invalid JSON: ${e.message}`);
    }

    const outputPath = path.join(repoPath, '.eck', 'profiles.json');
    await fs.writeFile(outputPath, JSON.stringify(parsedProfiles, null, 2));

    const profileKeys = Object.keys(parsedProfiles);
    spinner.succeed(`Successfully detected and saved ${profileKeys.length} profiles to ${outputPath}`);

    console.log('\n‚ú® Detected Profiles:');
    console.log('---------------------------');
    for (const profileName of profileKeys) {
        console.log(`  - ${profileName}`);
    }
    console.log('\nYou can now use these profile names with the --profile flag.');

  } catch (error) {
    spinner.fail(`Failed to detect profiles: ${error.message}`);
  }
}


--- File: /src/cli/commands/detectProject.js ---

import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { displayProjectInfo } from '../../utils/fileUtils.js';
import chalk from 'chalk';

/**
 * Command to detect and display project information
 * @param {string} projectPath - Path to the project
 * @param {object} options - Command options
 */
export async function detectProject(projectPath = '.', options = {}) {
  console.log(chalk.blue('üîç Detecting project type...\n'));
  
  try {
    // Detect project type
    const detection = await detectProjectType(projectPath);
    displayProjectInfo(detection);
    
    // Show filtering rules that would be applied
    if (detection.type !== 'unknown') {
      const filtering = await getProjectSpecificFiltering(detection.type);
      
      if (filtering.filesToIgnore.length > 0 || 
          filtering.dirsToIgnore.length > 0 || 
          filtering.extensionsToIgnore.length > 0) {
        console.log(chalk.yellow('üìã Project-specific filtering rules:'));
        
        if (filtering.filesToIgnore.length > 0) {
          console.log(`   Files to ignore: ${filtering.filesToIgnore.join(', ')}`);
        }
        
        if (filtering.dirsToIgnore.length > 0) {
          console.log(`   Directories to ignore: ${filtering.dirsToIgnore.join(', ')}`);
        }
        
        if (filtering.extensionsToIgnore.length > 0) {
          console.log(`   Extensions to ignore: ${filtering.extensionsToIgnore.join(', ')}`);
        }
        
        console.log('');
      }
    }
    
    // Show Android parsing info if it's an Android project
    if (detection.type === 'android') {
      console.log(chalk.green('ü§ñ Android parsing supported via unified segmenter'));
      console.log('');
    }
    
    // Show verbose details if requested
    if (options.verbose && detection.allDetections) {
      console.log(chalk.blue('üìä All detection results:'));
      for (const result of detection.allDetections) {
        console.log(`   ${result.type}: score ${result.score}, priority ${result.priority}`);
      }
      console.log('');
    }
    
    // Provide suggestions
    console.log(chalk.blue('üí° Suggested commands:'));
    
    if (detection.type === 'android') {
      console.log('   eck-snapshot snapshot --profile android-core    # Core Android files');
      console.log('   eck-snapshot snapshot --profile android-config  # Build configuration');
      console.log('   eck-snapshot index                              # For large projects');
    } else if (detection.type === 'nodejs') {
      console.log('   eck-snapshot snapshot --profile backend         # Backend code');
      console.log('   eck-snapshot snapshot --profile frontend        # Frontend code');
      console.log('   eck-snapshot index                              # For large projects');
    } else {
      console.log('   eck-snapshot snapshot                           # Full project snapshot');
      console.log('   eck-snapshot index                              # For semantic search');
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error detecting project:'), error.message);
    process.exit(1);
  }
}

/**
 * Command to test file parsing using the unified segmenter
 * @param {string} filePath - Path to the file to test
 * @param {object} options - Command options
 */
export async function testFileParsing(filePath, options = {}) {
  console.log(chalk.blue(`üß™ Testing file parsing: ${filePath}\n`));
  
  try {
    const { segmentFile } = await import('../../core/segmenter.js');
    const fs = await import('fs/promises');
    
    // Read file content
    const content = await fs.readFile(filePath, 'utf-8');
    console.log(chalk.blue(`üìÑ File size: ${content.length} characters`));
    
    // Parse file using unified segmenter
    const chunks = await segmentFile(filePath);
    
    console.log(chalk.green(`\nüéØ Extracted ${chunks.length} chunks:`));
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      console.log(`\n${i + 1}. ${chalk.yellow(chunk.chunk_name)} (${chunk.chunk_type})`);
      
      if (options.showContent) {
        const preview = chunk.code.substring(0, 200);
        console.log(chalk.gray(`   Content preview: ${preview}${chunk.code.length > 200 ? '...' : ''}`));
      }
    }
    
  } catch (error) {
    console.error(chalk.red('‚ùå Error parsing file:'), error.message);
    process.exit(1);
  }
}

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    let files = await getProjectFiles(projectPath);
    
    // --- Apply Advanced Profile Filtering ---
    const defaultProfile = await getProfile('default', projectPath);
    if (options.profile) {
        mainSpinner.text = `Applying profile filter: '${options.profile}'...`;
        files = await applyProfileFilter(files, options.profile, projectPath);
        mainSpinner.info(`Filtered down to ${files.length} files using profile: '${options.profile}'.`);
    } else if (defaultProfile) {
        mainSpinner.text = "Applying detected 'default' profile...";
        files = micromatch(files, defaultProfile.include, { ignore: defaultProfile.exclude });
        mainSpinner.info(`Filtered down to ${files.length} files using detected 'default' profile.`);
    }
    if (files.length === 0) {
        throw new Error(`Profile filter resulted in 0 files. Aborting.`);
    }
    // --- End Profile Filtering ---

    const profileName = options.profile || 'default';
    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await releaseAnalysisModel();
    await releaseEmbeddingModel();
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}


--- File: /src/cli/commands/pruneSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import ora from 'ora';
import { dispatchAnalysisTask } from '../../services/dispatcherService.js';
import { parseSnapshotContent, parseSize, formatSize } from '../../utils/fileUtils.js';

function extractJson(text) {
  const match = text.match(/```(json)?([\s\S]*?)```/);
  if (match && match[2]) {
    return match[2].trim();
  }
  const firstBracket = text.indexOf('[');
  const lastBracket = text.lastIndexOf(']');
  if (firstBracket !== -1 && lastBracket !== -1 && lastBracket > firstBracket) {
    return text.substring(firstBracket, lastBracket + 1).trim();
  }
  return text.trim();
}

export async function pruneSnapshot(snapshotFile, options) {
  const spinner = ora('Starting snapshot pruning process...').start();
  try {
    const targetSize = parseSize(options.targetSize);
    spinner.text = `Reading snapshot file: ${snapshotFile}`;
    const snapshotContent = await fs.readFile(snapshotFile, 'utf-8');
    const snapshotHeader = snapshotContent.split('--- File: /')[0];
    const files = parseSnapshotContent(snapshotContent);

    if (files.length === 0) {
      spinner.warn('No files found in the snapshot.');
      return;
    }

    const currentSize = Buffer.byteLength(snapshotContent, 'utf-8');
    if (currentSize <= targetSize) {
      spinner.succeed(`Snapshot is already smaller than the target size. (${formatSize(currentSize)} < ${formatSize(targetSize)})`);
      return;
    }

    spinner.text = 'Asking AI to rank files by importance...';
    const filePaths = files.map(f => f.path);
    const prompt = `You are a software architect. Given the following list of file paths from a project snapshot, rank them by importance for understanding the project's core functionality. The most critical files (e.g., entry points, core logic, configurations) should be first. Your output MUST be ONLY a JSON array of strings, with the file paths in ranked order. Do not add any other text.\n\nFILE LIST:\n${JSON.stringify(filePaths, null, 2)}`;

    const aiResponseObject = await dispatchAnalysisTask(prompt);
    const rawText = aiResponseObject.result || aiResponseObject.response_text;
    const cleanedJson = extractJson(rawText);

    let rankedFiles;
    try {
      rankedFiles = JSON.parse(cleanedJson);
      if (!Array.isArray(rankedFiles) || rankedFiles.some(item => typeof item !== 'string')) {
        throw new Error('AI response is not an array of strings.');
      }
    } catch (e) {
      spinner.fail(`Failed to parse AI's file ranking: ${e.message}`);
      console.error('Received from AI:', cleanedJson);
      return;
    }

    spinner.text = 'Building pruned snapshot...';
    const fileMap = new Map(files.map(f => [f.path, f.content]));
    let newSnapshotContent = snapshotHeader;
    let newSize = Buffer.byteLength(newSnapshotContent, 'utf-8');
    let filesIncluded = 0;

    for (const filePath of rankedFiles) {
      if (fileMap.has(filePath)) {
        const fileContent = fileMap.get(filePath);
        const fileEntry = `--- File: /${filePath} ---\n\n${fileContent}\n\n`;
        const entrySize = Buffer.byteLength(fileEntry, 'utf-8');

        if (newSize + entrySize > targetSize) {
          break;
        }

        newSnapshotContent += fileEntry;
        newSize += entrySize;
        filesIncluded++;
      }
    }

    const outputFilename = `${path.basename(snapshotFile, path.extname(snapshotFile))}_pruned_${options.targetSize}${path.extname(snapshotFile)}`;
    const outputPath = path.join(path.dirname(snapshotFile), outputFilename);

    await fs.writeFile(outputPath, newSnapshotContent);

    spinner.succeed('Snapshot pruning complete!');
    console.log(`- Original Size: ${formatSize(currentSize)}`);
    console.log(`- New Size: ${formatSize(newSize)}`);
    console.log(`- Files Included: ${filesIncluded} / ${files.length}`);
    console.log(`- Pruned snapshot saved to: ${outputPath}`);

  } catch (error) {
    spinner.fail(`An error occurred during pruning: ${error.message}`);
  }
}


--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `--- File: /${relativePath} ---\n\n`;
        snapshotContent += codeSnippets.join('\n\n---\n\n');
        snapshotContent += '\n\n';
    }

    const sanitizedQuery = sanitizeForFilename(query);
    const outputFilename = options.output || `rag_snapshot_${sanitizedQuery}.md`;
    await fs.writeFile(outputFilename, snapshotContent);

    mainSpinner.succeed(`RAG-—Å–Ω–∞–ø—à–æ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: ${outputFilename}`);

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

export async function viewIndex(options) {
  const spinner = ora('Connecting to database...').start();
  const knex = getKnex();

  try {
    await initDb();

    spinner.text = 'Fetching code chunks from database...';

    // Build query with optional filters
    let query = knex('code_chunks')
      .select('id', 'file_path', 'chunk_type', 'chunk_name', 'profile')
      .orderBy('id', 'asc');

    // Apply file filter if specified
    if (options.file) {
      query = query.where('file_path', 'like', `%${options.file}%`);
      spinner.info(`Filtering by file path: ${options.file}`);
    }

    // Apply pagination
    if (options.limit) {
      query = query.limit(options.limit);
    }
    if (options.offset) {
      query = query.offset(options.offset);
    }

    const chunks = await query;

    if (chunks.length === 0) {
      spinner.warn('No code chunks found in the database.');
      return;
    }

    spinner.succeed(`Found ${chunks.length} code chunks`);

    // Display results in a formatted table
    console.log('\nüìä Code Chunks Index:');
    console.log('‚ïê'.repeat(100));
    console.table(chunks.map(chunk => ({
      ID: chunk.id,
      'File Path': chunk.file_path.replace(process.cwd(), '.'),
      Type: chunk.chunk_type,
      Name: chunk.chunk_name,
      Profile: chunk.profile || 'default'
    })));

    // Show summary
    const totalCount = await knex('code_chunks').count('* as count').first();
    console.log(`\nShowing ${chunks.length} of ${totalCount.count} total chunks`);

    if (options.limit && chunks.length === options.limit) {
      console.log(`\nüí° Use --offset ${(options.offset || 0) + options.limit} to view the next page`);
    }

  } catch (error) {
    spinner.fail(`Failed to view index: ${error.message}`);
  } finally {
    await destroyDb();
  }
}

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}

--- File: /src/cli/commands/setupGemini.js ---

import which from 'which';
import path from 'path';
import fs from 'fs/promises';
import os from 'os';
import chalk from 'chalk';

/**
 * Sets up claude.toml configuration for gemini-cli integration with dynamic paths
 * @param {Object} options - Command options
 */
export async function setupGemini(options = {}) {
  try {
    console.log(chalk.blue('üîß Setting up gemini-cli integration with dynamic paths...'));

    // Check if gemini-cli is installed
    let geminiCliPath;
    try {
      geminiCliPath = await which('gemini-cli');
      console.log(chalk.green(`‚úÖ Found gemini-cli at: ${geminiCliPath}`));
    } catch (error) {
      console.error(chalk.red('‚ùå gemini-cli not found in PATH'));
      console.log(chalk.yellow('üí° Please install gemini-cli first:'));
      console.log(chalk.cyan('   npm install -g gemini-cli'));
      process.exit(1);
    }

    // Get current working directory for dynamic path resolution
    const currentDir = process.cwd();
    const indexJsPath = path.join(currentDir, 'index.js');

    // Verify index.js exists
    try {
      await fs.access(indexJsPath);
      console.log(chalk.green(`‚úÖ Found eck-snapshot index.js at: ${indexJsPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Could not find index.js at: ${indexJsPath}`));
      console.log(chalk.yellow('üí° Make sure you are running this command from the eck-snapshot project directory'));
      process.exit(1);
    }

    // Create gemini tools directory
    const homeDir = os.homedir();
    const geminiToolsDir = path.join(homeDir, '.gemini', 'tools');

    try {
      await fs.mkdir(geminiToolsDir, { recursive: true });
      console.log(chalk.green(`‚úÖ Created/verified gemini tools directory: ${geminiToolsDir}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to create gemini tools directory: ${error.message}`));
      process.exit(1);
    }

    // Read environment variables from setup.json if available
    let envVars = {};
    try {
      const setupJsonPath = path.join(currentDir, 'setup.json');
      const setupContent = await fs.readFile(setupJsonPath, 'utf-8');
      const setupData = JSON.parse(setupContent);

      // Extract relevant environment variables
      if (setupData.environmentDetection) {
        envVars.ECK_SNAPSHOT_PATH = currentDir;
        console.log(chalk.blue(`üìã Using project context from setup.json`));
      }
    } catch (error) {
      console.log(chalk.yellow('‚ö†Ô∏è  setup.json not found or invalid, using defaults'));
    }

    // Generate claude.toml content with dynamic paths
    const claudeTomlContent = generateClaudeToml(indexJsPath, envVars);

    // Write claude.toml file
    const claudeTomlPath = path.join(geminiToolsDir, 'claude.toml');
    try {
      await fs.writeFile(claudeTomlPath, claudeTomlContent, 'utf-8');
      console.log(chalk.green(`‚úÖ Generated claude.toml at: ${claudeTomlPath}`));
    } catch (error) {
      console.error(chalk.red(`‚ùå Failed to write claude.toml: ${error.message}`));
      process.exit(1);
    }

    // Success summary
    console.log(chalk.green('\nüéâ Setup completed successfully!'));
    console.log(chalk.blue('\nüìã Configuration summary:'));
    console.log(chalk.cyan(`   ‚Ä¢ gemini-cli: ${geminiCliPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ eck-snapshot: ${indexJsPath}`));
    console.log(chalk.cyan(`   ‚Ä¢ claude.toml: ${claudeTomlPath}`));

    if (Object.keys(envVars).length > 0) {
      console.log(chalk.cyan(`   ‚Ä¢ Environment variables: ${Object.keys(envVars).join(', ')}`));
    }

    console.log(chalk.blue('\nüöÄ You can now use:'));
    console.log(chalk.cyan('   gemini-cli claude "Your prompt here"'));
    console.log(chalk.green('\n‚ú® Cross-platform path resolution is automatically handled!'));

  } catch (error) {
    console.error(chalk.red(`‚ùå Setup failed: ${error.message}`));
    if (options.verbose) {
      console.error(chalk.red('Stack trace:'), error.stack);
    }
    process.exit(1);
  }
}

/**
 * Generates claude.toml content with dynamic paths
 * @param {string} indexJsPath - Path to eck-snapshot index.js
 * @param {Object} envVars - Environment variables to include
 * @returns {string} - Generated TOML content
 */
function generateClaudeToml(indexJsPath, envVars = {}) {
  const envSection = Object.keys(envVars).length > 0
    ? `# Environment variables from setup.json
${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}

`
    : '';

  return `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'
# This file uses dynamic paths to work across WSL/Windows environments

${envSection}[claude]
# eck-snapshot integration for AI-powered repository analysis
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

# Command examples:
# gemini-cli claude "Create a snapshot of the current project"
# gemini-cli claude "Analyze the database structure"
# gemini-cli claude "Generate a project overview"

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
generated_at = "${new Date().toISOString()}"
platform = "${process.platform}"
node_version = "${process.version}"
working_directory = "${path.dirname(indexJsPath)}"

# Cross-platform compatibility notes:
# - Paths are automatically resolved using process.cwd()
# - Works in WSL, Windows, macOS, and Linux
# - No hardcoded /mnt/c/ paths required
`;
}

--- File: /src/cli/commands/setupGemini.test.js ---

import { describe, it, expect } from 'vitest';

describe('setupGemini integration', () => {
  it('should validate path resolution logic', () => {
    // Test path join functionality that setupGemini uses
    const currentDir = '/test/project';
    const indexJsPath = `${currentDir}/index.js`;

    expect(indexJsPath).toBe('/test/project/index.js');
    expect(indexJsPath).toContain('index.js');
  });

  it('should validate gemini tools directory structure', () => {
    const homeDir = '/home/user';
    const geminiToolsDir = `${homeDir}/.gemini/tools`;
    const claudeTomlPath = `${geminiToolsDir}/claude.toml`;

    expect(geminiToolsDir).toBe('/home/user/.gemini/tools');
    expect(claudeTomlPath).toBe('/home/user/.gemini/tools/claude.toml');
  });

  it('should validate TOML content structure', () => {
    const indexJsPath = '/test/project/index.js';
    const envVars = { ECK_SNAPSHOT_PATH: '/test/project' };

    // Test environment section generation
    const envSection = Object.keys(envVars).length > 0
      ? `# Environment variables from setup.json\n${Object.entries(envVars).map(([key, value]) => `${key} = "${value}"`).join('\n')}\n\n`
      : '';

    // Test main TOML structure
    const tomlContent = `# Claude.toml - Dynamic configuration for eck-snapshot integration
# Generated automatically by 'eck-snapshot setup-gemini'

${envSection}[claude]
name = "eck-snapshot"
description = "AI-powered repository snapshot and analysis tool with cross-platform support"
command = "node"
args = ["${indexJsPath}", "ask-claude"]

[claude.metadata]
version = "4.0.0"
author = "eck-snapshot"
platform = "${process.platform}"
working_directory = "${indexJsPath.replace('/index.js', '')}"`;

    expect(tomlContent).toContain('[claude]');
    expect(tomlContent).toContain('name = "eck-snapshot"');
    expect(tomlContent).toContain(`args = ["${indexJsPath}", "ask-claude"]`);
    expect(tomlContent).toContain('[claude.metadata]');
    expect(tomlContent).toContain('ECK_SNAPSHOT_PATH = "/test/project"');
  });

  it('should handle cross-platform paths correctly', () => {
    const testPaths = [
      { platform: 'windows', path: 'C:\\Users\\test\\project\\index.js' },
      { platform: 'unix', path: '/home/user/project/index.js' },
      { platform: 'wsl', path: '/mnt/c/Users/test/project/index.js' }
    ];

    testPaths.forEach(({ platform, path }) => {
      expect(path).toContain('index.js');
      expect(path.length).toBeGreaterThan(0);

      // Test that the path is absolute (platform-appropriate)
      if (platform === 'windows') {
        expect(path).toMatch(/^[A-Z]:\\/);
      } else {
        expect(path).toMatch(/^\//);
      }
    });
  });

  it('should validate error handling patterns', () => {
    // Test error message patterns that setupGemini should handle
    const errorPatterns = [
      'gemini-cli not found in PATH',
      'Could not find index.js',
      'Failed to create gemini tools directory',
      'Failed to write claude.toml'
    ];

    errorPatterns.forEach(pattern => {
      expect(pattern).toBeDefined();
      expect(typeof pattern).toBe('string');
      expect(pattern.length).toBeGreaterThan(0);
    });
  });

  it('should test JSON parsing for setup.json', () => {
    const validSetupData = {
      environmentDetection: {
        detected: true
      }
    };

    const jsonString = JSON.stringify(validSetupData);
    const parsed = JSON.parse(jsonString);

    expect(parsed.environmentDetection).toBeDefined();
    expect(parsed.environmentDetection.detected).toBe(true);

    // Test invalid JSON handling pattern
    const invalidJson = 'invalid json {';
    let parseError = null;
    try {
      JSON.parse(invalidJson);
    } catch (e) {
      parseError = e;
    }

    expect(parseError).toBeDefined();
    expect(parseError.message).toContain('JSON');
  });
});

--- File: /src/cli/commands/trainTokens.js ---

import { addTrainingPoint, showEstimationStats } from '../../utils/tokenEstimator.js';

/**
 * Train token estimation with actual results
 * @param {string} projectType - Type of project (android, nodejs, etc.)
 * @param {string} fileSizeStr - File size in bytes
 * @param {string} estimatedStr - Estimated tokens
 * @param {string} actualStr - Actual tokens (from user input)
 */
export async function trainTokens(projectType, fileSizeStr, estimatedStr, actualStr) {
  try {
    const fileSizeInBytes = parseInt(fileSizeStr, 10);
    const estimatedTokens = parseInt(estimatedStr, 10);
    
    // Parse actual tokens from user input (remove any text like "tokens", commas, etc.)
    const actualTokens = parseInt(actualStr.replace(/[^\d]/g, ''), 10);
    
    if (isNaN(fileSizeInBytes) || isNaN(estimatedTokens) || isNaN(actualTokens)) {
      throw new Error('Invalid numeric values provided');
    }
    
    await addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens);
    
    console.log('\nüìà Updated polynomial coefficients for improved estimation.');
    
  } catch (error) {
    console.error(`‚ùå Error training token estimation: ${error.message}`);
    console.error('Usage: eck-snapshot train-tokens <project-type> <file-size-bytes> <estimated-tokens> <actual-tokens>');
    process.exit(1);
  }
}

/**
 * Show token estimation statistics
 */
export async function showTokenStats() {
  await showEstimationStats();
}

--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/core/segmenter.js ---

import { parse } from '@babel/parser';
import _traverse from '@babel/traverse';
const traverse = _traverse.default;
import fs from 'fs/promises';
import path from 'path';
import crypto from 'crypto';
import Parser from 'tree-sitter';
import Python from 'tree-sitter-python';
import Java from 'tree-sitter-java';
import Kotlin from 'tree-sitter-kotlin';
import C from 'tree-sitter-c';

function generateHash(content) {
  return crypto.createHash('sha256').update(content).digest('hex');
}

const tsParser = new Parser();
const languageParsers = {
    '.py': Python,
    '.java': Java,
    '.kt': Kotlin,
    '.c': C,
    '.h': C,
};

async function _segmentWithTreeSitter(content, filePath, language) {
    tsParser.setLanguage(language);
    const tree = tsParser.parse(content);
    const chunks = [];
    // Graph relations for tree-sitter are not implemented in this step.
    const relations = [];

    function walk(node) {
        const nodeTypeMap = {
            'function_definition': 'function', 'class_definition': 'class', // Python
            'function_declaration': 'function', 'class_declaration': 'class', // Kotlin/Java
            'method_declaration': 'function', // Java
            'struct_specifier': 'struct', 'enum_specifier': 'enum', 'union_specifier': 'union', 'type_definition': 'typedef', // C
        };

        if (nodeTypeMap[node.type]) {
            const nameNode = node.childForFieldName('name') || node.child(1);
            const chunkName = nameNode ? nameNode.text : 'anonymous';
            const chunkCode = node.text;
            chunks.push({
                filePath,
                chunk_type: nodeTypeMap[node.type],
                chunk_name: chunkName,
                code: chunkCode,
                contentHash: generateHash(chunkCode)
            });
        }
        node.children.forEach(walk);
    }
    walk(tree.rootNode);
    return { chunks, relations };
}

async function _segmentJavaScript(content, filePath) {
    const chunks = [];
    const relations = [];

    try {
        const ast = parse(content, { sourceType: 'module', plugins: ['typescript', 'jsx'], errorRecovery: true });

        const getChunkData = (node) => {
            const chunkName = node.id ? node.id.name : 'anonymous';
            const chunkCode = content.substring(node.start, node.end);
            return { filePath, chunk_name: chunkName, code: chunkCode, contentHash: generateHash(chunkCode) };
        };

        traverse(ast, {
            enter(path) {
                let currentScopeName = 'file';
                const parentFunction = path.findParent((p) => p.isFunctionDeclaration() || p.isClassDeclaration());
                if (parentFunction && parentFunction.node.id) {
                    currentScopeName = parentFunction.node.id.name;
                }

                if (path.isFunctionDeclaration() || path.isClassDeclaration()) {
                    chunks.push({ ...getChunkData(path.node), chunk_type: path.isClassDeclaration() ? 'class' : 'function' });
                }

                if (path.isImportDeclaration()) {
                    const sourceFile = path.node.source.value;
                    relations.push({ from: filePath, to: sourceFile, type: 'IMPORTS' });
                }

                if (path.isCallExpression()) {
                    const calleeName = path.get('callee').toString();
                    relations.push({ from: currentScopeName, to: calleeName, type: 'CALLS' });
                }
            }
        });
    } catch (e) {
        console.error(`Babel parsing error in ${filePath}: ${e.message}`);
    }
    return { chunks, relations };
}

export async function segmentFile(filePath) {
    try {
        const content = await fs.readFile(filePath, 'utf-8');
        const extension = path.extname(filePath);
        let result = { chunks: [], relations: [] };

        if (['.js', '.jsx', '.ts', '.tsx'].includes(extension)) {
            result = await _segmentJavaScript(content, filePath);
        } else if (languageParsers[extension]) {
            result = await _segmentWithTreeSitter(content, filePath, languageParsers[extension]);
        }
        
        // Fallback: if no specific chunks, treat the whole file as one
        if (result.chunks.length === 0) {
            const code = content;
            result.chunks.push({ filePath, chunk_type: 'file', chunk_name: path.basename(filePath), code, contentHash: generateHash(code) });
        }

        return result;
    } catch (error) {
        console.error(`Failed to segment file ${filePath}: ${error.message}`);
        return { chunks: [], relations: [] };
    }
}

--- File: /src/database/postgresConnector.js ---

import knex from 'knex';
import fs from 'fs/promises';
import path from 'path';
import config from '../../knexfile.js';
import { fileURLToPath } from 'url';
const __dirname = path.dirname(fileURLToPath(import.meta.url));

let knexInstance = null;

function getKnex() {
  if (!knexInstance) {
    console.log('Initializing Knex connection...');
    knexInstance = knex(config.development);
  }
  return knexInstance;
}

async function initDb() {
  const db = getKnex();
  try {
    console.log('Checking database connection...');
    await db.raw('SELECT 1+1 AS result');
    console.log('Connection successful.');

    console.log('Applying database schema...');
    // Try full schema first, fallback to simple schema
    try {
      const schemaPath = path.join(__dirname, 'schema.sql');
      const schemaSQL = await fs.readFile(schemaPath, 'utf-8');
      await db.raw(schemaSQL);
      console.log('Full schema with vector extensions applied successfully.');
    } catch (error) {
      console.log('Vector extensions not available, using simplified schema...');
      const simpleSchemaPath = path.join(__dirname, 'schema_simple.sql');
      const simpleSchemaSQL = await fs.readFile(simpleSchemaPath, 'utf-8');
      await db.raw(simpleSchemaSQL);
      console.log('Simplified schema applied successfully.');
    }
  } catch (error) {
    console.error('Error initializing database:', error.message);
    throw error;
  }
}

async function destroyDb() {
  if (knexInstance) {
    console.log('Destroying Knex connection pool...');
    await knexInstance.destroy();
    knexInstance = null;
  }
}

export {
  getKnex,
  initDb,
  destroyDb,
};

--- File: /src/database/schema.sql ---

-- –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE EXTENSION IF NOT EXISTS vector;

-- –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –¥–ª—è –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
-- (–ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ Apache AGE —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –≤–∞—à–µ–π –≤–µ—Ä—Å–∏–∏ PG)
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ (–µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç)
SELECT create_graph('eck_snapshot_graph');

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞ (—É–∑–ª–æ–≤ –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding VECTOR(768), -- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è Jina Code v2
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π (—Ä–µ–±–µ—Ä –≥—Ä–∞—Ñ–∞)
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

-- –°–æ–∑–¥–∞–µ–º HNSW-–∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX IF NOT EXISTS code_chunks_embedding_idx ON code_chunks USING HNSW (embedding vector_cosine_ops);

--- File: /src/database/schema_simple.sql ---

-- Simplified schema without vector and graph extensions for testing

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞
CREATE TABLE IF NOT EXISTS code_chunks (
    id SERIAL PRIMARY KEY,
    file_path TEXT NOT NULL,
    chunk_type VARCHAR(50) NOT NULL, -- 'function', 'class', 'file'
    chunk_name TEXT,
    code TEXT NOT NULL,
    summary TEXT, -- –°—é–¥–∞ –±—É–¥–µ—Ç –ø–∏—Å–∞—Ç—å "–ê–Ω–∞–ª–∏—Ç–∏–∫ –∫–æ–¥–∞"
    tokens INT,
    embedding TEXT, -- JSON string representation for now
    content_hash TEXT NOT NULL UNIQUE, -- –î–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
    profile VARCHAR(100) -- –ü—Ä–æ—Ñ–∏–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
);

-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≤—è–∑–µ–π
CREATE TABLE IF NOT EXISTS relations (
    id SERIAL PRIMARY KEY,
    from_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    to_id INT REFERENCES code_chunks(id) ON DELETE CASCADE,
    relation_type VARCHAR(50) NOT NULL -- 'imports', 'calls'
);

--- File: /src/services/analysisService.js ---

import { pipeline } from '@xenova/transformers';

class AnalysisService {
    static instance = null;
    static modelName = 'Xenova/distilgpt2'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = await pipeline('text-generation', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∞–Ω–∞–ª–∏—Ç–∏–∫ –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: ${this.modelName}...`);
            this.instance = null;
        }
    }
}

export async function getCodeSummary(codeChunk) {
    const generator = await AnalysisService.getInstance();

    const prompt = `This code:\n${codeChunk.substring(0, 150)}\nSummary:`;

    const output = await generator(prompt, {
        max_new_tokens: 50,
        temperature: 0.7,
        do_sample: true
    });

    const generatedText = output[0].generated_text;
    const summary = generatedText.replace(prompt, '').trim() || 'Auto-generated description';
    return summary.substring(0, 200); // Limit summary length
}

export const releaseModel = AnalysisService.releaseModel;

--- File: /src/services/authService.js ---

import ora from 'ora';
import { execa } from 'execa';

/**
 * Initiates the interactive login flow by spawning 'codex login'.
 * This will open a browser and wait for the user to complete authentication.
 * @returns {Promise<void>}
 */
export async function initiateLogin() {
  const spinner = ora('Authentication required. Please follow the browser instructions.').start();
  try {
    // Run `codex login` interactively, inheriting stdio to show user instructions.
    await execa('codex', ['login'], { stdio: 'inherit' });
    spinner.succeed('Login successful. Retrying original command...');
  } catch (e) {
    spinner.fail('Login process failed or was cancelled.');
    // Re-throw to notify p-retry that the attempt failed.
    throw new Error(`Login failed: ${e.message}`);
  }
}

--- File: /src/services/claudeCliService.js ---

import { execa } from 'execa';
import { spawn } from 'child_process';
import pRetry from 'p-retry';

/**
 * Executes a prompt using the claude-code CLI in non-interactive print mode.
 * @param {string} prompt The prompt to send to Claude.
 * @param {boolean} continueConversation Whether to continue the last conversation with -c flag.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePrompt(prompt, continueConversation = false) {
  try {
    // Ensure the log directory exists
    try {
      await import('fs/promises').then(fs => fs.mkdir('./.eck/logs', { recursive: true }));
    } catch (e) {
      console.error(`Failed to create log directory: ${e.message}`);
      // Do not block execution if log dir creation fails, just warn
    }
    let sessionId = null;
    if (continueConversation) {
      sessionId = await getLastSessionId();
      if (!sessionId) {
        console.warn('No previous session found, starting new conversation');
      } else {
        console.log(`Continuing conversation with session: ${sessionId}`);
      }
    }

    return await attemptClaudeExecution(prompt, sessionId);
  } catch (error) {
    // Check for claude session limits first
    if (isSessionLimitError(error)) {
      await logSessionLimitError(error, prompt);
      throw new Error(`Claude session limit reached: ${error.message}. Please take a break and try again later.`);
    }
    
    // If the first attempt fails (timeout, interactive prompts, etc), try to handle it
    if (error.message.includes('timeout') || error.message.includes('SIGTERM')) {
      console.log('First attempt failed, attempting interactive recovery...');
      
      try {
        // Try running claude interactively to see what prompts appear
        const interactiveResult = await execa('claude', [], {
          input: '\n',
          timeout: 10000,
          stdio: ['pipe', 'pipe', 'pipe']
        });
        
        // –õ–æ–≥–∏—Ä—É–µ–º –ª—é–±–æ–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ
        const interactiveLogFile = `./.eck/logs/claude-interactive-${Date.now()}.log`;
        const interactiveLogContent = `=== Claude Interactive Recovery Log ${new Date().toISOString()} ===\n` +
                                     `Original prompt: "${prompt}"\n` +
                                     `Original error: ${error.message}\n` +
                                     `Recovery command: claude (with newline input)\n` +
                                     `STDOUT:\n${interactiveResult.stdout}\n` +
                                     `STDERR:\n${interactiveResult.stderr}\n` +
                                     `=== End Interactive Log ===\n\n`;
        
        await import('fs/promises').then(fs => fs.appendFile(interactiveLogFile, interactiveLogContent, 'utf8'));
        console.log(`Interactive recovery logged to: ${interactiveLogFile}`);
        
        // Wait a moment for any setup to be processed
        await new Promise(resolve => setTimeout(resolve, 1000));
        
        // Now try the original prompt again
        return await attemptClaudeExecution(prompt, sessionId);
      } catch (retryError) {
        // –õ–æ–≥–∏—Ä—É–µ–º –Ω–µ—É–¥–∞—á—É –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        const failureLogFile = `./.eck/logs/claude-recovery-failure-${Date.now()}.log`;
        const failureLogContent = `=== Claude Recovery Failure Log ${new Date().toISOString()} ===\n` +
                                 `Original prompt: "${prompt}"\n` +
                                 `Original error: ${error.message}\n` +
                                 `Retry error: ${retryError.message}\n` +
                                 `Retry stack: ${retryError.stack}\n` +
                                 `=== End Failure Log ===\n\n`;
        
        try {
          await import('fs/promises').then(fs => fs.appendFile(failureLogFile, failureLogContent, 'utf8'));
          console.log(`Recovery failure logged to: ${failureLogFile}`);
        } catch (logError) {
          console.error('Failed to log recovery failure:', logError.message);
        }
        
        console.error('Recovery attempt failed:', retryError.message);
        throw new Error(`Failed to execute claude command even after interactive recovery. Original error: ${error.message}, Retry error: ${retryError.message}`);
      }
    }
    
    throw error;
  }
}

/**
 * Attempts to execute a claude command and parse the JSON output.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<object>} The parsed result object.
 */
async function attemptClaudeExecution(prompt, sessionId = null) {
  const timestamp = new Date().toISOString();
  const logFile = `./.eck/logs/claude-execution-${Date.now()}.log`;
  
  try {
    // Use spawn instead of execa for better control over streaming and timeouts
    const result = await executeClaudeWithDynamicTimeout(prompt, sessionId);
    const { stdout, stderr } = result;

    // –õ–æ–≥–∏—Ä—É–µ–º –≤–µ—Å—å –≤—ã–≤–æ–¥ –≤ —Ñ–∞–π–ª
    const commandStr = sessionId ? 
      `claude "${prompt}" --resume ${sessionId} -p --output-format=stream-json --verbose` :
      `claude "${prompt}" -p --output-format=stream-json --verbose`;
    const logContent = `=== Claude Execution Log ${timestamp} ===\n` +
                       `Command: ${commandStr}\n` +
                       `STDOUT:\n${stdout}\n` +
                       `STDERR:\n${stderr}\n` +
                       `=== End Log ===\n\n`;
    
    await import('fs/promises').then(fs => fs.appendFile(logFile, logContent, 'utf8'));
    console.log(`Claude execution logged to: ${logFile}`);

    if (stderr) {
      console.warn('Warning from claude-code process:', stderr);
    }

    const lines = stdout.trim().split('\n');
    
    // Find the final result JSON object
    let resultJson = null;
    for (const line of lines) {
      try {
        const parsed = JSON.parse(line);
        if (parsed.type === 'result') {
          resultJson = parsed;
        }
      } catch (e) {
        // Skip invalid JSON lines
        continue;
      }
    }

    if (!resultJson) {
      throw new Error('No result JSON found in claude-code output.');
    }

    return {
      result: resultJson.result,
      cost: resultJson.total_cost_usd,
      usage: resultJson.usage,
      duration_ms: resultJson.duration_ms
    };
  } catch (error) {
    // –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —Ç–æ–∂–µ
    const errorLogContent = `=== Claude Execution Error ${timestamp} ===\n` +
                           `Command: claude "${prompt}" -p --output-format=stream-json --verbose\n` +
                           `Error: ${error.message}\n` +
                           `Stack: ${error.stack}\n` +
                           `=== End Error Log ===\n\n`;
    
    try {
      await import('fs/promises').then(fs => fs.appendFile(logFile, errorLogContent, 'utf8'));
      console.log(`Claude execution error logged to: ${logFile}`);
    } catch (logError) {
      console.error('Failed to log error:', logError.message);
    }
    
    throw error;
  }
}

/**
 * Checks if the error is related to Claude session limits.
 * @param {Error} error The error to check.
 * @returns {boolean} True if it's a session limit error.
 */
function isSessionLimitError(error) {
  // Don't treat simple timeouts as session limits
  if (error.message.includes('Command timed out after') && 
      !error.message.includes('5-hour') && 
      !error.message.includes('limit')) {
    return false;
  }
  
  const limitPatterns = [
    /approaching 5-hour limit/i,
    /5-hour limit/i,
    /session limit reached/i,
    /daily limit reached/i,
    /usage limit reached/i,
    /rate limit exceeded/i,
    /quota exceeded/i,
    /too many requests/i,
    /maximum session duration/i,
    /session expired/i
  ];
  
  const errorText = error.message + ' ' + (error.stdout || '') + ' ' + (error.stderr || '');
  return limitPatterns.some(pattern => pattern.test(errorText));
}

/**
 * Logs session limit errors with helpful recommendations.
 * @param {Error} error The limit error.
 * @param {string} prompt The original prompt.
 */
async function logSessionLimitError(error, prompt) {
  const timestamp = new Date().toISOString();
  const currentTime = new Date();
  const limitLogFile = `./.eck/logs/claude-session-limit-${Date.now()}.log`;
  
  // Calculate suggested wait times based on error type
  const limitInfo = analyzeLimitType(error.message);
  const waitMinutes = limitInfo.suggestedWaitMinutes;
  const resumeTime = new Date(currentTime.getTime() + waitMinutes * 60000);
  
  const recommendations = [
    "üõë CLAUDE SESSION LIMIT REACHED",
    "",
    "üìã What happened:",
    `- Error: ${error.message}`,
    `- Prompt: "${prompt}"`,
    `- Time: ${timestamp}`,
    `- Limit type: ${limitInfo.type}`,
    limitInfo.extractedFromMessage ? `- Claude said available again at: ${limitInfo.exactEndTime}` : "",
    "",
    "‚è∞ Timing information:",
    `- Current time: ${currentTime.toLocaleString()}`,
    `- Suggested wait: ${waitMinutes} minutes`,
    `- Try again after: ${resumeTime.toLocaleString()}`,
    `- Resume at: ${resumeTime.toISOString()}`,
    limitInfo.extractedFromMessage ? "- ‚úÖ Time extracted directly from Claude's message" : "- ‚ö†Ô∏è Time estimated based on limit type",
    "",
    "üîÑ Recommended actions:",
    `1. Take a break for at least ${waitMinutes} minutes`,
    "2. Try again after the suggested time above",
    limitInfo.type === '5-hour' ? "3. Consider splitting work into shorter sessions (< 4 hours)" : "3. Monitor usage to avoid hitting limits again",
    "4. Check claude status page for any service issues",
    "",
    "‚ö° Prevention tips:",
    "- Use shorter, more focused prompts",
    "- Batch multiple questions efficiently", 
    "- Take regular breaks during long coding sessions",
    limitInfo.type === '5-hour' ? "- Set reminders to take breaks every 3-4 hours" : "",
    "",
    "üìä Full error details:"
  ].filter(line => line !== ""); // Remove empty strings
  
  const limitLogContent = recommendations.join('\n') + '\n' +
                         `STDOUT: ${error.stdout || 'N/A'}\n` +
                         `STDERR: ${error.stderr || 'N/A'}\n` +
                         `Stack: ${error.stack || 'N/A'}\n` +
                         `=== End Session Limit Log ===\n\n`;
  
  try {
    await import('fs/promises').then(fs => fs.appendFile(limitLogFile, limitLogContent, 'utf8'));
    console.log(`üõë Session limit error logged to: ${limitLogFile}`);
    console.log(`‚è∞ Recommendation: Take a break and try again later!`);
  } catch (logError) {
    console.error('Failed to log session limit error:', logError.message);
  }
}

/**
 * Analyzes the limit error message to determine wait time and type.
 * @param {string} errorMessage The error message to analyze.
 * @returns {{type: string, suggestedWaitMinutes: number}} Limit analysis results.
 */
function analyzeLimitType(errorMessage) {
  const message = errorMessage.toLowerCase();
  
  // Try to extract exact end time from claude's message
  const timePatterns = [
    /session will end at (\d{1,2}:\d{2})/i,
    /available again at (\d{1,2}:\d{2})/i,
    /try again after (\d{1,2}:\d{2})/i,
    /resume at (\d{1,2}:\d{2})/i,
    /until (\d{1,2}:\d{2})/i
  ];
  
  for (const pattern of timePatterns) {
    const match = errorMessage.match(pattern);
    if (match) {
      const timeString = match[1];
      const [hours, minutes] = timeString.split(':').map(Number);
      const now = new Date();
      const endTime = new Date();
      endTime.setHours(hours, minutes, 0, 0);
      
      // If end time is earlier than now, assume it's tomorrow
      if (endTime <= now) {
        endTime.setDate(endTime.getDate() + 1);
      }
      
      const waitMinutes = Math.ceil((endTime - now) / (1000 * 60));
      return {
        type: 'exact-time',
        suggestedWaitMinutes: Math.max(waitMinutes, 5), // At least 5 minutes
        exactEndTime: endTime.toLocaleString(),
        extractedFromMessage: true
      };
    }
  }
  
  if (message.includes('approaching 5-hour') || message.includes('5-hour limit')) {
    // 5-hour limit - suggest waiting 1 hour (limits usually reset within 1-2 hours)
    return {
      type: '5-hour',
      suggestedWaitMinutes: 60
    };
  }
  
  if (message.includes('daily limit') || message.includes('24-hour')) {
    // Daily limit - suggest waiting until next day
    const now = new Date();
    const tomorrow = new Date(now);
    tomorrow.setDate(tomorrow.getDate() + 1);
    tomorrow.setHours(0, 0, 0, 0); // Start of next day
    const minutesUntilMidnight = Math.ceil((tomorrow - now) / (1000 * 60));
    
    return {
      type: 'daily',
      suggestedWaitMinutes: Math.min(minutesUntilMidnight, 24 * 60) // Max 24 hours
    };
  }
  
  if (message.includes('rate limit') || message.includes('too many requests')) {
    // Rate limit - usually short, suggest 15-30 minutes
    return {
      type: 'rate-limit',
      suggestedWaitMinutes: 30
    };
  }
  
  if (message.includes('quota exceeded')) {
    // Quota limit - could be monthly, suggest checking billing/usage
    return {
      type: 'quota',
      suggestedWaitMinutes: 60
    };
  }
  
  // Default for unknown limit types
  return {
    type: 'unknown',
    suggestedWaitMinutes: 45
  };
}

/**
 * Extracts the last session_id from recent logs.
 * @returns {Promise<string|null>} The last session_id or null if not found.
 */
async function getLastSessionId() {
  try {
    const fs = await import('fs/promises');
    const path = await import('path');
    
    // Get all log files sorted by modification time (newest first)
    const logFiles = await fs.readdir('./.eck/logs');
    const executionLogs = logFiles
      .filter(file => file.startsWith('claude-execution-') && file.endsWith('.log'))
      .map(file => ({
        name: file,
        path: `./.eck/logs/${file}`,
        time: parseInt(file.match(/claude-execution-(\d+)\.log/)?.[1] || '0')
      }))
      .sort((a, b) => b.time - a.time);
    
    // Read the most recent log file
    if (executionLogs.length > 0) {
      const content = await fs.readFile(executionLogs[0].path, 'utf8');
      
      // Extract session_id from the log content
      const sessionMatch = content.match(/"session_id":"([^"]+)"/);
      if (sessionMatch) {
        return sessionMatch[1];
      }
    }
    
    return null;
  } catch (error) {
    console.warn('Failed to extract session_id from logs:', error.message);
    return null;
  }
}

/**
 * Executes a prompt with a specific session ID.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string} sessionId The specific session ID to resume.
 * @returns {Promise<object>} A promise that resolves with the final JSON output object from Claude.
 */
export async function executePromptWithSession(prompt, sessionId) {
  console.log(`Resuming conversation with session: ${sessionId}`);
  return await attemptClaudeExecution(prompt, sessionId);
}

/**
 * Executes claude with dynamic timeout that extends when output is detected.
 * @param {string} prompt The prompt to send to Claude.
 * @param {string|null} sessionId Session ID to resume, or null for new session.
 * @returns {Promise<{stdout: string, stderr: string}>} The execution result.
 */
async function executeClaudeWithDynamicTimeout(prompt, sessionId = null) {
  return new Promise((resolve, reject) => {
    
    const args = [];
    if (sessionId) {
      args.push('--resume', sessionId);
    }

    // Always add the skip permissions flag for automation reliability
    args.push('--dangerously-skip-permissions');

    args.push(prompt, '-p', '--output-format=stream-json', '--verbose');
    
    const child = spawn('claude', args, {
      stdio: ['ignore', 'pipe', 'pipe']
    });
    
    let stdout = '';
    let stderr = '';
    let lastOutputTime = Date.now();
    let isFinished = false;
    
    const INITIAL_TIMEOUT = 30000; // 30 seconds initial
    const ACTIVITY_TIMEOUT = 60000; // 1 minute of inactivity allowed
    const MAX_TOTAL_TIME = 20 * 60000; // 20 minutes maximum
    
    // Reset timeout whenever we see new output
    const resetTimeout = () => {
      lastOutputTime = Date.now();
    };
    
    // Monitor for activity and kill if inactive too long
    const activityChecker = setInterval(() => {
      if (isFinished) return;
      
      const timeSinceLastOutput = Date.now() - lastOutputTime;
      const totalTime = Date.now() - lastOutputTime + timeSinceLastOutput;
      
      if (totalTime > MAX_TOTAL_TIME) {
        console.log('‚è∞ Maximum execution time reached (20 minutes)');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Maximum execution time exceeded (20 minutes)'));
        return;
      }
      
      if (timeSinceLastOutput > ACTIVITY_TIMEOUT) {
        console.log('üíÄ No activity detected for 1 minute, killing process');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error(`No output received for ${ACTIVITY_TIMEOUT/1000} seconds`));
        return;
      }
      
      // Show activity indicators we're looking for
      if (stdout.includes('‚úª') || stdout.includes('üîç') || stdout.includes('‚öôÔ∏è') || 
          stdout.includes('Forging') || stdout.includes('Processing') || stdout.includes('Searching')) {
        console.log('‚ú® Claude is active, extending timeout...');
        resetTimeout();
      }
    }, 5000); // Check every 5 seconds
    
    child.stdout.on('data', (data) => {
      stdout += data.toString();
      resetTimeout();
      
      // Log interesting activity
      const newData = data.toString();
      if (newData.includes('‚úª') || newData.includes('Forging') || newData.includes('Processing')) {
        console.log('üîÑ Activity detected:', newData.trim().substring(0, 50) + '...');
      }
    });
    
    child.stderr.on('data', (data) => {
      stderr += data.toString();
      resetTimeout();
    });
    
    child.on('close', (code) => {
      isFinished = true;
      clearInterval(activityChecker);
      
      if (code === 0) {
        resolve({ stdout, stderr });
      } else {
        reject(new Error(`Claude process exited with code ${code}`));
      }
    });
    
    child.on('error', (error) => {
      isFinished = true;
      clearInterval(activityChecker);
      reject(error);
    });
    
    // Initial timeout
    setTimeout(() => {
      if (!isFinished && stdout.length === 0) {
        console.log('‚è∞ Initial timeout - no output received');
        child.kill('SIGTERM');
        clearInterval(activityChecker);
        reject(new Error('Initial timeout - no response from claude'));
      }
    }, INITIAL_TIMEOUT);
  });
}

/**
 * Executes a prompt using gemini-cli delegation with retry logic for transient errors.
 * @param {string} prompt The prompt to send to Claude via gemini-cli.
 * @returns {Promise<object>} A promise that resolves with the response from Claude.
 */
export async function askClaude(prompt) {
  return pRetry(async () => {
    try {
      const result = await execa('gemini-cli', ['claude', prompt], {
        timeout: 120000 // 2 minute timeout
      });

      // Parse mcp_feedback if present in prompt
      let mcpFeedback = null;
      try {
        const promptObj = JSON.parse(prompt);
        if (promptObj.payload && promptObj.payload.post_execution_steps && promptObj.payload.post_execution_steps.mcp_feedback) {
          mcpFeedback = promptObj.payload.post_execution_steps.mcp_feedback;

          // Log if errors array is non-empty
          if (mcpFeedback.errors && Array.isArray(mcpFeedback.errors) && mcpFeedback.errors.length > 0) {
            console.warn('MCP feedback contains errors:', mcpFeedback.errors);
          }
        }
      } catch (parseError) {
        // If prompt is not valid JSON or doesn't contain mcp_feedback, continue normally
      }

      return {
        stdout: result.stdout,
        stderr: result.stderr,
        success: true,
        mcp_feedback: mcpFeedback
      };
    } catch (error) {
      // Check if this is a transient error that should be retried
      if (isTransientError(error)) {
        console.log(`Transient error detected, retrying: ${error.message}`);
        throw error; // This will trigger a retry
      }

      // Non-transient errors should not be retried
      console.error(`Non-transient error in askClaude: ${error.message}`);
      return {
        stdout: error.stdout || '',
        stderr: error.stderr || error.message,
        success: false,
        error: error.message
      };
    }
  }, {
    retries: 3,
    minTimeout: 1000,
    maxTimeout: 5000,
    onFailedAttempt: (error) => {
      console.log(`Attempt ${error.attemptNumber} failed. ${error.retriesLeft} retries left.`);
    }
  });
}

/**
 * Checks if an error is transient and should be retried.
 * @param {Error} error The error to check.
 * @returns {boolean} True if the error is transient.
 */
export function isTransientError(error) {
  const errorMessage = (error.message || '').toLowerCase();
  const stderr = (error.stderr || '').toLowerCase();
  const stdout = (error.stdout || '').toLowerCase();
  const allOutput = `${errorMessage} ${stderr} ${stdout}`;

  // Network-related errors
  const networkErrors = [
    'network',
    'timeout',
    'connection',
    'econnreset',
    'enotfound',
    'econnrefused',
    'socket hang up'
  ];

  // Quota/rate limit errors
  const quotaErrors = [
    'quota exceeded',
    'rate limit',
    'too many requests',
    'service unavailable',
    'temporarily unavailable',
    '429',
    '500',
    '502',
    '503',
    '504'
  ];

  const transientPatterns = [...networkErrors, ...quotaErrors];

  return transientPatterns.some(pattern => allOutput.includes(pattern));
}

--- File: /src/services/claudeCliService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';
import { askClaude } from './claudeCliService.js';

// Mock execa
vi.mock('execa', () => ({
  execa: vi.fn()
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', () => ({
  default: async (fn, options) => {
    // For tests, we'll execute the function directly without retries
    return await fn();
  }
}));

describe('claudeCliService', () => {
  let mockExeca;

  beforeEach(async () => {
    const { execa } = await import('execa');
    mockExeca = execa;
    vi.clearAllMocks();
  });

  afterEach(() => {
    vi.resetAllMocks();
  });

  describe('askClaude', () => {
    it('should successfully execute gemini-cli claude command', async () => {
      const mockResponse = {
        stdout: '{"result": "test response", "success": true}',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith('gemini-cli', ['claude', 'test prompt'], {
        timeout: 120000
      });
      expect(result).toEqual({
        stdout: mockResponse.stdout,
        stderr: mockResponse.stderr,
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle non-transient errors without retry', async () => {
      const mockError = new Error('EACCES: permission denied');
      mockError.code = 'EACCES';
      mockError.stdout = '';
      mockError.stderr = 'permission denied';

      mockExeca.mockRejectedValue(mockError);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: 'permission denied',
        success: false,
        error: 'EACCES: permission denied'
      });
    });

    it('should identify transient network errors', async () => {
      const mockError = new Error('Connection timeout');
      mockError.stdout = '';
      mockError.stderr = 'network timeout occurred';

      const { isTransientError } = await import('./claudeCliService.js');

      expect(isTransientError(mockError)).toBe(true);
    });


    it('should handle JSON parsing in gemini-cli response', async () => {
      const complexJsonResponse = {
        stdout: JSON.stringify({
          type: 'result',
          data: {
            analysis: 'test analysis',
            metrics: { tokens: 100, cost: 0.05 }
          },
          timestamp: new Date().toISOString()
        }),
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(complexJsonResponse);

      const result = await askClaude('analyze this code');

      expect(result.success).toBe(true);
      expect(result.stdout).toContain('test analysis');
      expect(result.stdout).toContain('tokens');
    });

    it('should handle empty responses gracefully', async () => {
      const mockResponse = {
        stdout: '',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result).toEqual({
        stdout: '',
        stderr: '',
        success: true,
        mcp_feedback: null
      });
    });

    it('should handle stderr warnings without failing', async () => {
      const mockResponse = {
        stdout: '{"result": "success"}',
        stderr: 'Warning: deprecated feature used',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const result = await askClaude('test prompt');

      expect(result.success).toBe(true);
      expect(result.stderr).toContain('deprecated feature');
    });

    it('should respect timeout configuration', async () => {
      mockExeca.mockResolvedValue({
        stdout: 'response',
        stderr: '',
        exitCode: 0
      });

      await askClaude('test prompt');

      expect(mockExeca).toHaveBeenCalledWith(
        'gemini-cli',
        ['claude', 'test prompt'],
        { timeout: 120000 }
      );
    });

    it('should parse mcp_feedback from JSON prompt', async () => {
      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithFeedback = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: true,
              errors: [],
              mcp_version: '1.0'
            }
          }
        }
      });

      const result = await askClaude(promptWithFeedback);

      expect(result.mcp_feedback).toEqual({
        success: true,
        errors: [],
        mcp_version: '1.0'
      });
    });

    it('should log warning when mcp_feedback contains errors', async () => {
      const consoleSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

      const mockResponse = {
        stdout: 'success',
        stderr: '',
        exitCode: 0
      };

      mockExeca.mockResolvedValue(mockResponse);

      const promptWithErrors = JSON.stringify({
        payload: {
          post_execution_steps: {
            mcp_feedback: {
              success: false,
              errors: ['Error 1', 'Error 2'],
              mcp_version: '1.0'
            }
          }
        }
      });

      await askClaude(promptWithErrors);

      expect(consoleSpy).toHaveBeenCalledWith('MCP feedback contains errors:', ['Error 1', 'Error 2']);

      consoleSpy.mockRestore();
    });
  });

  describe('transient error detection', () => {
    it('should treat network errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const networkErrors = [
        'network error',
        'timeout',
        'connection refused',
        'ECONNRESET',
        'ENOTFOUND',
        'socket hang up'
      ];

      networkErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should treat quota errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const quotaErrors = [
        'quota exceeded',
        'rate limit',
        'too many requests',
        '429',
        '503'
      ];

      quotaErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(true);
      });
    });

    it('should not treat permission errors as transient', async () => {
      const { isTransientError } = await import('./claudeCliService.js');

      const permanentErrors = [
        'EACCES: permission denied',
        'Invalid API key',
        'Authentication failed'
      ];

      permanentErrors.forEach(errorMsg => {
        const error = new Error(errorMsg);
        expect(isTransientError(error)).toBe(false);
      });
    });
  });
});

--- File: /src/services/dispatcherService.js ---

import { ask as askGpt } from './gptService.js';
import { executePrompt as askClaude } from './claudeCliService.js';

/**
 * Dispatches an analytical task to the most efficient AI model with a fallback.
 * Priority 1: Codex (GPT) with low reasoning for speed and cost.
 * Priority 2: Claude as a reliable fallback.
 * @param {string} prompt The JSON payload or prompt string for the task.
 * @returns {Promise<object>} The result from the successful AI agent.
 */
export async function dispatchAnalysisTask(prompt) {
  try {
    console.log('üß† Dispatcher: Attempting analysis with Codex (low reasoning)...');
    const gptOptions = {
      model: 'gpt-5-codex',
      reasoning: 'low'
    };
    // The 'ask' function expects payload as first arg, and options as second.
    // Since prompt is a string here, we wrap it in an object for consistency if needed,
    // but for simple prompts it can often be passed directly.
    const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
    return await askGpt(payload, { verbose: false, ...gptOptions });
  } catch (gptError) {
    console.warn(`‚ö†Ô∏è Codex (low reasoning) failed: ${gptError.message}`);
    console.log('üîÑ Failing over to Claude for analysis...');
    try {
      return await askClaude(prompt);
    } catch (claudeError) {
      console.error(`‚ùå Critical Failure: Both Codex and Claude failed for analysis task.`);
      throw new Error(`Primary (Codex) Error: ${gptError.message}\nFallback (Claude) Error: ${claudeError.message}`);
    }
  }
}

--- File: /src/services/embedding.js ---

import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';
import pLimit from 'p-limit';

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "embedding-001" });

async function generateEmbedding(text, taskType = 'RETRIEVAL_DOCUMENT') {
  try {
    const result = await model.embedContent({ 
      content: { parts: [{ text }] },
      taskType
    });
    return result.embedding.values;
  } catch (error) {
    console.error('‚ùå Gemini Embedding Error:', error.message);
    throw error;
  }
}

async function generateBatchEmbeddings(segments, taskType = 'RETRIEVAL_DOCUMENT') {
    if (segments.length === 0) return [];

    const BATCH_COUNT_LIMIT = 100;
    const BATCH_SIZE_LIMIT = 3000000; // 3MB for safety
    const allBatches = [];
    let currentBatch = [];
    let currentBatchSize = 0;

    for (const segment of segments) {
        const segmentSize = Buffer.byteLength(segment.content, 'utf8');
        if (segmentSize > BATCH_SIZE_LIMIT) { // Handle single oversized segments
            console.log(chalk.yellow(`  -> Warning: Segment '${segment.name}' in '${segment.filePath}' is oversized and will be truncated.`));
            segment.content = segment.content.substring(0, 20000) + '... [truncated]'; // Truncate oversized segment
        }
        if (currentBatch.length > 0 && (currentBatch.length >= BATCH_COUNT_LIMIT || currentBatchSize + Buffer.byteLength(segment.content, 'utf8') > BATCH_SIZE_LIMIT)) {
            allBatches.push(currentBatch);
            currentBatch = [];
            currentBatchSize = 0;
        }
        currentBatch.push(segment);
        currentBatchSize += Buffer.byteLength(segment.content, 'utf8');
    }
    if (currentBatch.length > 0) {
        allBatches.push(currentBatch);
    }

    console.log(chalk.cyan(`‚è≥ Generating embeddings for ${segments.length} segments, divided into ${allBatches.length} safe chunks...`));

    const limit = pLimit(5); // Set concurrency to 5 parallel requests
    let processedCount = 0;

    const promises = allBatches.map((batch, i) => {
        return limit(async () => {
            const batchSizeKB = (Buffer.byteLength(batch.map(s => s.content).join(''), 'utf8') / 1024).toFixed(2);
            console.log(chalk.blue(`  -> Sending chunk ${i + 1}/${allBatches.length} (${batch.length} segments, ${batchSizeKB} KB)...`));
            try {
                const contents = batch.map(s => ({ parts: [{ text: s.content }] }));
                const result = await model.batchEmbedContents({ 
                    requests: contents.map(content => ({ content, taskType }))
                });
                processedCount++;
                console.log(chalk.green(`  <- Chunk ${i + 1}/${allBatches.length} processed successfully.`));
                return result.embeddings.map(e => e.values);
            } catch (error) {
                console.error(chalk.red(`‚ùå Gemini Batch Embedding Error (Chunk ${i + 1}/${allBatches.length}):`), error.message);
                throw error;
            }
        });
    });

    const chunkResults = await Promise.all(promises);
    const allEmbeddings = chunkResults.flat();

    console.log(chalk.green.bold('‚úÖ Batch embeddings generated successfully.'));
    return allEmbeddings;
}

export const embeddingService = {
  generateEmbedding,
  generateBatchEmbeddings
};

--- File: /src/services/embeddingService.js ---

import { pipeline } from '@xenova/transformers';

class EmbeddingService {
    static instance = null;
    static modelName = 'Xenova/jina-embeddings-v2-base-en'; // Can be made configurable

    static async getInstance() {
        if (this.instance === null) {
            console.log(`–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = await pipeline('feature-extraction', this.modelName);
            console.log('–ú–æ–¥–µ–ª—å-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä –≥–æ—Ç–æ–≤–∞.');
        }
        return this.instance;
    }

    static releaseModel() {
        if (this.instance) {
            console.log(`–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏-–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞: ${this.modelName}...`);
            this.instance = null;
            // In Node.js, there's no explicit GPU memory release, 
            // relying on the garbage collector is the standard way.
        }
    }
}

export async function generateEmbedding(code) {
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(code, { pooling: 'mean', normalize: true });
    return Array.from(result.data);
}

export async function generateBatchEmbeddings(texts) {
    if (!texts || texts.length === 0) return [];
    const extractor = await EmbeddingService.getInstance();
    const result = await extractor(texts, { pooling: 'mean', normalize: true });
    
    // Convert tensor to array of arrays
    const embeddings = [];
    for (let i = 0; i < result.dims[0]; i++) {
        const start = i * result.dims[1];
        const end = start + result.dims[1];
        embeddings.push(Array.from(result.data.slice(start, end)));
    }
    return embeddings;
}

export const releaseModel = EmbeddingService.releaseModel;

--- File: /src/services/gptService.js ---

import { execa } from 'execa';
import fs from 'fs/promises';
import path from 'path';
import pRetry from 'p-retry';
import ora from 'ora';
import { loadProjectEckManifest } from '../utils/fileUtils.js';
import { initiateLogin } from './authService.js';
import which from 'which';

const SYSTEM_PROMPT = 'You are a Coder agent. Apply code changes per JSON spec. Respond only in JSON: {success: bool, changes: array, errors: array, post_steps: object}';

class AuthError extends Error {
  constructor(message) {
    super(message);
    this.name = 'AuthError';
  }
}

/**
 * Checks if the codex CLI tool is available in the system's PATH.
 * Throws an error if not found.
 */
async function ensureCodexCliExists() {
  try {
    await which('codex');
  } catch (error) {
    throw new Error('The `codex` CLI tool is not installed or not in your PATH. Please install it from https://github.com/openai/codex to use this command.');
  }
}

/**
 * Delegates an apply_code_changes payload to the codex CLI with auto-login.
 * @param {string|object} payload - JSON string or object payload to forward to the agent.
 * @param {{ verbose?: boolean, model?: string, reasoning?: string }} [options]
 * @returns {Promise<object>}
 */
export async function ask(payload, options = {}) {
  const { verbose = false, model = 'gpt-5-codex', reasoning = 'high' } = options;
  await ensureCodexCliExists();

  const run = async () => {
    const spinner = verbose ? null : ora('Sending payload to Codex agent...').start();
    try {
      const payloadObject = await parsePayload(payload);
      const manifest = await loadProjectEckManifest(process.cwd());
      const userPrompt = buildUserPrompt(payloadObject, manifest);
      const promptInput = `${SYSTEM_PROMPT}\n\n${userPrompt}`;

      const args = [
        'exec',
        // Use full-auto mode to prevent interactive prompts from the agent,
        // as this service is designed for non-interactive delegation.
        '--full-auto',
        '--model', model,
        '-c', `model_reasoning_effort=${reasoning}`
      ];

      debug(verbose, `Executing: codex ${args.join(' ')} <stdin>`);

      const cliResult = await execa('codex', args, {
        cwd: process.cwd(),
        timeout: 300000, // 5-minute timeout
        input: promptInput // Stream large prompts via stdin to avoid argv limits
      });

      const output = cliResult?.stdout?.trim();
      if (!output) {
        throw new Error('codex CLI returned empty response');
      }

      const parsed = extractFinalJson(output);
      if (parsed) {
        if (parsed.post_steps || parsed.post_execution_steps) {
          const postSteps = parsed.post_steps || parsed.post_execution_steps;
          await handlePostExecutionSteps(postSteps, payloadObject);
          parsed.mcp_feedback = postSteps?.mcp_feedback || null;
        }
        spinner?.succeed('Codex agent completed the task.');
        return parsed;
      }

      // If parsing fails, surface the raw response text for upstream handling.
      spinner?.succeed('Codex agent completed the task.');
      return { success: true, changes: [], errors: [], response_text: output };

    } catch (error) {
        spinner?.fail('Codex execution failed.');
        handleCliError(error); // This will throw a specific error type
    }
  };

  return pRetry(run, {
    retries: 1, // Only retry once after a successful login
    minTimeout: 0,
    onFailedAttempt: async (error) => {
      if (error.name === 'AuthError') {
        await initiateLogin();
      } else {
        throw error; // Don't retry for other errors, fail immediately
      }
    }
  });
}


async function parsePayload(payload) {
  if (typeof payload === 'string') {
    try {
      return JSON.parse(payload);
    } catch (error) {
      throw new Error(`Failed to parse payload JSON: ${error.message}`);
    }
  }
  if (typeof payload === 'object' && payload !== null) {
    return payload;
  }
  throw new Error('Invalid payload type. Expected JSON string or object.');
}

function buildUserPrompt(payloadObject, manifest) {
  const payloadString = JSON.stringify(payloadObject);
  if (!manifest) {
    return payloadString;
  }

  const sections = [];
  if (manifest.context) {
    sections.push('## .eck Context\n' + manifest.context);
  }
  if (manifest.operations) {
    sections.push('## .eck Operations\n' + manifest.operations);
  }
  if (manifest.journal) {
    sections.push('## .eck Journal\n' + manifest.journal);
  }
  if (manifest.environment && Object.keys(manifest.environment).length > 0) {
    sections.push('## .eck Environment\n' + JSON.stringify(manifest.environment, null, 2));
  }

  if (sections.length === 0) {
    return payloadString;
  }

  return `${payloadString}\n\n# Project Context\n${sections.join('\n\n')}`;
}

function debug(verbose, message) {
  if (verbose) {
    console.log(`[ask-gpt] ${message}`);
  }
}

function extractFinalJson(text) {
  const trimmed = text?.trim();
  if (!trimmed) {
    return null;
  }

  try {
    return JSON.parse(trimmed);
  } catch (error) {
    // Continue with fallback parsing when logs precede the JSON payload.
  }

  const fencedMatch = trimmed.match(/```(?:json)?\s*([\s\S]*?)```/i);
  if (fencedMatch && fencedMatch[1]) {
    const fencedContent = fencedMatch[1].trim();
    try {
      return JSON.parse(fencedContent);
    } catch (error) {
      // Ignore and fall through to final brace search.
    }
  }

  const lastBraceIndex = trimmed.lastIndexOf('{');
  if (lastBraceIndex === -1) {
    return null;
  }

  const jsonCandidate = trimmed.slice(lastBraceIndex);
  try {
    return JSON.parse(jsonCandidate);
  } catch (error) {
    return null;
  }
}

function handleCliError(error) {
  const combined = `${error?.message || ''} ${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
  // Check for text that `codex` outputs when auth is missing.
  if (combined.includes('authentication is required') || combined.includes('please run `codex login`')) {
    const authError = new Error('Codex authentication is required. Attempting to log in.');
    authError.name = 'AuthError';
    throw authError;
  }

  throw new Error(`codex CLI failed: ${error.stderr || error.message}`);
}

async function handlePostExecutionSteps(postSteps, payloadObject) {
  if (!postSteps || typeof postSteps !== 'object') {
    return;
  }

  if (postSteps.journal_entry) {
    await applyJournalEntry(postSteps.journal_entry, payloadObject);
  }

  if (postSteps.mcp_feedback) {
    logMcpFeedback(postSteps.mcp_feedback);
  }
}

async function applyJournalEntry(entry, payloadObject) {
  const journalEntry = normalizeJournalEntry(entry);
  const journalPath = path.join(process.cwd(), '.eck', 'JOURNAL.md');

  await fs.mkdir(path.dirname(journalPath), { recursive: true });

  let existing = '';
  try {
    existing = await fs.readFile(journalPath, 'utf-8');
  } catch (error) {
    if (error.code !== 'ENOENT') {
      throw new Error(`Failed to read JOURNAL.md: ${error.message}`);
    }
  }

  const taskId = payloadObject?.task_id || payloadObject?.payload?.task_id || journalEntry.task_id || 'ask-gpt';
  const isoDate = new Date().toISOString();

  const frontmatter = [
    '---',
    `task_id: ${taskId}`,
    `date: ${isoDate}`,
    `type: ${journalEntry.type}`,
    `scope: ${journalEntry.scope}`,
    '---',
    ''
  ].join('\n');

  const summary = journalEntry.summary ? `## ${journalEntry.summary}\n` : '';
  const details = journalEntry.details ? `${journalEntry.details}\n` : '';

  const entryBlock = `${frontmatter}${summary ? `${summary}\n` : ''}${details}\n`;

  const existingTrimmed = existing ? existing.replace(/^\n+/, '') : '';
  const newContent = `${entryBlock}${existingTrimmed}`.replace(/\n{3,}/g, '\n\n');

  await fs.writeFile(journalPath, newContent.trimEnd() + '\n');

  await stageJournal(journalPath);
  await commitJournal(journalEntry);
}

function normalizeJournalEntry(entry) {
  return {
    type: entry.type || 'chore',
    scope: entry.scope || 'journal',
    summary: entry.summary || 'Update journal entry',
    details: entry.details || ''
  };
}

async function stageJournal(journalPath) {
  const relativePath = path.relative(process.cwd(), journalPath);
  try {
    await execa('git', ['add', relativePath], { cwd: process.cwd() });
  } catch (error) {
    throw new Error(`Failed to stage journal entry: ${error.message}`);
  }
}

async function commitJournal(entry) {
  const scopePart = entry.scope ? `(${entry.scope})` : '';
  const summary = (entry.summary || 'Update journal entry').replace(/\s+/g, ' ').trim();
  const commitMessage = `${entry.type}${scopePart}: ${summary}`;

  try {
    await execa('git', ['commit', '-m', commitMessage], { cwd: process.cwd() });
  } catch (error) {
    const text = `${error?.stderr || ''} ${error?.stdout || ''}`.toLowerCase();
    if (text.includes('nothing to commit')) {
      console.warn('Journal entry already committed or no changes to commit.');
      return;
    }
    throw new Error(`Failed to commit journal entry: ${error.message}`);
  }
}

function logMcpFeedback(feedback) {
  if (!feedback) {
    return;
  }

  const errors = Array.isArray(feedback.errors) ? feedback.errors : [];
  if (!feedback.success || errors.length > 0) {
    console.warn('MCP feedback indicates issues:', errors.length > 0 ? errors : feedback);
  } else {
    console.log('MCP feedback:', feedback);
  }
}


--- File: /src/services/gptService.test.js ---

import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest';

// Mock execa and which
vi.mock('execa', () => ({ execa: vi.fn() }));
vi.mock('which', () => ({ default: vi.fn() }));

// Mock fs/promises for journal entries
const mkdirMock = vi.fn();
const readFileMock = vi.fn();
const writeFileMock = vi.fn();
const loadProjectEckManifestMock = vi.fn();
vi.mock('fs/promises', () => ({
  mkdir: mkdirMock,
  readFile: readFileMock,
  writeFile: writeFileMock
}));
vi.mock('../utils/fileUtils.js', () => ({
  loadProjectEckManifest: loadProjectEckManifestMock
}));

// Mock p-retry to control retry behavior in tests
vi.mock('p-retry', async (importOriginal) => {
  const actual = await importOriginal();
  return {
    ...actual,
    default: vi.fn(async (fn, options) => {
      try {
        return await fn();
      } catch (error) {
        if (options.onFailedAttempt) {
          await options.onFailedAttempt(error);
          // In a real scenario, p-retry would re-run fn. For testing, we simulate one retry.
          if (error.name === 'AuthError') {
             return await fn();
          }
        }
        throw error;
      }
    })
  };
});

// Mock the authService
vi.mock('./authService.js', () => ({
  initiateLogin: vi.fn()
}));

describe('gptService with codex CLI', () => {
  let ask;
  let execaMock;
  let whichMock;
  let initiateLoginMock;

  beforeEach(async () => {
    vi.clearAllMocks();

    ({ execa: execaMock } = await import('execa'));
    const which = (await import('which')).default;
    whichMock = which;
    ({ initiateLogin: initiateLoginMock } = await import('./authService.js'));
    ({ ask } = await import('./gptService.js'));

    whichMock.mockResolvedValue('/usr/bin/codex');
    loadProjectEckManifestMock.mockResolvedValue(null);
  });

  it('should call codex CLI with correct arguments and parse final JSON from noisy output', async () => {
    const codexLogs = '[2025-10-06 20:04:22] OpenAI Codex v0.42.0\nSome setup log...\n\n{"success": true, "changes": ["change1"], "errors": []}';
    execaMock.mockResolvedValue({ stdout: codexLogs });

    const payload = { objective: 'Test' };
    const result = await ask(payload);

    expect(result).toEqual({ success: true, changes: ['change1'], errors: [] });
    expect(execaMock).toHaveBeenCalledWith('codex', expect.arrayContaining(['exec', '--full-auto', '--model']), expect.any(Object));
    const [, , options] = execaMock.mock.calls[0];
    expect(options.input).toContain(JSON.stringify(payload));
  });

  it('should trigger login flow on authentication error and retry', async () => {
    const authError = new Error('Authentication is required. Please run `codex login`.');
    authError.name = 'AuthError'; // Custom error name to trigger retry
    authError.stderr = 'Authentication is required. Please run `codex login`.';

    const successResponse = {
      id: 'task2',
      msg: {
        type: 'task_complete',
        last_agent_message: '{"success": true}'
      }
    };

    // First call fails, second call (retry) succeeds
    execaMock
      .mockRejectedValueOnce(authError)
      .mockResolvedValueOnce({ stdout: JSON.stringify(successResponse) });

    initiateLoginMock.mockResolvedValue();

    const result = await ask({ objective: 'Retry test' });

    expect(result).toEqual({ success: true });
    expect(initiateLoginMock).toHaveBeenCalledTimes(1);
    expect(execaMock).toHaveBeenCalledTimes(2); // Initial call + retry
  });

  it('should throw if codex CLI is not found', async () => {
    whichMock.mockRejectedValue(new Error('not found'));
    await expect(ask({})).rejects.toThrow('The `codex` CLI tool is not installed');
  });

  it('should throw non-auth errors immediately without retry', async () => {
    const otherError = new Error('Some other CLI error');
    otherError.stderr = 'Something else went wrong';
    execaMock.mockRejectedValueOnce(otherError);

    await expect(ask({})).rejects.toThrow('codex CLI failed: Something else went wrong');
    expect(initiateLoginMock).not.toHaveBeenCalled();
  });
});


--- File: /src/templates/agent-prompt.template.md ---

# AI Junior Architect Instructions

You are the **Junior Architect** agent (`gemini_wsl`). Your primary goal is to execute high-level strategic tasks delegated to you by the Senior Architect.

## Your Context
- You are running in **WSL**.
- You have access to a detailed `_ja.md` snapshot (which is *this* file).
- You have a special capability: the `/claude` command, which delegates to a Coder agent.

## Hierarchical Role
- The **Senior Architect (Gemini)** gives you high-level `execute_strategic_task` commands.
- **You (Junior Architect / `gemini_wsl`)** analyze the task, break it down, and use your tools.
- The **Coder (`claude`)** is your primary tool for *writing code*.

## CRITICAL WORKFLOW: Using the Coder (`/claude`)

The `claude` agent (who you command via `/claude`) is a **specialized Coder**. It is highly trained for code generation.

When you need to write or modify code, you **MUST** use the `/claude` command and provide it with a **JSON payload** (as a single-line JSON string) in the `apply_code_changes` format.

**DO NOT** ask `claude` to "write a function" in natural language. You *must* command it with this precise JSON structure:

**IMPORTANT:** The JSON payload must be passed as a **single-line string wrapped in SINGLE QUOTES (`'`)**. This is the simplest and safest way to pass the complete JSON (which uses double quotes) through the shell without it breaking.

```
/claude '{"target_agent":"local_dev","command_for_agent":"apply_code_changes","task_id":"ja-subtask-123","payload":{"objective":"Write the `doSomething` function","context":"This function is for the `UserService`...","files_to_modify":[{"path":"src/services/UserService.js","action":"add","location":"After the `getUser` function","details":"...new function code..."}],"new_files":[],"validation_steps":[]},"post_execution_steps":{"journal_entry":{"type":"feat","scope":"api","summary":"Implement `doSomething` function","details":"Delegated from JA"}}}'
```

Your other tools (like `bash`) can be used for analysis and validation.

--- File: /src/templates/architect-prompt.template.md ---

# AI Architect Instructions

You are an autonomous AI Architect. Your primary goal is to develop and evolve a software project by planning high-level architecture and delegating implementation tasks to an execution agent named Claude.

## Core Workflow: The Thought-Tool-Observation Loop

Your entire operational process follows a strict loop:
1.  **Thought:** Analyze the user's request, the current state of the project, and previous observations. Formulate a plan and decide on the next immediate action. You must explain your reasoning and your chosen action in plain text.
2.  **Tool:** Immediately after your thought process, you MUST issue a command to either the local `eck-snapshot` environment or the `claude_code_agent`.
3.  **Observation:** After issuing a command, you MUST STOP and wait for an `Observation:` message from the system, which will contain the result of your command. Do not proceed until you receive it.

## Commanding the Execution Agent (Claude)

To delegate any coding task (writing, editing, testing, refactoring), you MUST generate a JSON command block for the `claude_code_agent`. This is your primary method of modifying the codebase.

**JSON Command Format:**
```json
{
  "target_agent": "claude_code_agent",
  "command_for_agent": "apply_code_changes",
  "payload": {
    "objective": "A brief, clear task description for Claude.",
    "context": "Explain why this change is needed and any relevant architectural context.",
    "files_to_modify": [
      {
        "path": "exact/path/to/file.js",
        "action": "add | modify | replace | delete",
        "location": "line numbers, function name, or a unique search pattern",
        "details": "Precise, step-by-step instructions for Claude to implement."
      }
    ]
  }
}
```

## Interacting with the Local Environment

To understand the project state, you can command the `eck-snapshot` tool directly. Use this for discovery, analysis, and managing project context.

**Tool Command Format:** `[tool_code: eck-snapshot <command> <options>]`

**Available Commands:**
- `eck-snapshot snapshot`: To create a new snapshot of the current state.
- `eck-snapshot query "<question>"`: To search the codebase.
- `eck-snapshot detect`: To analyze the project structure.
- `eck-snapshot restore <snapshot_file> --include ...`: To view specific files from a snapshot.

## Final Mandate

Your existence is defined by this loop. Think, act by issuing a tool command, and then wait for the observation. This is the only way you can make progress.

--- File: /src/templates/envScanRequest.md ---

Run this command and return the output:
```
node -e "console.log(['ENV', process.platform+'/'+require('os').arch(), process.version, require('fs').readdirSync('.').some(f=>f.endsWith('.sqlite')||f.endsWith('.sqlite3'))?'SQLite':'none', require('path').basename(process.cwd())].join('|'))"
```

--- File: /src/templates/gitWorkflow.md ---

## Git Commit Workflow

**IMPORTANT**: This project is a Git repository. After successfully completing any development task, you MUST propose a Git commit as the final step.

### Commit Process
1. **Review Changes**: Before committing, briefly summarize what was accomplished
2. **Stage Files**: Include `git add .` or specific files in your command block
3. **Create Commit**: Use a clear, descriptive commit message following this format:
   - Start with the task context (e.g., "feat:", "fix:", "docs:", "refactor:")
   - Include the task_id from your command block for traceability
   - Keep it under 50 characters for the first line
   - Add detailed description if needed

### Example Git Commands to Include
```bash
git add .
git commit -m "feat: implement user authentication system

Task ID: auth-system-implementation-1
- Added login/register endpoints
- Implemented JWT token validation
- Added user session management"
```

### When to Commit
- After completing any feature implementation
- After fixing bugs or issues
- After refactoring or code improvements
- After adding tests or documentation
- Before major architectural changes

**Remember**: Always include the Git commit step in your final command block to ensure proper version control and project history.

--- File: /src/templates/multiAgent.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /path/to/file ---` headers.

**Snapshot Stats:**
- **Files Included:** {{stats.includedFiles}}
- **Total Files in Repo:** {{stats.totalFiles}}

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

{{projectOverview}}

{{eckManifestSection}}

### CRITICAL WORKFLOW: Structured Commits via `journal_entry`

To ensure proper project history, all code changes **MUST** be committed using the project's built-in structured workflow.

**Your Role (Architect):**
Your JSON command payload **MUST** include a `post_execution_steps.journal_entry` object. This object is the *trigger* for the execution agent's internal `/eck:commit` command.

**DO NOT** generate `git add` or `git commit` commands yourself. The `journal_entry` object handles everything:
1.  Staging all changes (`git add .`).
2.  Creating a YAML frontmatter entry for the journal.
3.  Prepending the entry to `.eck/JOURNAL.md`.
4.  Executing the conventional Git commit.

**Example `journal_entry` in your payload:**
```json
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "api",
        "summary": "Implement user authentication endpoint",
        "details": "- Added /login route\n- Implemented JWT validation"
      }
    }
````

### Strategic Manifest Files

As the Architect, you are also responsible for maintaining other strategic files in the `.eck` directory, such as `ROADMAP.md` and `TECH_DEBT.md`. Propose modifications to these files as needed to reflect the project's status.

### CORE WORKFLOW: The Interactive Command Cycle

1.  **Check Environment:** Request ENV scan from agent first
2.  **Analyze User Request:** Understand the user's goal in their native language.
3.  **Formulate a Plan:** Create a high-level technical plan appropriate for the detected environment and .eck manifest context.
4.  **Propose & Await Confirmation:** Present the plan to the user in their language and ask for approval to generate the command. **CRITICAL: Stop and wait for the user's response. Do NOT generate the command block at this stage.**
5.  **Generate Command on Demand:** This is the execution step, triggered ONLY by a positive user response.
      - **On Approval:** If the user confirms the plan (e.g., "yes", "proceed") or provides a minor correction, your *next response* must be **only the command block**. Do not include any conversational text.
      - **On Direct Order:** If the user explicitly asks for the command (e.g., "make the command for Claude now") and you have all the necessary information, you may skip step 3 and directly generate the command block.
6.  **Review & Report:** After the command is executed, analyze the results and report back to the user in their language.
7.  **Iterate:** Continue the cycle based on user feedback.

### HIERARCHICAL AGENT WORKFLOW

Your primary role is **Senior Architect**. You formulate high-level strategy. For complex code implementation, you will delegate to a **Junior Architect** agent (`gemini_wsl`), who has a detailed (`_ja.md`) snapshot and the ability to command a **Coder** agent (`claude`).

  - **Senior Architect (You):** Sets strategy, defines high-level tasks.
  - **Junior Architect (`gemini_wsl`):** Receives strategic tasks, analyzes the `_ja.md` snapshot, breaks the task down, and commands the Coder.
  - **Coder (`claude`):** Receives small, precise coding tasks from the Junior Architect. **Claude is highly trained for code generation and should be used for all primary code-writing tasks**, while `gemini_wsl` can use its own tools for analysis, validation, and running shell commands.

### COMMAND FORMATS

You MUST use one of two JSON command formats based on your target:

**1. For Coders (`local_dev`, `production_server`, `android_wsl_dev`, `gemini_windows`) - LOW-LEVEL EXECUTION:**
Use `apply_code_changes` for simple, direct tasks where you provide all details.

```json
{
  "target_agent": "local_dev",
  "agent_environment": "Development environment with full GUI support and development tools",
  "command_for_agent": "apply_code_changes",
  "task_id": "unique-task-id",
  "payload": {
    "objective": "Brief, clear task description",
    "context": "Why this change is needed - include relevant .eck manifest context",
    "files_to_modify": [
      {
        "path": "exact/file/path.js",
        "action": "specific action (add, modify, replace, delete)",
        "location": "line numbers, function name, or search pattern",
        "details": "precise description of the change"
      }
    ],
    "new_files": [
      {
        "path": "path/to/new/file.js",
        "content_type": "javascript/json/markdown/config",
        "purpose": "why this file is needed"
      }
    ],
    "dependencies": {
      "install": ["package-name@version"],
      "remove": ["old-package-name"]
    },
    "validation_steps": [
      "npm run test",
      "node index.js --help",
      "specific command to verify functionality"
    ],
    "expected_outcome": "what should work after changes",
    "post_execution_steps": {
      "journal_entry": {
        "type": "feat",
        "scope": "authentication",
        "summary": "Brief description of what was accomplished",
        "details": "Detailed explanation of changes, impacts, and technical notes"
      },
      "mcp_feedback": {
        "success": true,
        "errors": [],
        "mcp_version": "1.0"
      }
    }
  }
}
```

**2. For Junior Architects (`gemini_wsl`) - HIGH-LEVEL DELEGATION:**
Use `execute_strategic_task` for complex features. The JA will use its own snapshot and Coder agent to complete the task.

```json
{
  "target_agent": "gemini_wsl",
  "command_for_agent": "execute_strategic_task",
  "payload": {
    "objective": "Implement the user authentication feature",
    "context": "This is a high-level task. Use your _ja.md snapshot to analyze the codebase. Use your 'claude (delegate)' capability to implement the necessary code across all required files (routes, controllers, services).",
    "constraints": [
      "Must use JWT for tokens",
      "Add new routes to `routes/api.js`",
      "Ensure all new code is covered by tests"
    ],
    "validation_steps": [
      "npm run test"
    ]
  }
}
```

### COMMUNICATION PROTOCOL

  - **User Interaction:** ALWAYS communicate with the user in the language they use.
  - **Agent Commands:** ALWAYS formulate the JSON payload and technical instructions for the execution agent in **ENGLISH** to ensure technical accuracy.
  - **Context Integration:** When briefing agents, include relevant information from the .eck manifest to provide better context.

### AVAILABLE EXECUTION AGENTS

You can command multiple specialized agents. **YOU must choose the most appropriate agent** based on the task requirements and target environment:

{{agentDefinitions}}



--- File: /src/templates/vectorMode.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a context-aware snapshot of the **{{repoName}}** software repository, generated by the `eck-snapshot` tool on **{{timestamp}}**. The content has been filtered based on vector similarity to your query: "{{userQuery}}"

* **Source of Truth:** Treat this snapshot as the relevant source code for your specific task.
* **Structure:** The file contains the full content of each relevant file, demarcated by `--- File: /path/to/file ---` headers.

---

## 2. Your Core Operational Workflow

You are the Project Manager and Solution Architect AI. Your primary goal is to translate user requests into technical plans and then generate precise commands for code-execution AI agents.

### PROJECT OVERVIEW
- **Project:** {{repoName}}
- **User Query:** "{{userQuery}}"
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.

{{multiAgentSection}}

---

--- File: /src/utils/aiHeader.js ---

import { loadSetupConfig, getAllProfiles } from '../config.js';
import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Simple template renderer for basic variable substitution
function render(template, data) {
  let output = template;
  for (const key in data) {
    const value = data[key];
    if (typeof value === 'object' && value !== null) {
      for (const nestedKey in value) {
        output = output.replace(new RegExp(`{{${key}.${nestedKey}}}`, 'g'), value[nestedKey]);
      }
    } else {
      output = output.replace(new RegExp(`{{${key}}}`, 'g'), value);
    }
  }
  return output;
}

function buildAgentDefinitions(executionAgents) {
  let definitions = '';
  for (const key in executionAgents) {
    const agent = executionAgents[key];
    if (agent.active) {
      definitions += `
### ${agent.name} (ID: "${key}")
- **Description:** ${agent.description}
- **GUI Support:** ${agent.guiSupport ? 'Yes' : 'No (Headless)'}
- **Capabilities:** ${agent.capabilities.join(', ')}
- **Restrictions:** ${agent.restrictions.join(', ')}
`;
    }
  }
  return definitions;
}

function buildEckManifestSection(eckManifest) {
  if (!eckManifest) {
    return '';
  }

  let section = '\n## Project-Specific Manifest (.eck Directory)\n\n';
  section += 'This project includes a `.eck` directory with specific context and configuration:\n\n';

  if (eckManifest.context) {
    section += '### Project Context\n\n';
    section += eckManifest.context + '\n\n';
  }

  if (eckManifest.operations) {
    section += '### Operations Guide\n\n';
    section += eckManifest.operations + '\n\n';
  }

  if (eckManifest.journal) {
    section += '### Development Journal\n\n';
    section += eckManifest.journal + '\n\n';
  }

  if (Object.keys(eckManifest.environment).length > 0) {
    section += '### Environment Overrides\n\n';
    section += 'The following environment settings override auto-detected values:\n\n';
    for (const [key, value] of Object.entries(eckManifest.environment)) {
      section += `- **${key}**: ${value}\n`;
    }
    section += '\n';
  }

  section += '**Important**: Use this manifest information when formulating technical plans and briefing execution agents. The context, operations guide, and journal provide crucial project-specific knowledge that should inform your decisions.\n\n';
  section += '---\n\n';

  return section;
}

export async function generateEnhancedAIHeader(context, isGitRepo = false) {
  try {
    const setupConfig = await loadSetupConfig();
    const { aiInstructions } = setupConfig;
    const { architectPersona, executionAgents, promptTemplates } = aiInstructions;

    // Helper function to read a template file or return the string if it's not a path
    const loadTemplate = async (templatePathOrString) => {
      if (templatePathOrString && (templatePathOrString.endsWith('.md') || templatePathOrString.endsWith('.txt'))) {
        try {
          // Resolve path relative to the project root. __dirname is src/utils.
          const resolvedPath = path.join(__dirname, '..', '..', templatePathOrString);
          return await fs.readFile(resolvedPath, 'utf-8');
        } catch (e) {
          return `ERROR: FAILED TO LOAD TEMPLATE ${templatePathOrString}: ${e.message}`;
        }
      }
      return templatePathOrString; // Fallback for old-style inline strings or errors
    };

    // P1 Bug Fix: Normalize manifest structure as per Consilium report
    function normalizeManifest(raw) {
      if (!raw) return null;
      const out = {};
      // Handle `setup.json` structure (e.g., `projectContext.name`)
      if (raw.projectContext) {
        out.context = raw.projectContext.description || JSON.stringify(raw.projectContext, null, 2);
        out.operations = raw.operations || raw.projectContext.operations || ''; // Assuming .eck/OPERATIONS.md is separate
        out.journal = raw.journal || raw.projectContext.journal || ''; // Assuming .eck/JOURNAL.md is separate
        out.environment = raw.environment || raw.projectContext.environment || {}; // Assuming .eck/ENVIRONMENT.md is separate
      } else {
        // Handle direct .eck file structure (e.g., raw.context from CONTEXT.md)
        out.context = raw.context || '';
        out.operations = raw.operations || '';
        out.journal = raw.journal || '';
        out.environment = raw.environment || {};
      }
      // Add fallback text if still empty
      if (!out.context) out.context = 'No project context provided.';
      if (!out.operations) out.operations = 'No operations guide provided.';
      if (!out.journal) out.journal = 'No journal entries found.';

      return out;
    }

    // --- Build common context sections --- 
    const projectOverview = `### PROJECT OVERVIEW
- **Project:** ${context.repoName || 'Unknown'}
- **Description:** A monorepo POS system with Electron frontend and Node.js backend.
`;
    const normalizedEck = normalizeManifest(context.eckManifest);
    let eckManifestSection = '';
    if (normalizedEck) {
      eckManifestSection = buildEckManifestSection(normalizedEck);
    } else {
      eckManifestSection = '### PROJECT-SPECIFIC MANIFEST (.eck Directory)\n\nWARNING: .eck manifest was not found or was empty.\n';
    }
    // --- End context building ---


    // Check if agent mode is enabled
    if (context.options && context.options.agent) {
      const agentPromptTemplate = await loadTemplate(promptTemplates.agent);

      const agentHeader = `${agentPromptTemplate}

${projectOverview}
${eckManifestSection}
---

## Project Snapshot Information

- **Project**: ${context.repoName || 'Unknown'}
- **Timestamp**: ${new Date().toISOString()}
- **Files Included**: ${context.stats ? context.stats.includedFiles : 'Unknown'}
- **Total Files in Repo**: ${context.stats ? context.stats.totalFiles : 'Unknown'}

---

`;
      return agentHeader;
    }

    // --- This is the main/Senior Architect prompt logic --- 
    let template;
    if (context.mode === 'vector') {
      template = await loadTemplate(promptTemplates.vectorMode);
      // Inject context for vector mode
      template = template.replace('{{multiAgentSection}}', `
${projectOverview}
${eckManifestSection}
`); 
    } else {
      template = await loadTemplate(promptTemplates.multiAgent);
      // --- INJECT DYNAMIC CONTEXT --- 
      template = template.replace('{{projectOverview}}', projectOverview);
      template = template.replace('{{eckManifestSection}}', eckManifestSection);
      // --- END INJECT --- 
    }

    const agentDefinitions = buildAgentDefinitions(executionAgents);

    const data = {
      ...context,
      timestamp: new Date().toISOString(),
      architectPersona,
      agentDefinitions
    };

    let renderedTemplate = render(template, data);
    
    // Inject dynamic profile context if a profile is active
    if (context.options && context.options.profile && context.repoPath) {
      let metadataHeader = '\n\n## Partial Snapshot Context\n';
      metadataHeader += `- **Profile(s) Active:** ${context.options.profile}\n`;
      try {
          const allProfiles = await getAllProfiles(context.repoPath);
          const activeProfileNames = context.options.profile.split(',').map(p => p.trim().replace(/^-/, ''));
          const allProfileNames = Object.keys(allProfiles).filter(p => !activeProfileNames.includes(p));
          if (allProfileNames.length > 0) {
               metadataHeader += `- **Other Available Profiles:** ${allProfileNames.join(', ')}\n`;
          }
      } catch (e) { /* fail silently on metadata generation */ }
      
      const insertMarker = "### HIERARCHICAL AGENT WORKFLOW"; // Use our new marker
      renderedTemplate = renderedTemplate.replace(insertMarker, metadataHeader + '\n' + insertMarker);
    }

    return renderedTemplate;

  } catch (error) {
    console.warn('Warning: Could not load setup.json, using minimal header', error.message);
    return `# Snapshot for ${context.repoName || 'Project'}

Generated: ${new Date().toISOString()}

---

`;
  }
}

--- File: /src/utils/fileUtils.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import ignore from 'ignore';
import { detectProjectType, getProjectSpecificFiltering } from './projectDetector.js';
import { dispatchAnalysisTask } from '../services/dispatcherService.js';
import { getProfile } from '../config.js';
import micromatch from 'micromatch';

export function parseSize(sizeStr) {
  const units = { B: 1, KB: 1024, MB: 1024 ** 2, GB: 1024 ** 3 };
  const match = sizeStr.match(/^(\d+(?:\.\d+)?)\s*(B|KB|MB|GB)?$/i);
  if (!match) throw new Error(`Invalid size format: ${sizeStr}`);
  const [, size, unit = 'B'] = match;
  return Math.floor(parseFloat(size) * units[unit.toUpperCase()]);
}

export function formatSize(bytes) {
  const units = ['B', 'KB', 'MB', 'GB'];
  let size = bytes;
  let unitIndex = 0;
  while (size >= 1024 && unitIndex < units.length - 1) {
    size /= 1024;
    unitIndex++;
  }
  return `${size.toFixed(1)} ${units[unitIndex]}`;
}

export function matchesPattern(filePath, patterns) {
  const fileName = path.basename(filePath);
  return patterns.some(pattern => {
    const regexPattern = '^' + pattern.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
    try {
      const regex = new RegExp(regexPattern);
      return regex.test(fileName);
    } catch (e) {
      console.warn(`‚ö†Ô∏è Invalid regex pattern in config: "${pattern}"`);
      return false;
    }
  });
}

export async function checkGitAvailability() {
  try {
    await execa('git', ['--version']);
  } catch (error) {
    throw new Error('Git is not installed or not available in PATH');
  }
}

export async function checkGitRepository(repoPath) {
  try {
    await execa('git', ['rev-parse', '--git-dir'], { cwd: repoPath });
    return true;
  } catch (error) {
    return false;
  }
}

export async function scanDirectoryRecursively(dirPath, config, relativeTo = dirPath, projectType = null) {
  const files = [];
  
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(relativeTo);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  try {
    const entries = await fs.readdir(dirPath, { withFileTypes: true });
    
    for (const entry of entries) {
      const fullPath = path.join(dirPath, entry.name);
      const relativePath = path.relative(relativeTo, fullPath).replace(/\\/g, '/');
      
      if (effectiveConfig.dirsToIgnore.some(dir => 
        entry.name === dir.replace('/', '') || 
        relativePath.startsWith(dir)
      )) {
        continue;
      }
      
      if (!effectiveConfig.includeHidden && entry.name.startsWith('.')) {
        continue;
      }
      
      if (entry.isDirectory()) {
        const subFiles = await scanDirectoryRecursively(fullPath, effectiveConfig, relativeTo, projectType);
        files.push(...subFiles);
      } else {
        if (effectiveConfig.extensionsToIgnore.includes(path.extname(entry.name)) ||
            matchesPattern(relativePath, effectiveConfig.filesToIgnore)) {
          continue;
        }
        
        files.push(relativePath);
      }
    }
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dirPath} - ${error.message}`);
  }
  
  return files;
}

export async function loadGitignore(repoPath) {
  try {
    const gitignoreContent = await fs.readFile(path.join(repoPath, '.gitignore'), 'utf-8');
    const ig = ignore().add(gitignoreContent);
    console.log('‚úÖ .gitignore patterns loaded');
    return ig;
  } catch {
    console.log('‚ÑπÔ∏è No .gitignore file found or could not be read');
    return ignore();
  }
}

export async function readFileWithSizeCheck(filePath, maxFileSize) {
  try {
    const stats = await fs.stat(filePath);
    if (stats.size > maxFileSize) {
      throw new Error(`File too large: ${formatSize(stats.size)}`);
    }
    return await fs.readFile(filePath, 'utf-8');
  } catch (error) {
    if (error.message.includes('too large')) throw error;
    throw new Error(`Could not read file: ${error.message}`);
  }
}

export async function generateDirectoryTree(dir, prefix = '', allFiles, depth = 0, maxDepth = 10, config) {
  if (depth > maxDepth) return '';
  
  try {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const sortedEntries = entries.sort((a, b) => {
      if (a.isDirectory() && !b.isDirectory()) return -1;
      if (!a.isDirectory() && b.isDirectory()) return 1;
      return a.name.localeCompare(b.name);
    });
    
    let tree = '';
    const validEntries = [];
    
    for (const entry of sortedEntries) {
      if (config.dirsToIgnore.some(d => entry.name.includes(d.replace('/', '')))) continue;
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(process.cwd(), fullPath).replace(/\\/g, '/');
      if (entry.isDirectory() || allFiles.includes(relativePath)) {
        validEntries.push({ entry, fullPath, relativePath });
      }
    }
    
    for (let i = 0; i < validEntries.length; i++) {
      const { entry, fullPath, relativePath } = validEntries[i];
      const isLast = i === validEntries.length - 1;
      
      const connector = isLast ? '‚îî‚îÄ‚îÄ ' : '‚îú‚îÄ‚îÄ ';
      const nextPrefix = prefix + (isLast ? '    ' : '‚îÇ   ');
      
      if (entry.isDirectory()) {
        tree += `${prefix}${connector}${entry.name}/\n`;
        tree += await generateDirectoryTree(fullPath, nextPrefix, allFiles, depth + 1, maxDepth, config);
      } else {
        tree += `${prefix}${connector}${entry.name}\n`;
      }
    }
    
    return tree;
  } catch (error) {
    console.warn(`‚ö†Ô∏è Warning: Could not read directory: ${dir}`);
    return '';
  }
}

export function parseSnapshotContent(content) {
  const files = [];
  const fileRegex = /--- File: \/(.+) ---/g;
  const sections = content.split(fileRegex);
  
  for (let i = 1; i < sections.length; i += 2) {
    const filePath = sections[i].trim();
    let fileContent = sections[i + 1] || '';

    if (fileContent.startsWith('\n\n')) {
      fileContent = fileContent.substring(2);
    }
    if (fileContent.endsWith('\n\n')) {
      fileContent = fileContent.substring(0, fileContent.length - 2);
    }
    
    files.push({ path: filePath, content: fileContent });
  }

  return files;
}

export function filterFilesToRestore(files, options) {
  let filtered = files;
  
  if (options.include) {
    const includePatterns = Array.isArray(options.include) ?
      options.include : [options.include];
    filtered = filtered.filter(file => 
      includePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  if (options.exclude) {
    const excludePatterns = Array.isArray(options.exclude) ? 
      options.exclude : [options.exclude];
    filtered = filtered.filter(file => 
      !excludePatterns.some(pattern => {
        const regex = new RegExp(pattern.replace(/\*/g, '.*'));
        return regex.test(file.path);
      })
    );
  }
  
  return filtered;
}

export function validateFilePaths(files, targetDir) {
  const invalidFiles = [];
  
  for (const file of files) {
    const normalizedPath = path.normalize(file.path);
    if (normalizedPath.includes('..') || 
        normalizedPath.startsWith('/') || 
        normalizedPath.includes('\0') ||
        /[<>:"|?*]/.test(normalizedPath)) {
      invalidFiles.push(file.path);
    }
  }
  
  return invalidFiles;
}

export async function loadConfig(configPath) {
  const { DEFAULT_CONFIG } = await import('../config.js');
  let config = { ...DEFAULT_CONFIG };
  
  if (configPath) {
    try {
      const configModule = await import(path.resolve(configPath));
      config = { ...config, ...configModule.default };
      console.log(`‚úÖ Configuration loaded from: ${configPath}`);
    } catch (error) {
      console.warn(`‚ö†Ô∏è Warning: Could not load config file: ${configPath}`);
    }
  } else {
    const possibleConfigs = [
      '.ecksnapshot.config.js',
      '.ecksnapshot.config.mjs',
      'ecksnapshot.config.js'
    ];
    
    for (const configFile of possibleConfigs) {
      try {
        await fs.access(configFile);
        const configModule = await import(path.resolve(configFile));
        config = { ...config, ...configModule.default };
        console.log(`‚úÖ Configuration loaded from: ${configFile}`);
        break;
      } catch {
        // Config file doesn't exist, continue
      }
    }
  }
  
  return config;
}

export function generateTimestamp() {
  const now = new Date();
  const YYYY = now.getFullYear();
  const MM = String(now.getMonth() + 1).padStart(2, '0');
  const DD = String(now.getDate()).padStart(2, '0');
  const hh = String(now.getHours()).padStart(2, '0');
  const mm = String(now.getMinutes()).padStart(2, '0');
  const ss = String(now.getSeconds()).padStart(2, '0');
  return `${YYYY}-${MM}-${DD}_${hh}-${mm}-${ss}`;
}

export function sanitizeForFilename(text) {
  return text
    .toLowerCase()
    .replace(/\s+/g, '-') // Replace spaces with hyphens
    .replace(/[^a-z0-9-]/g, '') // Remove invalid characters
    .substring(0, 50); // Truncate to a reasonable length
}

/**
 * Displays project detection information in a user-friendly format
 * @param {object} detection - Project detection result
 */
export function displayProjectInfo(detection) {
  console.log('\nüîç Project Detection Results:');
  console.log(`   Type: ${detection.type} (confidence: ${(detection.confidence * 100).toFixed(0)}%)`);
  
  if (detection.details) {
    const details = detection.details;
    
    switch (detection.type) {
      case 'android':
        console.log(`   Language: ${details.language || 'unknown'}`);
        if (details.packageName) {
          console.log(`   Package: ${details.packageName}`);
        }
        if (details.sourceDirs && details.sourceDirs.length > 0) {
          console.log(`   Source dirs: ${details.sourceDirs.join(', ')}`);
        }
        if (details.libFiles && details.libFiles.length > 0) {
          console.log(`   Libraries: ${details.libFiles.length} .aar/.jar files`);
        }
        break;
        
      case 'nodejs':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        if (details.hasTypescript) {
          console.log(`   TypeScript: enabled`);
        }
        break;
        
      case 'nodejs-monorepo':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.monorepoTool) {
          console.log(`   Monorepo tool: ${details.monorepoTool}`);
        }
        if (details.workspaceCount) {
          console.log(`   Workspaces: ${details.workspaceCount}`);
        }
        if (details.framework) {
          console.log(`   Framework: ${details.framework}`);
        }
        break;
        
      case 'python-poetry':
      case 'python-pip':
      case 'python-conda':
        if (details.name) {
          console.log(`   Project: ${details.name}@${details.version || '?'}`);
        }
        if (details.packageManager) {
          console.log(`   Package manager: ${details.packageManager}`);
        }
        if (details.dependencies) {
          console.log(`   Dependencies: ${details.dependencies}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'django':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Django`);
        if (details.djangoApps && details.djangoApps.length > 0) {
          console.log(`   Django apps: ${details.djangoApps.join(', ')}`);
        }
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'flask':
        if (details.name) {
          console.log(`   Project: ${details.name}`);
        }
        console.log(`   Framework: Flask`);
        if (details.hasVirtualEnv) {
          console.log(`   Virtual environment: detected`);
        }
        break;
        
      case 'rust':
        if (details.name) {
          console.log(`   Package: ${details.name}@${details.version || '?'}`);
        }
        if (details.edition) {
          console.log(`   Rust edition: ${details.edition}`);
        }
        if (details.isWorkspace) {
          console.log(`   Cargo workspace: detected`);
        }
        break;
        
      case 'go':
        if (details.module) {
          console.log(`   Module: ${details.module}`);
        }
        if (details.goVersion) {
          console.log(`   Go version: ${details.goVersion}`);
        }
        break;
        
      case 'dotnet':
        if (details.language) {
          console.log(`   Language: ${details.language}`);
        }
        if (details.projectFiles && details.projectFiles.length > 0) {
          console.log(`   Project files: ${details.projectFiles.join(', ')}`);
        }
        if (details.hasSolution) {
          console.log(`   Solution: detected`);
        }
        break;
        
      case 'flutter':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        break;
        
      case 'react-native':
        if (details.name) {
          console.log(`   App: ${details.name}@${details.version || '?'}`);
        }
        if (details.reactNativeVersion) {
          console.log(`   React Native: ${details.reactNativeVersion}`);
        }
        break;
    }
  }
  
  if (detection.allDetections && detection.allDetections.length > 1) {
    console.log(`   Other possibilities: ${detection.allDetections.slice(1).map(d => d.type).join(', ')}`);
  }
  
  console.log('');
}

/**
 * Parses YAML-like content from ENVIRONMENT.md
 * @param {string} content - The raw content of ENVIRONMENT.md
 * @returns {object} Parsed key-value pairs
 */
function parseEnvironmentYaml(content) {
  const result = {};
  const lines = content.split('\n');
  
  for (const line of lines) {
    const trimmed = line.trim();
    if (trimmed && !trimmed.startsWith('#') && trimmed.includes(':')) {
      const [key, ...valueParts] = trimmed.split(':');
      const value = valueParts.join(':').trim();
      
      // Remove quotes if present
      const cleanValue = value.replace(/^["']|["']$/g, '');
      result[key.trim()] = cleanValue;
    }
  }
  
  return result;
}

/**
 * Loads and processes the .eck directory manifest
 * @param {string} repoPath - Path to the repository
 * @returns {Promise<object|null>} The eck manifest object or null if no .eck directory
 */
export async function loadProjectEckManifest(repoPath) {
  const eckDir = path.join(repoPath, '.eck');
  
  try {
    // Check if .eck directory exists
    const eckStats = await fs.stat(eckDir);
    if (!eckStats.isDirectory()) {
      return null;
    }
    
    console.log('üìã Found .eck directory - loading project manifest...');
    
    const manifest = {
      environment: {},
      context: '',
      operations: '',
      journal: ''
    };
    
    // Define the files to check
    const files = [
      { name: 'ENVIRONMENT.md', key: 'environment', parser: parseEnvironmentYaml },
      { name: 'CONTEXT.md', key: 'context', parser: content => content },
      { name: 'OPERATIONS.md', key: 'operations', parser: content => content },
      { name: 'JOURNAL.md', key: 'journal', parser: content => content }
    ];
    
    // Process each file
    for (const file of files) {
      const filePath = path.join(eckDir, file.name);
      try {
        const content = await fs.readFile(filePath, 'utf-8');
        manifest[file.key] = file.parser(content.trim());
        console.log(`   ‚úÖ Loaded ${file.name}`);
      } catch (error) {
        // File doesn't exist or can't be read - that's okay, use default
        console.log(`   ‚ö†Ô∏è  ${file.name} not found or unreadable`);
      }
    }
    
    return manifest;
  } catch (error) {
    // .eck directory doesn't exist - that's normal
    return null;
  }
}

/**
 * Ensures that 'snapshots/' is added to the target project's .gitignore file
 * @param {string} repoPath - Path to the repository
 */
export async function ensureSnapshotsInGitignore(repoPath) {
  const gitignorePath = path.join(repoPath, '.gitignore');
  const entryToAdd = '.eck/';
  const comment = '# Added by eck-snapshot to ignore metadata directory';
  
  try {
    // Check if the repo is a Git repository first
    const isGitRepo = await checkGitRepository(repoPath);
    if (!isGitRepo) {
      // Not a Git repo, skip .gitignore modification
      return;
    }
    
    let gitignoreContent = '';
    let fileExists = true;
    
    // Try to read existing .gitignore file
    try {
      gitignoreContent = await fs.readFile(gitignorePath, 'utf-8');
    } catch (error) {
      // File doesn't exist, we'll create it
      fileExists = false;
      gitignoreContent = '';
    }
    
    // Check if 'snapshots/' is already in the file
    const lines = gitignoreContent.split('\n');
    const hasSnapshotsEntry = lines.some(line => line.trim() === entryToAdd);
    
    if (!hasSnapshotsEntry) {
      // Add the entry
      let newContent = gitignoreContent;
      
      // If file exists and doesn't end with newline, add one
      if (fileExists && gitignoreContent && !gitignoreContent.endsWith('\n')) {
        newContent += '\n';
      }
      
      // Add comment and entry
      if (fileExists && gitignoreContent) {
        newContent += '\n';
      }
      newContent += comment + '\n' + entryToAdd + '\n';
      
      await fs.writeFile(gitignorePath, newContent);
      console.log(`‚úÖ Added '${entryToAdd}' to .gitignore`);
    }
  } catch (error) {
    // Silently fail - don't break the snapshot process if gitignore update fails
    console.warn(`‚ö†Ô∏è  Warning: Could not update .gitignore: ${error.message}`);
  }
}

// Helper function to determine if a string is a glob pattern
function isGlob(str) {
  return str.includes('*') || str.includes('?') || str.includes('{');
}

/**
 * Applies advanced profile filtering (multi-profile, exclusion, and ad-hoc globs) to a file list.
 */
export async function applyProfileFilter(allFiles, profileString, repoPath) {
  const profileParts = profileString.split(',').map(p => p.trim()).filter(Boolean);
  
  const includeGlobs = [];
  const excludeGlobs = [];
  const includeNames = [];
  const excludeNames = [];

  // Step 1: Differentiate between profile names and ad-hoc glob patterns
  for (const part of profileParts) {
    const isNegative = part.startsWith('-');
    const pattern = isNegative ? part.substring(1) : part;

    if (isGlob(pattern)) {
      if (isNegative) {
        excludeGlobs.push(pattern);
      } else {
        includeGlobs.push(pattern);
      }
    } else {
      if (isNegative) {
        excludeNames.push(pattern);
      } else {
        includeNames.push(pattern);
      }
    }
  }

  let workingFiles = [];
  let finalIncludes = [...includeGlobs];
  let finalExcludes = [...excludeGlobs];

  // Step 2: Load patterns from specified profile names
  const allProfileNames = [...new Set([...includeNames, ...excludeNames])];
  const profiles = new Map();
  for (const name of allProfileNames) {
    const profile = await getProfile(name, repoPath);
    if (profile) {
      profiles.set(name, profile);
    } else {
      // This is an ad-hoc glob, not a profile, so no warning is needed.
      if (!isGlob(name)) {
        console.warn(`‚ö†Ô∏è Warning: Profile '${name}' not found and will be skipped.`);
      }
    }
  }

  for (const name of includeNames) {
    if (profiles.has(name)) {
      finalIncludes.push(...(profiles.get(name).include || []));
      finalExcludes.push(...(profiles.get(name).exclude || []));
    }
  }
  for (const name of excludeNames) {
    if (profiles.has(name)) {
      finalExcludes.push(...(profiles.get(name).include || []));
    }
  }
  
  // Step 3: Apply the filtering logic
  if (finalIncludes.length > 0) {
    workingFiles = micromatch(allFiles, finalIncludes);
  } else if (includeNames.length > 0 && includeGlobs.length === 0) {
    workingFiles = [];
  } else {
    workingFiles = allFiles;
  }

  if (finalExcludes.length > 0) {
    workingFiles = micromatch.not(workingFiles, finalExcludes);
  }

  return workingFiles;
}

/**
 * Automatically initializes the .eck manifest directory, attempting dynamic generation via Claude.
 * @param {string} projectPath - Path to the project
 */
export async function initializeEckManifest(projectPath) {
  const eckDir = path.join(projectPath, '.eck');
  
  try {
    // Check if .eck directory already exists and has all required files
    let needsInitialization = false;
    try {
      const eckStats = await fs.stat(eckDir);
      if (eckStats.isDirectory()) {
        // Directory exists, check if all required files are present
        const requiredFiles = ['ENVIRONMENT.md', 'CONTEXT.md', 'OPERATIONS.md', 'JOURNAL.md'];
        for (const fileName of requiredFiles) {
          try {
            await fs.stat(path.join(eckDir, fileName));
          } catch (error) {
            console.log(`   ‚ÑπÔ∏è Missing ${fileName}, initialization needed`);
            needsInitialization = true;
            break;
          }
        }
        if (!needsInitialization) {
          // All files exist, no need to initialize
          return;
        }
      }
    } catch (error) {
      // Directory doesn't exist, we'll create it
      needsInitialization = true;
    }
    
    // Create .eck directory
    await fs.mkdir(eckDir, { recursive: true });
    console.log('üìã Initializing .eck manifest directory...');

    // --- NEW HYBRID LOGIC --- 
    // 1. Run static analysis first to gather facts.
    let staticFacts = {};
    try {
      staticFacts = await detectProjectType(projectPath);
      console.log(`   üîç Static analysis complete. Detected type: ${staticFacts.type}`);
    } catch (e) {
      console.warn(`   ‚ö†Ô∏è Static project detection failed: ${e.message}. Proceeding with generic prompts.`);
    }

    // Prevent AI hallucination by removing low-confidence "other possibilities"
    if (staticFacts && staticFacts.allDetections) {
      delete staticFacts.allDetections;
    }
    
    const staticFactsJson = JSON.stringify(staticFacts, null, 2);
    // --- END NEW LOGIC ---
    
    // Template files with their content
    const templateFiles = [
      {
        name: 'ENVIRONMENT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw YAML key-value content for an .eck/ENVIRONMENT.md file. Only include detected facts. DO NOT add any keys that are not present in the facts. DO NOT add conversational text or markdown wrappers. Your response MUST start directly with a YAML key (e.g., 'project_type: ...').`,
        content: `# This file is for environment overrides. Add agent-specific settings here.\nagent_id: local_dev\n` // Simple static fallback
      },
      {
        name: 'CONTEXT.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/CONTEXT.md file. Use the facts to write ## Description, ## Architecture, and ## Key Technologies. DO NOT add conversational text (like "Here is the file..."). Your response MUST start *directly* with the '# Project Overview' heading.`,
        content: `# Project Overview

## Description
Brief description of what this project does and its main purpose.

## Architecture
High-level overview of the system architecture, key components, and how they interact.

## Key Technologies
- Technology 1
- Technology 2
- Technology 3

## Important Notes
Any crucial information that developers should know when working on this project.
`
      },
      {
        name: 'OPERATIONS.md',
        prompt: `Given these static project analysis facts (especially package.json scripts):
${staticFactsJson}

Generate the raw Markdown content ONLY for a .eck/OPERATIONS.md file. DO NOT add conversational text. Your response MUST start *directly* with the '# Common Operations' heading. List commands for ## Development Setup, ## Running the Project, and ## Testing.`,
        content: `# Common Operations

## Development Setup
\`\`\`bash
# Setup commands
npm install
# or yarn install
\`\`\`

## Running the Project
\`\`\`bash
# Development mode
npm run dev

# Production build
npm run build
\`\`\`

## Testing
\`\`\`bash
# Run tests
npm test

# Run tests in watch mode
npm run test:watch
\`\`\`

## Deployment
\`\`\`bash
# Deployment commands
npm run deploy
\`\`\`

## Troubleshooting
Common issues and their solutions.
`
      },
      {
        name: 'JOURNAL.md',
        content: `# Development Journal

## Recent Changes
Track significant changes, decisions, and progress here.

---

### YYYY-MM-DD - Project Started
- Initial project setup
- Added basic structure
`
      },
      {
        name: 'ROADMAP.md',
        prompt: `Given these static project analysis facts:\n${staticFactsJson}\n\nGenerate the raw Markdown content ONLY for a .eck/ROADMAP.md file. DO NOT add conversational text. Start *directly* with '# Project Roadmap'. Propose 1-2 *plausible* placeholder items for ## Current Sprint/Phase and ## Next Phase based on the project type.`,
        content: `# Project Roadmap

## Current Sprint/Phase
- [ ] Feature 1
- [ ] Feature 2
- [ ] Bug fix 1

## Next Phase
- [ ] Future feature 1
- [ ] Future feature 2

## Long-term Goals
- [ ] Major milestone 1
- [ ] Major milestone 2

## Completed
- [x] Project initialization
`
      },
      {
        name: 'TECH_DEBT.md',
        prompt: `Generate the raw Markdown content ONLY for a .eck/TECH_DEBT.md file. DO NOT add conversational text. Start *directly* with '# Technical Debt'. Propose 1-2 *common* placeholder items for ## Code Quality Issues and ## Refactoring Opportunities.`,
        content: `# Technical Debt

## Current Technical Debt
Track technical debt, refactoring needs, and code quality issues.

### Code Quality Issues
- Issue 1: Description and priority
- Issue 2: Description and priority

### Refactoring Opportunities
- Opportunity 1: Description and impact
- Opportunity 2: Description and impact

### Performance Issues
- Performance issue 1: Description and impact
- Performance issue 2: Description and impact

### Security Concerns
- Security concern 1: Description and priority
- Security concern 2: Description and priority

## Resolved
- [x] Resolved issue 1
`
      }
    ];
    
    // Create each template file (only if it doesn't exist)
    for (const file of templateFiles) {
      const filePath = path.join(eckDir, file.name);
      
      // Skip if file already exists
      try {
        await fs.stat(filePath);
        console.log(`   ‚úÖ ${file.name} already exists, skipping`);
        continue;
      } catch (error) {
        // File doesn't exist, create it
      }
      
      let fileContent = file.content; // Start with fallback
      let generatedByAI = false;

      // For files with a prompt, try to dynamically generate
      if (file.prompt) {
        try {
          console.log(`   üß† Attempting to auto-generate ${file.name} via Claude...`);
          const aiResponseObject = await dispatchAnalysisTask(file.prompt); // Use the prompt
          const rawText = aiResponseObject.result || aiResponseObject.response_text; // Handle both Claude and GPT responses
          
          if (!rawText || typeof rawText.replace !== 'function') {
             throw new Error(`AI returned invalid content type: ${typeof rawText}`);
          }

          // Basic cleanup of potential markdown code blocks from Claude
          const cleanedResponse = rawText.replace(/^```(markdown|yaml)?\n|```$/g, '').trim();
          
          if (cleanedResponse) {
            fileContent = cleanedResponse;
            generatedByAI = true;
            console.log(`   ‚ú® AI successfully generated ${file.name}`);
          } else {
            throw new Error('AI returned empty content.');
          }
        } catch (error) {
          console.warn(`   ‚ö†Ô∏è AI generation failed for ${file.name}: ${error.message}. Using static template.`);
          // fileContent is already set to the fallback
        }
      }
      
      await fs.writeFile(filePath, fileContent);
      if (!generatedByAI) {
          console.log(`   ‚úÖ Created ${file.name} (static template)`);
      }
    }
    
    console.log('üìã .eck manifest initialized! Edit the files to provide project-specific context.');
    
  } catch (error) {
    // Silently fail - don't break the snapshot process if manifest initialization fails
    console.warn(`‚ö†Ô∏è  Warning: Could not initialize .eck manifest: ${error.message}`);
  }
}


--- File: /src/utils/projectDetector.js ---

import fs from 'fs/promises';
import path from 'path';
import { loadSetupConfig } from '../config.js';

/**
 * Detects the type of project based on file structure and configuration
 * @param {string} projectPath - Path to the project root
 * @returns {Promise<{type: string, confidence: number, details: object}>}
 */
export async function detectProjectType(projectPath = '.') {
  const config = await loadSetupConfig();
  const patterns = config.projectDetection?.patterns || {};
  
  const detections = [];
  
  for (const [type, pattern] of Object.entries(patterns)) {
    const score = await calculateTypeScore(projectPath, pattern);
    if (score > 0) {
      detections.push({
        type,
        score,
        priority: pattern.priority || 0,
        details: await getProjectDetails(projectPath, type)
      });
    }
  }
  
  // Sort by priority and score
  detections.sort((a, b) => (b.priority * 10 + b.score) - (a.priority * 10 + a.score));

  if (detections.length === 0) {
    return {
      type: 'unknown',
      confidence: 0,
      details: {}
    };
  }

  const best = detections[0];

  // Special handling for mixed monorepos
  const isLikelyMonorepo = detections.length > 1 && detections.some(d => d.score >= 40);

  if (isLikelyMonorepo) {
    // If we have multiple strong detections, prefer the highest priority with substantial evidence
    const strongDetections = detections.filter(d => d.score >= 40);
    if (strongDetections.length > 1) {
      const primaryType = strongDetections[0].type;
      return {
        type: primaryType,
        confidence: Math.min(strongDetections[0].score / 100, 1.0),
        details: {
          ...strongDetections[0].details,
          isMonorepo: true,
          additionalTypes: strongDetections.slice(1).map(d => d.type)
        },
        allDetections: detections
      };
    }
  }

  // Boost confidence for strong workspace indicators
  if (best.details && (best.details.isWorkspace || best.details.workspaceSize)) {
    const boostedScore = best.score + 20; // Bonus for workspace structure
    return {
      type: best.type,
      confidence: Math.min(boostedScore / 100, 1.0),
      details: best.details,
      allDetections: detections
    };
  }

  return {
    type: best.type,
    confidence: Math.min(best.score / 100, 1.0),
    details: best.details,
    allDetections: detections
  };
}

/**
 * Calculates a score for how well a project matches a specific type pattern
 */
async function calculateTypeScore(projectPath, pattern) {
  let score = 0;

  // Check for required files (check both root and common subdirectories)
  if (pattern.files) {
    for (const file of pattern.files) {
      // Check in root directory first
      const rootExists = await fileExists(path.join(projectPath, file));
      if (rootExists) {
        score += 25; // Each required file adds points
      } else {
        // For Cargo.toml and other project files, also check common subdirectory patterns
        const commonSubdirs = ['src', 'lib', 'app', 'core', 'backend', 'frontend'];
        // Add project-type specific subdirectories
        if (file === 'Cargo.toml') {
          commonSubdirs.push('codex-rs', 'rust', 'server', 'api');
        }
        if (file === 'package.json') {
          commonSubdirs.push('codex-cli', 'cli', 'client', 'web', 'ui');
        }

        for (const subdir of commonSubdirs) {
          const subdirExists = await fileExists(path.join(projectPath, subdir, file));
          if (subdirExists) {
            score += 20; // Slightly lower score for subdirectory finds
            break; // Only count once per file type
          }
        }
      }
    }
  }

  // Check for required directories (check both root and one level deep)
  if (pattern.directories) {
    for (const dir of pattern.directories) {
      const rootExists = await directoryExists(path.join(projectPath, dir));
      if (rootExists) {
        score += 20; // Each required directory adds points
      } else {
        // Check in common project subdirectories
        const projectSubdirs = ['codex-rs', 'codex-cli', 'src', 'lib', 'app'];
        for (const projDir of projectSubdirs) {
          const subdirExists = await directoryExists(path.join(projectPath, projDir, dir));
          if (subdirExists) {
            score += 15; // Lower score for nested directory finds
            break;
          }
        }
      }
    }
  }
  
  // Check for manifest files (Android specific) - limit search depth
  if (pattern.manifestFiles) {
    for (const manifest of pattern.manifestFiles) {
      const manifestPath = await findFileRecursive(projectPath, manifest, 2); // Reduced to 2 levels
      if (manifestPath) {
        score += 30; // Manifest files are strong indicators
      }
    }
  }
  
  // Check for content patterns in package.json (React Native, etc.)
  if (pattern.patterns) {
    try {
      const packageJsonPath = path.join(projectPath, 'package.json');
      const packageContent = await fs.readFile(packageJsonPath, 'utf-8');
      const packageJson = JSON.parse(packageContent);
      
      for (const patternText of pattern.patterns) {
        const allDeps = {
          ...packageJson.dependencies,
          ...packageJson.devDependencies,
          ...packageJson.peerDependencies
        };
        
        // Check for exact dependency names (more precise matching)
        const foundInDeps = Object.keys(allDeps).some(dep => dep === patternText || dep.startsWith(patternText + '/'));
        // Only check for exact matches in keywords array, not description (too broad)
        const foundInKeywords = packageJson.keywords && Array.isArray(packageJson.keywords)
          ? packageJson.keywords.some(keyword => keyword.toLowerCase() === patternText.toLowerCase())
          : false;
        
        if (foundInDeps || foundInKeywords) {
          score += 25; // Higher score for actual dependencies
        }
      }
    } catch (error) {
      // Ignore if package.json doesn't exist or is malformed
    }
  }
  
  return score;
}

/**
 * Gets detailed information about the detected project type
 */
async function getProjectDetails(projectPath, type) {
  const details = { type };
  
  switch (type) {
    case 'android':
      return await getAndroidDetails(projectPath);
    case 'nodejs':
      return await getNodejsDetails(projectPath);
    case 'flutter':
      return await getFlutterDetails(projectPath);
    case 'react-native':
      return await getReactNativeDetails(projectPath);
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
    case 'django':
    case 'flask':
      return await getPythonDetails(projectPath, type);
    case 'rust':
      return await getRustDetails(projectPath);
    case 'go':
      return await getGoDetails(projectPath);
    case 'dotnet':
      return await getDotnetDetails(projectPath);
    default:
      return details;
  }
}

async function getAndroidDetails(projectPath) {
  const details = { type: 'android' };
  
  try {
    // Check build.gradle files
    const buildGradleFiles = [];
    const appBuildGradle = path.join(projectPath, 'app', 'build.gradle');
    const appBuildGradleKts = path.join(projectPath, 'app', 'build.gradle.kts');
    
    if (await fileExists(appBuildGradle)) {
      buildGradleFiles.push('app/build.gradle');
      const content = await fs.readFile(appBuildGradle, 'utf-8');
      details.language = content.includes('kotlin') ? 'kotlin' : 'java';
    }
    
    if (await fileExists(appBuildGradleKts)) {
      buildGradleFiles.push('app/build.gradle.kts');
      details.language = 'kotlin';
    }
    
    details.buildFiles = buildGradleFiles;
    
    // Check for source directories
    const sourceDirs = [];
    const kotlinDir = path.join(projectPath, 'app', 'src', 'main', 'kotlin');
    const javaDir = path.join(projectPath, 'app', 'src', 'main', 'java');
    
    if (await directoryExists(kotlinDir)) {
      sourceDirs.push('app/src/main/kotlin');
    }
    if (await directoryExists(javaDir)) {
      sourceDirs.push('app/src/main/java');
    }
    
    details.sourceDirs = sourceDirs;
    
    // Check for AndroidManifest.xml
    const manifestPath = path.join(projectPath, 'app', 'src', 'main', 'AndroidManifest.xml');
    if (await fileExists(manifestPath)) {
      details.hasManifest = true;
      
      // Extract package name from manifest
      try {
        const manifestContent = await fs.readFile(manifestPath, 'utf-8');
        const packageMatch = manifestContent.match(/package="([^"]+)"/);
        if (packageMatch) {
          details.packageName = packageMatch[1];
        }
      } catch (error) {
        // Ignore parsing errors
      }
    }
    
    // Check for libs directory
    const libsDir = path.join(projectPath, 'app', 'libs');
    if (await directoryExists(libsDir)) {
      details.hasLibs = true;
      try {
        const libFiles = await fs.readdir(libsDir);
        details.libFiles = libFiles.filter(f => f.endsWith('.aar') || f.endsWith('.jar'));
      } catch (error) {
        // Ignore
      }
    }
    
  } catch (error) {
    console.warn('Error getting Android project details:', error.message);
  }
  
  return details;
}

async function getNodejsDetails(projectPath) {
  const details = { type: 'nodejs' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.hasTypescript = !!packageJson.devDependencies?.typescript || !!packageJson.dependencies?.typescript;
    details.framework = detectNodejsFramework(packageJson);
    
    // Check if it's a monorepo - be more strict
    const hasWorkspaces = !!packageJson.workspaces;
    const hasLerna = await fileExists(path.join(projectPath, 'lerna.json')) || !!packageJson.lerna;
    const hasNx = await fileExists(path.join(projectPath, 'nx.json'));
    const hasRush = await fileExists(path.join(projectPath, 'rush.json'));
    const hasPackagesDir = await directoryExists(path.join(projectPath, 'packages'));
    const hasAppsDir = await directoryExists(path.join(projectPath, 'apps'));
    const hasLibsDir = await directoryExists(path.join(projectPath, 'libs'));
    
    // Check if packages/apps/libs directories contain actual packages
    let hasSubPackages = false;
    
    for (const dir of ['packages', 'apps', 'libs']) {
      const dirPath = path.join(projectPath, dir);
      if (await directoryExists(dirPath)) {
        try {
          const entries = await fs.readdir(dirPath, { withFileTypes: true });
          for (const entry of entries) {
            if (entry.isDirectory()) {
              const packageJsonPath = path.join(dirPath, entry.name, 'package.json');
              if (await fileExists(packageJsonPath)) {
                hasSubPackages = true;
                break;
              }
            }
          }
          if (hasSubPackages) break;
        } catch (error) {
          // Ignore
        }
      }
    }
    
    // Only consider it a monorepo if it has workspace configuration AND actual sub-packages
    details.isMonorepo = !!(
      (hasWorkspaces || hasLerna || hasNx || hasRush) &&
      hasSubPackages
    );
    
    if (details.isMonorepo) {
      details.type = 'nodejs-monorepo';
      
      // Count workspaces
      if (packageJson.workspaces) {
        if (Array.isArray(packageJson.workspaces)) {
          details.workspaceCount = packageJson.workspaces.length;
        } else if (packageJson.workspaces.packages) {
          details.workspaceCount = packageJson.workspaces.packages.length;
        }
      }
      
      // Detect monorepo tool
      if (hasLerna) {
        details.monorepoTool = 'lerna';
      } else if (hasNx) {
        details.monorepoTool = 'nx';
      } else if (hasRush) {
        details.monorepoTool = 'rush';
      } else if (hasWorkspaces) {
        details.monorepoTool = 'npm-workspaces';
      }
    }
    
  } catch (error) {
    console.warn('Error getting Node.js project details:', error.message);
  }
  
  return details;
}

async function getFlutterDetails(projectPath) {
  const details = { type: 'flutter' };
  
  try {
    const pubspecPath = path.join(projectPath, 'pubspec.yaml');
    const content = await fs.readFile(pubspecPath, 'utf-8');
    
    // Basic parsing of pubspec.yaml
    const nameMatch = content.match(/^name:\s*(.+)$/m);
    if (nameMatch) {
      details.name = nameMatch[1].trim();
    }
    
    const versionMatch = content.match(/^version:\s*(.+)$/m);
    if (versionMatch) {
      details.version = versionMatch[1].trim();
    }
    
  } catch (error) {
    console.warn('Error getting Flutter project details:', error.message);
  }
  
  return details;
}

async function getReactNativeDetails(projectPath) {
  const details = { type: 'react-native' };
  
  try {
    const packageJsonPath = path.join(projectPath, 'package.json');
    const content = await fs.readFile(packageJsonPath, 'utf-8');
    const packageJson = JSON.parse(content);
    
    details.name = packageJson.name;
    details.version = packageJson.version;
    details.reactNativeVersion = packageJson.dependencies?.['react-native'];
    details.hasTypescript = !!packageJson.devDependencies?.typescript;
    
  } catch (error) {
    console.warn('Error getting React Native project details:', error.message);
  }
  
  return details;
}

function detectNodejsFramework(packageJson) {
  const deps = { ...packageJson.dependencies, ...packageJson.devDependencies };
  
  if (deps.express) return 'express';
  if (deps.next) return 'next.js';
  if (deps.nuxt) return 'nuxt.js';
  if (deps.vue) return 'vue';
  if (deps.react) return 'react';
  if (deps.electron) return 'electron';
  if (deps.fastify) return 'fastify';
  if (deps.koa) return 'koa';
  if (deps.hapi) return 'hapi';
  
  return 'node.js';
}

async function getPythonDetails(projectPath, type) {
  const details = { type };
  
  try {
    // Check for Poetry project
    if (type === 'python-poetry') {
      const pyprojectPath = path.join(projectPath, 'pyproject.toml');
      const content = await fs.readFile(pyprojectPath, 'utf-8');
      
      // Basic TOML parsing for project name and version
      const nameMatch = content.match(/name\s*=\s*"([^"]+)"/);
      const versionMatch = content.match(/version\s*=\s*"([^"]+)"/);
      
      if (nameMatch) details.name = nameMatch[1];
      if (versionMatch) details.version = versionMatch[1];
      
      details.packageManager = 'poetry';
    }
    
    // Check for requirements.txt
    if (await fileExists(path.join(projectPath, 'requirements.txt'))) {
      const reqContent = await fs.readFile(path.join(projectPath, 'requirements.txt'), 'utf-8');
      details.dependencies = reqContent.split('\n').filter(line => line.trim() && !line.startsWith('#')).length;
    }
    
    // Check for Django
    if (type === 'django' || await fileExists(path.join(projectPath, 'manage.py'))) {
      details.framework = 'django';
      details.type = 'django';
      
      // Look for Django apps
      try {
        const entries = await fs.readdir(projectPath, { withFileTypes: true });
        const djangoApps = [];
        
        for (const entry of entries) {
          if (entry.isDirectory() && !entry.name.startsWith('.')) {
            const appPath = path.join(projectPath, entry.name);
            if (await fileExists(path.join(appPath, 'models.py')) || 
                await fileExists(path.join(appPath, 'views.py'))) {
              djangoApps.push(entry.name);
            }
          }
        }
        
        details.djangoApps = djangoApps;
      } catch (error) {
        // Ignore
      }
    }
    
    // Check for Flask
    if (type === 'flask' || await fileExists(path.join(projectPath, 'app.py'))) {
      details.framework = 'flask';
      details.type = 'flask';
    }
    
    // Check for virtual environment
    if (await directoryExists(path.join(projectPath, 'venv')) ||
        await directoryExists(path.join(projectPath, '.venv')) ||
        await directoryExists(path.join(projectPath, 'env'))) {
      details.hasVirtualEnv = true;
    }
    
  } catch (error) {
    console.warn('Error getting Python project details:', error.message);
  }
  
  return details;
}

async function getRustDetails(projectPath) {
  const details = { type: 'rust' };

  try {
    // Check both root and common subdirectories for Cargo.toml
    let cargoPath = path.join(projectPath, 'Cargo.toml');
    let cargoContent = null;

    if (await fileExists(cargoPath)) {
      cargoContent = await fs.readFile(cargoPath, 'utf-8');
    } else {
      // Check common Rust project subdirectories
      const rustSubdirs = ['codex-rs', 'rust', 'src', 'core', 'server'];
      for (const subdir of rustSubdirs) {
        const subdirCargoPath = path.join(projectPath, subdir, 'Cargo.toml');
        if (await fileExists(subdirCargoPath)) {
          cargoPath = subdirCargoPath;
          cargoContent = await fs.readFile(subdirCargoPath, 'utf-8');
          details.primaryLocation = subdir;
          break;
        }
      }
    }

    if (!cargoContent) {
      return details;
    }

    const nameMatch = cargoContent.match(/name\s*=\s*"([^"]+)"/);
    const versionMatch = cargoContent.match(/version\s*=\s*"([^"]+)"/);
    const editionMatch = cargoContent.match(/edition\s*=\s*"([^"]+)"/);

    if (nameMatch) details.name = nameMatch[1];
    if (versionMatch) details.version = versionMatch[1];
    if (editionMatch) details.edition = editionMatch[1];

    // Check if it's a workspace
    if (cargoContent.includes('[workspace]')) {
      details.isWorkspace = true;

      // Count workspace members
      const workspaceMatch = cargoContent.match(/members\s*=\s*\[([\s\S]*?)\]/);
      if (workspaceMatch) {
        const members = workspaceMatch[1].split(',').map(m => m.trim().replace(/"/g, '')).filter(m => m);
        details.workspaceMembers = members.length;
      }
    }

    // Check for multiple Cargo.toml files (indicates workspace structure)
    if (details.primaryLocation) {
      const subdirPath = path.join(projectPath, details.primaryLocation);
      try {
        const subdirs = await fs.readdir(subdirPath, { withFileTypes: true });
        let cargoCount = 0;
        for (const entry of subdirs) {
          if (entry.isDirectory()) {
            const memberCargoPath = path.join(subdirPath, entry.name, 'Cargo.toml');
            if (await fileExists(memberCargoPath)) {
              cargoCount++;
            }
          }
        }
        if (cargoCount > 3) { // If many workspace members, this is definitely a Rust project
          details.workspaceSize = 'large';
        }
      } catch (error) {
        // Ignore
      }
    }

  } catch (error) {
    console.warn('Error getting Rust project details:', error.message);
  }

  return details;
}

async function getGoDetails(projectPath) {
  const details = { type: 'go' };
  
  try {
    const goModPath = path.join(projectPath, 'go.mod');
    const content = await fs.readFile(goModPath, 'utf-8');
    
    const moduleMatch = content.match(/module\s+([^\s\n]+)/);
    const goVersionMatch = content.match(/go\s+([0-9.]+)/);
    
    if (moduleMatch) details.module = moduleMatch[1];
    if (goVersionMatch) details.goVersion = goVersionMatch[1];
    
  } catch (error) {
    console.warn('Error getting Go project details:', error.message);
  }
  
  return details;
}

async function getDotnetDetails(projectPath) {
  const details = { type: 'dotnet' };
  
  try {
    // Look for project files
    const entries = await fs.readdir(projectPath);
    const projectFiles = entries.filter(file => 
      file.endsWith('.csproj') || 
      file.endsWith('.fsproj') || 
      file.endsWith('.vbproj')
    );
    
    if (projectFiles.length > 0) {
      details.projectFiles = projectFiles;
      
      // Determine language
      if (projectFiles.some(f => f.endsWith('.csproj'))) {
        details.language = 'C#';
      } else if (projectFiles.some(f => f.endsWith('.fsproj'))) {
        details.language = 'F#';
      } else if (projectFiles.some(f => f.endsWith('.vbproj'))) {
        details.language = 'VB.NET';
      }
    }
    
    // Check for solution file
    const solutionFiles = entries.filter(file => file.endsWith('.sln'));
    if (solutionFiles.length > 0) {
      details.hasSolution = true;
      details.solutionFiles = solutionFiles;
    }
    
  } catch (error) {
    console.warn('Error getting .NET project details:', error.message);
  }
  
  return details;
}

// Utility functions
async function fileExists(filePath) {
  try {
    await fs.access(filePath);
    return true;
  } catch {
    return false;
  }
}

async function directoryExists(dirPath) {
  try {
    const stat = await fs.stat(dirPath);
    return stat.isDirectory();
  } catch {
    return false;
  }
}

async function findFileRecursive(basePath, fileName, maxDepth = 3) {
  const searchInDir = async (currentPath, depth) => {
    if (depth > maxDepth) return null;
    
    try {
      const items = await fs.readdir(currentPath, { withFileTypes: true });
      
      // First, check if the file exists in current directory
      if (items.some(item => item.name === fileName && item.isFile())) {
        return path.join(currentPath, fileName);
      }
      
      // Then search in subdirectories
      for (const item of items) {
        if (item.isDirectory() && !item.name.startsWith('.')) {
          const found = await searchInDir(path.join(currentPath, item.name), depth + 1);
          if (found) return found;
        }
      }
    } catch (error) {
      // Ignore permission errors
    }
    
    return null;
  };
  
  return await searchInDir(basePath, 0);
}

/**
 * Gets project-specific filtering configuration
 * @param {string} projectType - The detected project type
 * @returns {object} Project-specific filtering rules
 */
export async function getProjectSpecificFiltering(projectType) {
  const config = await loadSetupConfig();
  const projectSpecific = config.fileFiltering?.projectSpecific?.[projectType];
  
  if (!projectSpecific) {
    return {
      filesToIgnore: [],
      dirsToIgnore: [],
      extensionsToIgnore: []
    };
  }
  
  return {
    filesToIgnore: projectSpecific.filesToIgnore || [],
    dirsToIgnore: projectSpecific.dirsToIgnore || [],
    extensionsToIgnore: projectSpecific.extensionsToIgnore || []
  };
}

--- File: /src/utils/tokenEstimator.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

/**
 * Adaptive token estimation system with project-specific polynomials
 */

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
const ESTIMATION_DATA_FILE = path.join(__dirname, '..', '..', '.eck-token-training.json');

/**
 * Default coefficients for different project types (bytes to tokens ratio)
 * Format: [constant, linear, quadratic, cubic] coefficients
 */
const DEFAULT_COEFFICIENTS = {
  'android': [0, 0.25, 0, 0], // Start with simple 1/4 ratio
  'nodejs': [0, 0.20, 0, 0],
  'python': [0, 0.22, 0, 0],
  'rust': [0, 0.18, 0, 0],
  'go': [0, 0.19, 0, 0],
  'c': [0, 0.23, 0, 0],
  'unknown': [0, 0.25, 0, 0]
};

/**
 * Load training data from file
 */
async function loadTrainingData() {
  try {
    const data = await fs.readFile(ESTIMATION_DATA_FILE, 'utf-8');
    const parsedData = JSON.parse(data);
    // Ensure the structure is complete by merging with defaults
    return {
        coefficients: { ...DEFAULT_COEFFICIENTS, ...parsedData.coefficients },
        trainingPoints: parsedData.trainingPoints || {}
    };
  } catch (error) {
    // If file doesn't exist or is malformed, return default structure
    return {
      coefficients: { ...DEFAULT_COEFFICIENTS },
      trainingPoints: {}
    };
  }
}

/**
 * Save training data to file
 */
async function saveTrainingData(data) {
  await fs.writeFile(ESTIMATION_DATA_FILE, JSON.stringify(data, null, 2));
}

/**
 * Calculate polynomial value
 */
function evaluatePolynomial(coefficients, x) {
  let result = 0;
  for (let i = 0; i < coefficients.length; i++) {
    result += coefficients[i] * Math.pow(x, i);
  }
  return Math.max(0, result); // Ensure non-negative result
}

/**
 * Estimate tokens using project-specific polynomial
 */
export async function estimateTokensWithPolynomial(projectType, fileSizeInBytes) {
  const data = await loadTrainingData();
  const coefficients = data.coefficients[projectType] || data.coefficients['unknown'];
  
  const estimatedTokens = evaluatePolynomial(coefficients, fileSizeInBytes);
  return Math.round(estimatedTokens);
}

/**
 * Generate training command string for data collection
 */
export function generateTrainingCommand(projectType, estimatedTokens, fileSizeInBytes, projectPath) {
  const projectName = path.basename(projectPath);
  
  return `eck-snapshot train-tokens ${projectType} ${fileSizeInBytes} ${estimatedTokens} `;
}

/**
 * Add training point and update polynomial coefficients
 */
export async function addTrainingPoint(projectType, fileSizeInBytes, estimatedTokens, actualTokens) {
  const data = await loadTrainingData();
  
  // Initialize training points array for project type if it doesn't exist
  if (!data.trainingPoints[projectType]) {
    data.trainingPoints[projectType] = [];
  }
  
  // Add new training point
  const trainingPoint = {
    fileSizeInBytes,
    estimatedTokens,
    actualTokens,
    timestamp: new Date().toISOString()
  };
  
  data.trainingPoints[projectType].push(trainingPoint);
  
  // Recalculate coefficients using least squares fitting
  updateCoefficients(data, projectType);
  
  await saveTrainingData(data);
  
  console.log(`‚úÖ Added training point for ${projectType}:`);
  console.log(`   File size: ${fileSizeInBytes} bytes`);
  console.log(`   Estimated: ${estimatedTokens} tokens`);
  console.log(`   Actual: ${actualTokens} tokens`);
  console.log(`   Error: ${Math.abs(actualTokens - estimatedTokens)} tokens (${Math.round(Math.abs(actualTokens - estimatedTokens) / actualTokens * 100)}%)`);
}

/**
 * Update polynomial coefficients using least squares fitting
 * For now, we'll use a simple adaptive approach
 */
function updateCoefficients(data, projectType) {
  const points = data.trainingPoints[projectType];

  if (!points || points.length === 0) {
    // No points, nothing to do.
    return;
  }

  if (points.length === 1) {
    // With one point, use a direct ratio for the linear coefficient.
    const point = points[0];
    if (point.fileSizeInBytes > 0) { // Avoid division by zero
        const ratio = point.actualTokens / point.fileSizeInBytes;
        data.coefficients[projectType] = [
            0, // intercept
            Math.max(0, ratio), // linear term (slope)
            0, 0 // quadratic, cubic
        ];
    }
    return;
  }

  // Use linear regression for 2 or more points.
  let sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
  const n = points.length;

  for (const point of points) {
    const x = point.fileSizeInBytes;
    const y = point.actualTokens;

    sumX += x;
    sumY += y;
    sumXY += x * y;
    sumX2 += x * x;
  }

  const denominator = (n * sumX2 - sumX * sumX);
  if (denominator === 0) return; // Avoid division by zero, can't calculate slope

  // Calculate linear coefficients: y = a + bx
  const slope = (n * sumXY - sumX * sumY) / denominator;
  const intercept = (sumY - slope * sumX) / n;

  // Update coefficients [constant, linear, quadratic, cubic]
  data.coefficients[projectType] = [
    Math.max(0, intercept), // constant term (ensure non-negative)
    Math.max(0, slope),     // linear term (ensure non-negative)
    0,                      // quadratic (not used yet)
    0                       // cubic (not used yet)
  ];
}

/**
 * Show current estimation statistics
 */
export async function showEstimationStats() {
  const data = await loadTrainingData();
  
  console.log('\nüìä Token Estimation Statistics:');
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
  
  for (const [projectType, coefficients] of Object.entries(data.coefficients)) {
    const points = data.trainingPoints[projectType] || [];
    console.log(`\nüî∏ ${projectType}:`);
    console.log(`   Coefficients: [${coefficients.map(c => c.toFixed(6)).join(', ')}]`);
    console.log(`   Training points: ${points.length}`);
    
    if (points.length > 0) {
      const errors = points.map(p => Math.abs(p.actualTokens - p.estimatedTokens));
      const avgError = errors.reduce((a, b) => a + b, 0) / errors.length;
      console.log(`   Average error: ${Math.round(avgError)} tokens`);
    }
  }
  
  console.log('‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ');
}

--- File: /test_knexfile.js ---

module.exports = {
  development: {
    client: 'sqlite3',
    connection: {
      filename: './dev.sqlite3'
    }
  },
  
  staging: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  },
  
  production: {
    client: 'postgresql',
    connection: {
      database: 'my_db',
      user:     'username',
      password: 'password'
    },
    pool: {
      min: 2,
      max: 10
    },
    migrations: {
      tableName: 'knex_migrations'
    }
  }
};


--- File: /test_snapshot.md ---

# Test Snapshot

## Project Overview
This is a test project for demonstrating the Gemini session API integration.

## Files
- `index.js` - Main entry point
- `src/` - Source code directory

## Instructions
You are an AI assistant helping with this test project. Please respond to user queries about the codebase.

--- File: /vitest.config.js ---

import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    testMatch: ['**/*.{test,spec}.js'],
    coverage: {
      reporter: ['text', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        '**/*.config.js',
        '**/*.test.js',
        '**/*.spec.js'
      ]
    }
  }
});



--- File: /test_prune.md/eckSnapshot_snapshot_2025-10-11_12-17-51_780d41e_pruned_50KB.md ---

# AI Instructions

## 1. How to Read This Snapshot

This document is a self-contained, single-file snapshot of the **eckSnapshot** software repository, generated by the `eck-snapshot` tool on **2025-10-11T12:17:51.271Z**. It is designed to provide a Large Language Model (LLM) with the complete context of a project.

* **Source of Truth:** Treat this snapshot as the complete and authoritative source code.
* **Structure:** The file contains a **Directory Structure** tree, followed by the full content of each file, demarcated by `--- File: /package.json ---

{
  "name": "@xelth/eck-snapshot",
  "version": "3.0.0",
  "description": "A powerful CLI tool to create and restore single-file text snapshots of Git repositories and directories. Optimized for AI context and LLM workflows.",
  "main": "index.js",
  "type": "module",
  "bin": {
    "eck-snapshot": "./index.js"
  },
  "files": [
    "index.js",
    ".ecksnapshot.config.js",
    "README.md",
    "LICENSE",
    "src/",
    "setup.json"
  ],
  "scripts": {
    "test": "vitest",
    "test:ui": "vitest --ui",
    "test:run": "vitest run",
    "docs:auto": "node index.js docs-auto",
    "test:gpt": "vitest src/services/gptService.test.js"
  },
  "author": "xelth-com",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/xelth-com/eckSnapshot.git"
  },
  "dependencies": {
    "@babel/parser": "^7.25.6",
    "@babel/traverse": "^7.25.6",
    "@google/generative-ai": "^0.21.0",
    "@xenova/transformers": "^2.17.2",
    "chalk": "^5.3.0",
    "cli-progress": "^3.12.0",
    "commander": "^12.1.0",
    "dotenv": "^16.6.1",
    "execa": "^8.0.1",
    "ignore": "^5.3.1",
    "inquirer": "^9.2.20",
    "is-binary-path": "^2.1.0",
    "knex": "^3.1.0",
    "micromatch": "^4.0.8",
    "ora": "^8.1.0",
    "p-limit": "^5.0.0",
    "p-retry": "^6.2.1",
    "pg": "^8.16.3",
    "tree-sitter": "^0.21.1",
    "tree-sitter-c": "^0.21.4",
    "tree-sitter-java": "^0.21.0",
    "tree-sitter-kotlin": "^0.3.6",
    "tree-sitter-python": "^0.21.0",
    "vectra": "^0.9.0",
    "which": "^4.0.0"
  },
  "devDependencies": {
    "jsdom": "^24.0.0",
    "vitest": "^2.0.0"
  }
}


--- File: /index.js ---

#!/usr/bin/env node

import dotenv from 'dotenv';
import path from 'path';
import { fileURLToPath } from 'url';
import fs from 'fs';
import os from 'os';

// Get the directory of this script
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Always load .env from the program directory, not current working directory
const envPath = path.join(__dirname, '.env');
dotenv.config({ path: envPath });

// Auto-detect WSL and adjust DB_HOST if needed
function detectWSLAndSetupDB() {
  const isWSL = process.platform === 'linux' && 
    (process.env.WSL_DISTRO_NAME || 
     fs.existsSync('/proc/version') && fs.readFileSync('/proc/version', 'utf8').includes('Microsoft'));
  
  if (isWSL) {
    // Always override DB_HOST in WSL if it's localhost or not set
    if (!process.env.DB_HOST || process.env.DB_HOST === 'localhost' || process.env.DB_HOST === '127.0.0.1') {
      // Try to find Windows host IP in WSL
      try {
        const resolveConf = fs.readFileSync('/etc/resolv.conf', 'utf8');
        const nameserverMatch = resolveConf.match(/nameserver\s+(\d+\.\d+\.\d+\.\d+)/);
        if (nameserverMatch) {
          process.env.DB_HOST = nameserverMatch[1];
          console.log(`üîç WSL detected, using Windows host: ${process.env.DB_HOST}`);
        } else {
          // Fallback to common WSL2 gateway
          process.env.DB_HOST = '172.29.16.1';
          console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
        }
      } catch (e) {
        // Fallback to common WSL2 gateway
        process.env.DB_HOST = '172.29.16.1';
        console.log(`üîç WSL detected, using fallback host: ${process.env.DB_HOST}`);
      }
    } else {
      console.log(`üîç WSL detected, using configured host: ${process.env.DB_HOST}`);
    }
  }
}

detectWSLAndSetupDB();

import { run } from './src/cli/cli.js';

run();

--- File: /src/cli/cli.js ---

import { Command } from 'commander';
import path from 'path';
import fs from 'fs/promises';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

import { createRepoSnapshot } from './commands/createSnapshot.js';
import { restoreSnapshot } from './commands/restoreSnapshot.js';
import { pruneSnapshot } from './commands/pruneSnapshot.js';
import { generateConsilium } from './commands/consilium.js';
import { indexProject } from './commands/indexProject.js';
import { queryProject, viewIndex } from './commands/queryProject.js';
import { detectProject, testFileParsing } from './commands/detectProject.js';
import { trainTokens, showTokenStats } from './commands/trainTokens.js';
import { askGpt } from './commands/askGpt.js';
import { ask as askGptService } from '../services/gptService.js';
import { executePrompt, executePromptWithSession } from '../services/claudeCliService.js';
import { detectProfiles } from './commands/detectProfiles.js';
import { setupGemini } from './commands/setupGemini.js';
import { generateAutoDocs } from './commands/autoDocs.js';
import inquirer from 'inquirer';
import ora from 'ora';
import { execa } from 'execa';
import chalk from 'chalk';

/**
 * Check code boundaries in a file
 */
async function checkCodeBoundaries(filePath, agentId) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const boundaryRegex = /\/\* AGENT_BOUNDARY:\[([^\]]+)\] START \*\/([\s\S]*?)\/\* AGENT_BOUNDARY:\[[^\]]+\] END \*\//g;
    
    const boundaries = [];
    let match;
    
    while ((match = boundaryRegex.exec(content)) !== null) {
      boundaries.push({
        owner: match[1],
        startIndex: match.index,
        endIndex: match.index + match[0].length,
        content: match[2]
      });
    }
    
    return {
      file: filePath,
      hasBoundaries: boundaries.length > 0,
      boundaries: boundaries,
      canModify: boundaries.every(b => b.owner === agentId || b.owner === 'SHARED')
    };
  } catch (error) {
    return {
      file: filePath,
      error: error.message,
      canModify: true // If can't read, assume can modify (new file)
    };
  }
}

// Main run function that sets up the CLI
export function run() {
  const program = new Command();

  program
    .name('eck-snapshot')
    .description('Multi-agent aware snapshot tool for repositories with consilium support')
    .version('4.0.0');

  // Main snapshot command
  program
    .command('snapshot', { isDefault: true })
    .description('Create a multi-agent aware snapshot of a repository')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .option('-o, --output <dir>', 'Output directory')
    .option('--no-tree', 'Exclude directory tree')
    .option('-v, --verbose', 'Show detailed processing')
    .option('--max-file-size <size>', 'Maximum file size', '10MB')
    .option('--max-total-size <size>', 'Maximum total size', '100MB')
    .option('--max-depth <number>', 'Maximum tree depth', (val) => parseInt(val), 10)
    .option('--config <path>', 'Configuration file path')
    .option('--include-hidden', 'Include hidden files')
    .option('--format <type>', 'Output format: md, json', 'md')
    .option('--no-ai-header', 'Skip AI instructions')
    .option('-d, --dir', 'Directory mode')
    .option('--enhanced', 'Use enhanced multi-agent headers (default: true)', true)
    .option('--profile <name>', 'Filter files using profiles and/or ad-hoc glob patterns.')
    .option('--agent', 'Generate a snapshot optimized for a command-line agent')
    .action(createRepoSnapshot)
    .addHelpText('after', `
Examples for --profile:
  --profile backend                      (Uses the 'backend' profile)
  --profile "backend,-**/tests/**"         (Uses 'backend' profile, excludes all test files)
  --profile "src/**/*.js,-**/*.test.js"  (Includes all JS files in src, excludes tests)

  Combine predefined profiles (from .eck/profiles.json) with ad-hoc glob patterns.
  Prefix a profile name or glob pattern with '-' to exclude it.
`);

  // Restore command
  program
    .command('restore')
    .description('Restore files from a snapshot')
    .argument('<snapshot_file>', 'Snapshot file path')
    .argument('[target_directory]', 'Target directory', process.cwd())
    .option('-f, --force', 'Skip confirmation')
    .option('-v, --verbose', 'Show detailed progress')
    .option('--dry-run', 'Preview without writing')
    .option('--include <patterns...>', 'Include patterns')
    .option('--exclude <patterns...>', 'Exclude patterns')
    .option('--concurrency <number>', 'Concurrent operations', (val) => parseInt(val), 10)
    .action(restoreSnapshot);

  // Prune command
  program
    .command('prune')
    .description('Intelligently reduce snapshot size using AI file ranking')
    .argument('<snapshot_file>', 'Path to the snapshot file to prune')
    .option('--target-size <size>', 'Target size (e.g., 500KB, 1MB)', '500KB')
    .action(pruneSnapshot);

  // Consilium command
  program
    .command('consilium')
    .description('Generate a consilium request for complex decisions')
    .option('--type <type>', 'Decision type', 'technical_decision')
    .option('--title <title>', 'Decision title')
    .option('--description <desc>', 'Detailed description')
    .option('--complexity <num>', 'Complexity score (1-10)', (val) => parseInt(val), 7)
    .option('--constraints <list>', 'Comma-separated constraints')
    .option('--snapshot <file>', 'Include snapshot file')
    .option('--agent <id>', 'Requesting agent ID')
    .option('-o, --output <file>', 'Output file', 'consilium_request.json')
    .action(generateConsilium);

  // Check boundaries command
  program
    .command('check-boundaries')
    .description('Check agent boundaries in a file')
    .argument('<file>', 'File to check')
    .option('--agent <id>', 'Your agent ID')
    .action(async (file, options) => {
      const result = await checkCodeBoundaries(file, options.agent || 'UNKNOWN');
      console.log(JSON.stringify(result, null, 2));
    });

  // Index command
  program
    .command('index')
    .description('Index the project for intelligent search')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('--profile <name>', 'Use a specific context profile for indexing')
    .option('--export [filename]', 'Export the synchronized index to a JSON file. If no filename is provided, one will be generated.')
    .action(indexProject);

  // Query command
  program
    .command('query')
    .description('Query the project with context-aware search')
    .argument('<query>', 'Search query')
    .option('-k, --top-k <number>', 'Number of top results', (val) => parseInt(val), 10)
    .option('-o, --output <file>', 'Output file for snapshot')
    .option('--profile <name>', 'Use a specific context profile for querying')
    .option('--import <filename>', 'Use a portable index file for the query instead of the local database.')
    .action(queryProject);

  // Index view command
  program
    .command('index-view')
    .description('View the contents of the code chunks database')
    .option('--limit <number>', 'Number of records to display', (val) => parseInt(val), 10)
    .option('--offset <number>', 'Number of records to skip', (val) => parseInt(val), 0)
    .option('--file <path>', 'Filter by file path')
    .action(viewIndex);

  program
    .command('ask-gpt')
    .description('Delegate tasks to OpenAI Codex agent with automatic authentication')
    .argument('<payload>', 'JSON payload string (e.g. \'{"objective": "Calculate 5+2"}\')')
    .option('-v, --verbose', 'Enable verbose logging and detailed execution output')
    .option('--model <name>', 'Model to use (default: gpt-5-codex)', 'gpt-5-codex')
    .option('--reasoning <level>', 'Reasoning level: low, medium, high (default: high)', 'high')
    .action((payloadArg, cmd) => askGpt(payloadArg, cmd))
    .addHelpText('after', `
Examples:
  Ask a simple question:
    eck-snapshot ask-gpt '{"objective": "What is 5+2?"}'

  Request code changes with context:
    eck-snapshot ask-gpt '{
      "target_agent": "local_dev",
      "task_id": "feature-123",
      "payload": {
        "objective": "Add error handling to login function",
        "files_to_modify": [{"path": "src/auth.js", "action": "modify"}]
      },
      "post_execution_steps": {
        "journal_entry": {
          "type": "feat",
          "scope": "auth",
          "summary": "Add error handling"
        }
      }
    }' --verbose

Prerequisites:
  1. Install Codex CLI: npm install -g @openai/codex
  2. Login: codex login (requires ChatGPT Plus/Pro subscription)
  3. The command automatically loads .eck project context

Authentication:
  - Uses your existing 'codex login' credentials
  - Auto-retries on authentication errors
  - Supports ChatGPT Plus/Pro subscriptions
`);

  // Project detection command
  program
    .command('detect')
    .description('Detect and display project type and configuration')
    .argument('[projectPath]', 'Path to the project', process.cwd())
    .option('-v, --verbose', 'Show detailed detection results')
    .action(detectProject);

  // Android parsing test command
  program
    .command('test-android')
    .description('Test Android file parsing capabilities')
    .argument('<filePath>', 'Path to Android source file (.kt or .java)')
    .option('--show-content', 'Show content preview of parsed segments')
    .action(testFileParsing);

  // Token training command
  program
    .command('train-tokens')
    .description('Train token estimation with actual results')
    .argument('<projectType>', 'Project type (android, nodejs, python, etc.)')
    .argument('<fileSizeBytes>', 'File size in bytes')
    .argument('<estimatedTokens>', 'Estimated token count')
    .argument('<actualTokens>', 'Actual token count from LLM')
    .action(trainTokens);

  // Token statistics command
  program
    .command('token-stats')
    .description('Show token estimation statistics and accuracy')
    .action(showTokenStats);

  // Profile detection command
  program
    .command('profile-detect')
    .description('Use AI to scan the directory tree and auto-generate local context profiles (saves to .eck/profiles.json)')
    .argument('[repoPath]', 'Path to the repository', process.cwd())
    .action(detectProfiles);

  // Ask Claude command
  program
    .command('ask-claude')
    .description('Execute a prompt using claude-code CLI and return JSON response')
    .argument('<prompt>', 'Prompt to send to Claude')
    .option('-c, --continue', 'Continue the most recent conversation')
    .action(async (prompt, options) => {
      try {
        const result = await executePrompt(prompt, options.continue);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.warn(`‚ö†Ô∏è Claude failed: ${error.message}`);
        console.log('üîÑ Failing over to GPT for task...');
        try {
          const payload = (typeof prompt === 'string' && prompt.startsWith('{')) ? prompt : JSON.stringify({ objective: prompt });
          const gptResult = await askGptService(payload, { verbose: false });
          console.log(JSON.stringify(gptResult, null, 2));
        } catch (gptError) {
          console.error('Failed to execute prompt with both Claude and GPT:', gptError.message);
          process.exit(1);
        }
      }
    });

  // Ask Claude with specific session
  program
    .command('ask-claude-session')
    .description('Execute a prompt using specific session ID')
    .argument('<sessionId>', 'Session ID to resume')
    .argument('<prompt>', 'Prompt to send to Claude')
    .action(async (sessionId, prompt) => {
      try {
        // Directly use the provided session ID
        const result = await executePromptWithSession(prompt, sessionId);
        console.log(JSON.stringify(result, null, 2));
      } catch (error) {
        console.error('Failed to execute prompt:', error.message);
        process.exit(1);
      }
    });




  program
    .command('generate-ai-prompt')
    .description('Generate a specific AI prompt from a template.')
    .option('--role <role>', 'The role for which to generate a prompt', 'architect')
    .action(async (options) => {
      try {
        const templatePath = path.join(__dirname, '..', 'templates', `${options.role}-prompt.template.md`);
        const template = await fs.readFile(templatePath, 'utf-8');
        // In the future, we can inject dynamic data here from setup.json
        console.log(template);
      } catch (error) {
        console.error(`Failed to generate prompt for role '${options.role}':`, error.message);
        process.exit(1);
      }
    });

  // Setup Gemini command
  program
    .command('setup-gemini')
    .description('Generate claude.toml configuration for gemini-cli integration with dynamic paths')
    .option('-v, --verbose', 'Show detailed output and error information')
    .action(setupGemini);

  // Auto-docs command
  program
    .command('docs-auto')
    .description('Auto-generate documentation from gemini-extension.json files')
    .action(generateAutoDocs);

  program.parse(process.argv);
}


--- File: /src/config.js ---

import fs from 'fs/promises';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

let cachedConfig = null;

export async function loadSetupConfig() {
  if (cachedConfig) {
    return cachedConfig;
  }

  try {
    const setupPath = path.join(__dirname, '..', 'setup.json');
    const setupContent = await fs.readFile(setupPath, 'utf-8');
    cachedConfig = JSON.parse(setupContent);
    return cachedConfig;
  } catch (error) {
    console.error('Error loading setup.json:', error.message);
    throw new Error('Failed to load setup.json configuration file');
  }
}

/**
 * Loads and merges all profiles (local-first).
 */
export async function getAllProfiles(repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine.
  }

  // Local profiles override global profiles
  return { ...globalProfiles, ...localProfiles };
}

/**
 * Smart profile loader (Step 2 of dynamic profiles).
 * Reads local .eck/profiles.json first, then falls back to global setup.json profiles.
 */
export async function getProfile(profileName, repoPath) {
  const globalConfig = await loadSetupConfig();
  const globalProfiles = globalConfig.contextProfiles || {};

  let localProfiles = {};
  const localProfilePath = path.join(repoPath, '.eck', 'profiles.json');

  try {
    const localProfileContent = await fs.readFile(localProfilePath, 'utf-8');
    localProfiles = JSON.parse(localProfileContent);
  } catch (e) {
    // No local profiles.json found, which is fine. We just use globals.
  }

  // Local profiles override global profiles
  const allProfiles = { ...globalProfiles, ...localProfiles };

  return allProfiles[profileName] || null;
}

// Fallback default config for backwards compatibility
export const DEFAULT_CONFIG = {
  smartModeTokenThreshold: 200000,
  filesToIgnore: ['package-lock.json', '*.log', 'yarn.lock'],
  extensionsToIgnore: ['.sqlite3', '.db', '.DS_Store', '.env', '.pyc'],
  dirsToIgnore: ['node_modules/', '.git/', 'dist/', 'build/'],
  maxFileSize: '10MB',
  maxTotalSize: '100MB',
  maxDepth: 10,
  concurrency: 10
};

--- File: /src/cli/commands/createSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { execa } from 'execa';
import pLimit from 'p-limit';
import { SingleBar, Presets } from 'cli-progress';
import isBinaryPath from 'is-binary-path';
import zlib from 'zlib';
import { promisify } from 'util';
import ora from 'ora';
import micromatch from 'micromatch';

import {
  parseSize, formatSize, matchesPattern, checkGitRepository, 
  scanDirectoryRecursively, loadGitignore, readFileWithSizeCheck, 
  generateDirectoryTree, loadConfig, displayProjectInfo, loadProjectEckManifest,
  ensureSnapshotsInGitignore, initializeEckManifest
} from '../../utils/fileUtils.js';
import { detectProjectType, getProjectSpecificFiltering } from '../../utils/projectDetector.js';
import { estimateTokensWithPolynomial, generateTrainingCommand } from '../../utils/tokenEstimator.js';
import { indexProject } from './indexProject.js';
import { loadSetupConfig, getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';

/**
 * Creates dynamic project context based on detection results
 */
function createDynamicProjectContext(detection) {
  const { type, details } = detection;
  const context = {
    name: details.name || 'detected-project',
    type: type,
    detectedAt: new Date().toISOString()
  };
  
  // Create architecture info based on project type
  const architecture = {
    stack: [],
    structure: type
  };
  
  switch (type) {
    case 'android':
      architecture.stack = ['Android', details.language || 'Java', 'Gradle'];
      if (details.packageName) {
        context.packageName = details.packageName;
      }
      break;
      
    case 'nodejs':
      architecture.stack = ['Node.js'];
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'nodejs-monorepo':
      architecture.stack = ['Node.js', 'Monorepo'];
      if (details.monorepoTool) {
        architecture.stack.push(details.monorepoTool);
      }
      if (details.framework) {
        architecture.stack.push(details.framework);
      }
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    case 'python-poetry':
    case 'python-pip':
    case 'python-conda':
      architecture.stack = ['Python'];
      if (details.packageManager) {
        architecture.stack.push(details.packageManager);
      }
      break;
      
    case 'django':
      architecture.stack = ['Python', 'Django'];
      break;
      
    case 'flask':
      architecture.stack = ['Python', 'Flask'];
      break;
      
    case 'rust':
      architecture.stack = ['Rust', 'Cargo'];
      if (details.edition) {
        architecture.stack.push(`Rust ${details.edition}`);
      }
      break;
      
    case 'go':
      architecture.stack = ['Go'];
      if (details.goVersion) {
        architecture.stack.push(`Go ${details.goVersion}`);
      }
      break;
      
    case 'dotnet':
      architecture.stack = ['.NET'];
      if (details.language) {
        architecture.stack.push(details.language);
      }
      break;
      
    case 'flutter':
      architecture.stack = ['Flutter', 'Dart'];
      break;
      
    case 'react-native':
      architecture.stack = ['React Native', 'JavaScript'];
      if (details.hasTypescript) {
        architecture.stack.push('TypeScript');
      }
      break;
      
    default:
      architecture.stack = ['Unknown'];
  }
  
  context.architecture = architecture;
  
  return context;
}
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';

const gzip = promisify(zlib.gzip);

async function getProjectFiles(projectPath, config) {
  const isGitRepo = await checkGitRepository(projectPath);
  if (isGitRepo) {
    const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
    return stdout.split('\n').filter(Boolean);
  }
  return scanDirectoryRecursively(projectPath, config);
}

async function getGitCommitHash(projectPath) {
  try {
    const isGitRepo = await checkGitRepository(projectPath);
    if (isGitRepo) {
      const { stdout } = await execa('git', ['rev-parse', '--short=7', 'HEAD'], { cwd: projectPath });
      return stdout.trim();
    }
  } catch (error) {
    // Ignore errors - not a git repo or no commits
  }
  return null;
}

async function estimateProjectTokens(projectPath, config, projectType = null) {
  // Get project-specific filtering if not provided
  if (!projectType) {
    const detection = await detectProjectType(projectPath);
    projectType = detection.type;
  }
  
  const projectSpecific = await getProjectSpecificFiltering(projectType);
  
  // Merge project-specific filters with global config (same as in scanDirectoryRecursively)
  const effectiveConfig = {
    ...config,
    dirsToIgnore: [...(config.dirsToIgnore || []), ...(projectSpecific.dirsToIgnore || [])],
    filesToIgnore: [...(config.filesToIgnore || []), ...(projectSpecific.filesToIgnore || [])],
    extensionsToIgnore: [...(config.extensionsToIgnore || []), ...(projectSpecific.extensionsToIgnore || [])]
  };
  
  const files = await getProjectFiles(projectPath, effectiveConfig);
  const gitignore = await loadGitignore(projectPath);
  const maxFileSize = parseSize(effectiveConfig.maxFileSize);
  let totalSize = 0;
  let includedFiles = 0;
  
  for (const file of files) {
    try {
      const normalizedPath = file.replace(/\\/g, '/');
      
      // Apply the same filtering logic as in runFileSnapshot
      if (effectiveConfig.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
        continue;
      }
      
      if (gitignore.ignores(normalizedPath)) {
        continue;
      }
      
      if (isBinaryPath(file)) {
        continue;
      }
      
      const fileExtension = path.extname(file);
      if (effectiveConfig.extensionsToIgnore.includes(fileExtension)) {
        continue;
      }
      
      if (matchesPattern(normalizedPath, effectiveConfig.filesToIgnore)) {
        continue;
      }
      
      const stats = await fs.stat(path.join(projectPath, file));
      if (stats.size > maxFileSize) {
        continue;
      }
      
      totalSize += stats.size;
      includedFiles++;
    } catch (e) { /* ignore errors for estimation */ }
  }
  
  // Use adaptive polynomial estimation
  const estimatedTokens = await estimateTokensWithPolynomial(projectType, totalSize);
  
  return { estimatedTokens, totalSize, includedFiles };
}

async function processProjectFiles(repoPath, options, config, projectType = null) {
  const originalCwd = process.cwd();
  console.log(`\nüì∏ Processing files for: ${path.basename(repoPath)}`);
  
  const stats = {
    totalFiles: 0,
    includedFiles: 0,
    excludedFiles: 0,
    binaryFiles: 0,
    oversizedFiles: 0,
    ignoredFiles: 0,
    totalSize: 0,
    processedSize: 0,
    errors: [],
    skipReasons: new Map(),
    skippedFilesDetails: new Map()
  };

  try {
    process.chdir(repoPath);
    
    console.log('üîç Scanning repository...');
    let allFiles = await getProjectFiles(repoPath, config);

    if (options.profile) {
      console.log(`Applying profile filter: '${options.profile}'...`);
      allFiles = await applyProfileFilter(allFiles, options.profile, repoPath);
      console.log(`Filtered down to ${allFiles.length} files based on profile rules.`);
      if (allFiles.length === 0) {
        throw new Error(`Profile filter '${options.profile}' resulted in 0 files. Aborting.`);
      }
    }
    const gitignore = await loadGitignore(repoPath);
    stats.totalFiles = allFiles.length;
    
    console.log(`üìä Found ${stats.totalFiles} files`);
    
    const progressBar = new SingleBar({
      format: 'üìÑ Processing |{bar}| {percentage}% | {value}/{total} files | {filename}',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.rect);
    progressBar.start(allFiles.length, 0);
    
    const trackSkippedFile = (filePath, reason) => {
      if (!stats.skippedFilesDetails.has(reason)) {
        stats.skippedFilesDetails.set(reason, []);
      }
      stats.skippedFilesDetails.get(reason).push(filePath);
      stats.skipReasons.set(reason, (stats.skipReasons.get(reason) || 0) + 1);
    };
    
    const limit = pLimit(config.concurrency);
    const processFile = async (filePath, index) => {
      const normalizedPath = filePath.replace(/\\/g, '/');
      progressBar.update(index + 1, { filename: normalizedPath.slice(0, 50) });
      
      try {
        // Check if file should be ignored by directory patterns
        if (config.dirsToIgnore.some(dir => normalizedPath.startsWith(dir))) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Directory ignore patterns');
          return null;
        }
        
        // Check gitignore patterns
        if (gitignore.ignores(normalizedPath)) {
          stats.ignoredFiles++;
          trackSkippedFile(normalizedPath, 'Gitignore rules');
          return null;
        }
        
        // Check if binary file
        if (isBinaryPath(filePath)) {
          stats.binaryFiles++;
          trackSkippedFile(normalizedPath, 'Binary files');
          return null;
        }
        
        // Check extensions and file patterns
        const fileExtension = path.extname(filePath);
        if (config.extensionsToIgnore.includes(fileExtension)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, `File extension filter (${fileExtension})`);
          return null;
        }
        
        if (matchesPattern(normalizedPath, config.filesToIgnore)) {
          stats.excludedFiles++;
          trackSkippedFile(normalizedPath, 'File pattern filter');
          return null;
        }
        
        // Read file with size check
        const fullPath = path.join(repoPath, filePath);
        const fileStats = await fs.stat(fullPath);
        stats.totalSize += fileStats.size;
        
        const maxFileSize = parseSize(config.maxFileSize);
        if (fileStats.size > maxFileSize) {
          stats.oversizedFiles++;
          trackSkippedFile(normalizedPath, `File too large (${formatSize(fileStats.size)} > ${formatSize(maxFileSize)})`);
          return null;
        }
        
        const content = await readFileWithSizeCheck(fullPath, maxFileSize);
        stats.includedFiles++;
        stats.processedSize += fileStats.size;
        
        return {
          content: `

--- File: /src/cli/commands/indexProject.js ---

import path from 'path';
import { execa } from 'execa';
import ora from 'ora';
import micromatch from 'micromatch';
import { segmentFile } from '../../core/segmenter.js';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateBatchEmbeddings, releaseModel as releaseEmbeddingModel } from '../../services/embeddingService.js';
import { getCodeSummary } from '../../services/analysisService.js';
import { releaseModel as releaseAnalysisModel } from '../../services/analysisService.js';
import { getProfile } from '../../config.js';
import { applyProfileFilter } from '../../utils/fileUtils.js';
import { initializeEckManifest } from '../../utils/fileUtils.js';

async function getProjectFiles(projectPath) {
  const { stdout } = await execa('git', ['ls-files'], { cwd: projectPath });
  return stdout.split('\n').filter(Boolean);
}

export async function indexProject(projectPath, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...').start();
  try {
    // Initialize .eck manifest directory if it doesn't exist
    await initializeEckManifest(projectPath);
    
    await initDb();
    const knex = getKnex();
    let files = await getProjectFiles(projectPath);
    
    // --- Apply Advanced Profile Filtering ---
    const defaultProfile = await getProfile('default', projectPath);
    if (options.profile) {
        mainSpinner.text = `Applying profile filter: '${options.profile}'...`;
        files = await applyProfileFilter(files, options.profile, projectPath);
        mainSpinner.info(`Filtered down to ${files.length} files using profile: '${options.profile}'.`);
    } else if (defaultProfile) {
        mainSpinner.text = "Applying detected 'default' profile...";
        files = micromatch(files, defaultProfile.include, { ignore: defaultProfile.exclude });
        mainSpinner.info(`Filtered down to ${files.length} files using detected 'default' profile.`);
    }
    if (files.length === 0) {
        throw new Error(`Profile filter resulted in 0 files. Aborting.`);
    }
    // --- End Profile Filtering ---

    const profileName = options.profile || 'default';
    mainSpinner.text = '–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...';
    const existingRows = await knex('code_chunks').where({ profile: profileName }).select('content_hash', 'summary', 'embedding');
    const cache = new Map(existingRows.map(r => [r.content_hash, { summary: r.summary, embedding: r.embedding }]));
    mainSpinner.succeed(`–ù–∞–π–¥–µ–Ω–æ ${cache.size} –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.`);

    const allProjectChunks = [];
    const allProjectRelations = [];
    for (const filePath of files) {
        const { chunks, relations } = await segmentFile(path.join(projectPath, filePath));
        allProjectChunks.push(...chunks);
        allProjectRelations.push(...relations);
    }

    const chunksToProcessAI = allProjectChunks.filter(c => !cache.has(c.contentHash));
    mainSpinner.info(`–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${allProjectChunks.length}. –ù–æ–≤—ã—Ö/–∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö –¥–ª—è –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: ${chunksToProcessAI.length}.`);

    if (chunksToProcessAI.length > 0) {
        mainSpinner.text = `[1/2] –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ (${chunksToProcessAI.length} —á–∞–Ω–∫–æ–≤)...`;
        const summaries = await Promise.all(chunksToProcessAI.map(c => getCodeSummary(c.code)));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].summary = summaries[i]; }

        mainSpinner.text = `[2/2] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...`;
        const embeddings = await generateBatchEmbeddings(chunksToProcessAI.map(c => c.code));
        for (let i = 0; i < chunksToProcessAI.length; i++) { chunksToProcessAI[i].embedding = embeddings[i]; }
    }

    mainSpinner.text = '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –ë–î...';
    const allChunksData = allProjectChunks.map(c => {
        const cached = cache.get(c.contentHash);
        const finalEmbedding = c.embedding || (cached?.embedding ? JSON.parse(cached.embedding) : null);
        return {
            file_path: c.filePath,
            chunk_type: c.chunk_type,
            chunk_name: c.chunk_name,
            code: c.code,
            summary: c.summary || cached?.summary,
            tokens: Math.round(c.code.length / 4),
            embedding: finalEmbedding ? JSON.stringify(finalEmbedding) : null,
            content_hash: c.contentHash,
            profile: profileName,
        };
    });

    if (allChunksData.length > 0) {
      await knex('code_chunks')
          .insert(allChunksData)
          .onConflict('content_hash')
          .merge();
    }

    mainSpinner.text = '–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–≤—è–∑–µ–π...';
    const allDbChunks = await knex('code_chunks').where({ profile: profileName }).select('id', 'chunk_name', 'file_path');
    const nameToDbId = new Map(allDbChunks.map(c => [c.chunk_name, c.id]));
    const pathToDbId = new Map(allDbChunks.filter(c => c.chunk_type === 'file').map(c => [c.file_path, c.id]));

    const relationsToInsert = allProjectRelations
        .map(rel => {
            const fromId = nameToDbId.get(rel.from) || pathToDbId.get(rel.from);
            const toId = nameToDbId.get(rel.to);
            if (fromId && toId) {
                return { from_id: fromId, to_id: toId, relation_type: rel.type };
            }
            return null;
        })
        .filter(Boolean);
    
    if (relationsToInsert.length > 0) {
        await knex('relations').del(); // Clear old relations for simplicity
        await knex('relations').insert(relationsToInsert);
        mainSpinner.info(`–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ${relationsToInsert.length} —Å–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ.`);
    }

    const currentHashes = new Set(allProjectChunks.map(c => c.contentHash));
    const hashesToDelete = existingRows.filter(r => !currentHashes.has(r.content_hash)).map(r => r.content_hash);
    if (hashesToDelete.length > 0) {
        await knex('code_chunks').whereIn('content_hash', hashesToDelete).del();
        mainSpinner.info(`–£–¥–∞–ª–µ–Ω–æ ${hashesToDelete.length} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —á–∞–Ω–∫–æ–≤.`);
    }

  } catch (error) {
    mainSpinner.fail(`–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${error.message}`);
  } finally {
    await releaseAnalysisModel();
    await releaseEmbeddingModel();
    await destroyDb();
    mainSpinner.succeed('–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.');
  }
}


--- File: /src/cli/commands/queryProject.js ---

import ora from 'ora';
import path from 'path';
import fs from 'fs/promises';
import { getKnex, initDb, destroyDb } from '../../database/postgresConnector.js';
import { generateEmbedding } from '../../services/embeddingService.js';
import { generateEnhancedAIHeader } from '../../utils/aiHeader.js';
import { sanitizeForFilename } from '../../utils/fileUtils.js';

// Helper function to calculate cosine similarity between two vectors
function cosineSimilarity(a, b) {
    if (a.length !== b.length) return 0;
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
        dotProduct += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }
    
    const magnitude = Math.sqrt(normA) * Math.sqrt(normB);
    return magnitude === 0 ? 0 : dotProduct / magnitude;
}

export async function queryProject(query, options) {
  const mainSpinner = ora('–ó–∞–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ RAG-–ø–æ–∏—Å–∫–∞...').start();
  const knex = getKnex();

  try {
    // Step 1: Get Query Vector
    mainSpinner.text = '–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞...';
    const queryVector = await generateEmbedding(query);
    const queryVectorString = JSON.stringify(queryVector);

    // Step 2: Vector Search (using cosine similarity with JSON embeddings)
    mainSpinner.text = '–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...';
    let chunksQuery = knex('code_chunks').select('id', 'embedding', 'file_path', 'code');
    
    // Filter by profile if specified
    if (options.profile) {
        chunksQuery = chunksQuery.where('profile', options.profile);
        mainSpinner.info(`–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Ñ–∏–ª–µ: '${options.profile}'`);
    }
    
    const allChunks = await chunksQuery;
    
    // Calculate cosine similarity in JavaScript since we don't have pgvector
    const similarities = allChunks.map(chunk => {
        const chunkEmbedding = JSON.parse(chunk.embedding);
        const similarity = cosineSimilarity(queryVector, chunkEmbedding);
        return { ...chunk, similarity };
    });
    
    // Sort by similarity (highest first) and take top k
    const topResults = similarities
        .sort((a, b) => b.similarity - a.similarity)
        .slice(0, options.k || 10);
    
    const initialIds = topResults.map(row => row.id);
    if (initialIds.length === 0) {
        mainSpinner.warn('–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∫–æ–¥–∞.');
        return;
    }

    // Step 3: Graph Expansion
    mainSpinner.text = `–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ –≥—Ä–∞—Ñ (–Ω–∞–π–¥–µ–Ω–æ ${initialIds.length} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö —É–∑–ª–æ–≤)...`;
    const graphExpansionResults = await knex.raw(`
        WITH RECURSIVE graph_traversal AS (
            SELECT from_id, to_id FROM relations WHERE from_id = ANY(?)
            UNION
            SELECT r.from_id, r.to_id
            FROM relations r
            INNER JOIN graph_traversal gt ON gt.to_id = r.from_id
        )
        SELECT from_id as id FROM graph_traversal
        UNION
        SELECT to_id as id FROM graph_traversal;
    `, [initialIds]);

    const relatedIds = graphExpansionResults.rows.map(row => row.id);
    const allIds = [...new Set([...initialIds, ...relatedIds])];

    // Step 4: Fetch Code Chunks
    mainSpinner.text = `–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è ${allIds.length} —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...`;
    const finalChunks = await knex('code_chunks')
        .whereIn('id', allIds)
        .select('file_path', 'code');

    // Step 5: Assemble Snapshot
    mainSpinner.text = '–°–±–æ—Ä–∫–∞ RAG-—Å–Ω–∞–ø—à–æ—Ç–∞...';
    const header = await generateEnhancedAIHeader({
        repoName: path.basename(process.cwd()),
        userQuery: query,
        mode: 'vector'
    });

    // Group code by file path to maintain file structure in the output
    const filesContentMap = new Map();
    for (const chunk of finalChunks) {
        if (!filesContentMap.has(chunk.file_path)) {
            filesContentMap.set(chunk.file_path, []);
        }
        filesContentMap.get(chunk.file_path).push(chunk.code);
    }

    let snapshotContent = header;
    for (const [filePath, codeSnippets] of filesContentMap.entries()) {
        const relativePath = path.relative(process.cwd(), filePath);
        snapshotContent += `

--- File: /src/cli/commands/restoreSnapshot.js ---

import fs from 'fs/promises';
import path from 'path';
import { SingleBar, Presets } from 'cli-progress';
import pLimit from 'p-limit';
import zlib from 'zlib';
import { promisify } from 'util';
import inquirer from 'inquirer';

import { parseSnapshotContent, filterFilesToRestore, validateFilePaths } from '../../utils/fileUtils.js';

const gunzip = promisify(zlib.gunzip);

export async function restoreSnapshot(snapshotFile, targetDir, options) {
  const absoluteSnapshotPath = path.resolve(snapshotFile);
  const absoluteTargetDir = path.resolve(targetDir);
  
  console.log(`üìÑ Starting restore from snapshot: ${absoluteSnapshotPath}`);
  console.log(`üìÅ Target directory: ${absoluteTargetDir}`);

  try {
    let rawContent;
    
    if (snapshotFile.endsWith('.gz')) {
      const compressedBuffer = await fs.readFile(absoluteSnapshotPath);
      rawContent = (await gunzip(compressedBuffer)).toString('utf-8');
      console.log('‚úÖ Decompressed gzipped snapshot');
    } else {
      rawContent = await fs.readFile(absoluteSnapshotPath, 'utf-8');
    }

    let filesToRestore;
    
    try {
      const jsonData = JSON.parse(rawContent);
      if (jsonData.content) {
        console.log('üìÑ Detected JSON format, extracting content');
        filesToRestore = parseSnapshotContent(jsonData.content);
      } else {
        throw new Error('JSON format detected, but no "content" key found');
      }
    } catch (e) {
      console.log('üìÑ Treating snapshot as plain text format');
      filesToRestore = parseSnapshotContent(rawContent);
    }
    
    if (filesToRestore.length === 0) {
      console.warn('‚ö†Ô∏è No files found to restore in the snapshot');
      return;
    }

    if (options.include || options.exclude) {
      filesToRestore = filterFilesToRestore(filesToRestore, options);
      if (filesToRestore.length === 0) {
        console.warn('‚ö†Ô∏è No files remaining after applying filters');
        return;
      }
    }

    const invalidFiles = validateFilePaths(filesToRestore, absoluteTargetDir);
    if (invalidFiles.length > 0) {
      console.error('‚ùå Invalid file paths detected (potential directory traversal):');
      invalidFiles.forEach(file => console.error(`  ${file}`));
      process.exit(1);
    }

    console.log(`üìä Found ${filesToRestore.length} files to restore`);
    
    if (options.dryRun) {
      console.log('\nüîç Dry run mode - files that would be restored:');
      filesToRestore.forEach(file => {
        const fullPath = path.join(absoluteTargetDir, file.path);
        console.log(`  ${fullPath}`);
      });
      return;
    }

    if (!options.force) {
      const { confirm } = await inquirer.prompt([{
        type: 'confirm',
        name: 'confirm',
        message: `You are about to write ${filesToRestore.length} files to ${absoluteTargetDir}. Existing files will be overwritten. Continue?`,
        default: false
      }]);
      
      if (!confirm) {
        console.log('üö´ Restore operation cancelled by user');
        return;
      }
    }

    await fs.mkdir(absoluteTargetDir, { recursive: true });
    
    const stats = {
      totalFiles: filesToRestore.length,
      restoredFiles: 0,
      failedFiles: 0,
      errors: []
    };
    
    const progressBar = options.verbose ? null : new SingleBar({
      format: 'Restoring |{bar}| {percentage}% | {value}/{total} files',
      barCompleteChar: '\u2588',
      barIncompleteChar: '\u2591',
      hideCursor: true
    }, Presets.shades_classic);
    
    if (progressBar) progressBar.start(filesToRestore.length, 0);

    const limit = pLimit(options.concurrency || 10);
    const filePromises = filesToRestore.map((file, index) => 
      limit(async () => {
        try {
          const fullPath = path.join(absoluteTargetDir, file.path);
          const dir = path.dirname(fullPath);

          await fs.mkdir(dir, { recursive: true });
          await fs.writeFile(fullPath, file.content, 'utf-8');
          
          stats.restoredFiles++;
          
          if (progressBar) {
            progressBar.update(index + 1);
          } else if (options.verbose) {
            console.log(`‚úÖ Restored: ${file.path}`);
          }
          
          return { success: true, file: file.path };
        } catch (error) {
          stats.failedFiles++;
          stats.errors.push({ file: file.path, error: error.message });
          
          if (options.verbose) {
            console.log(`‚ùå Failed to restore: ${file.path} - ${error.message}`);
          }
          
          return { success: false, file: file.path, error: error.message };
        }
      })
    );

    await Promise.allSettled(filePromises);
    if (progressBar) progressBar.stop();

    console.log('\nüìä Restore Summary');
    console.log('='.repeat(50));
    console.log(`üéâ Restore completed!`);
    console.log(`‚úÖ Successfully restored: ${stats.restoredFiles} files`);
    
    if (stats.failedFiles > 0) {
      console.log(`‚ùå Failed to restore: ${stats.failedFiles} files`);
      if (stats.errors.length > 0) {
        console.log('\n‚ö†Ô∏è Errors encountered:');
        stats.errors.slice(0, 5).forEach(({ file, error }) => {
          console.log(`  ${file}: ${error}`);
        });
        if (stats.errors.length > 5) {
          console.log(`  ... and ${stats.errors.length - 5} more errors`);
        }
      }
    }
    
    console.log(`üìÅ Target directory: ${absoluteTargetDir}`);
    console.log('='.repeat(50));
    
  } catch (error) {
    console.error('\n‚ùå An error occurred during restore:');
    console.error(error.message);
    if (options.verbose) {
      console.error(error.stack);
    }
    process.exit(1);
  }
}



--- File: /test_snapshot.md ---

# Test Snapshot

## Project Overview
This is a test project for demonstrating the Gemini session API integration.

## Files
- `index.js` - Main entry point
- `src/` - Source code directory

## Instructions
You are an AI assistant helping with this test project. Please respond to user queries about the codebase.

--- File: /vitest.config.js ---

import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    testMatch: ['**/*.{test,spec}.js'],
    coverage: {
      reporter: ['text', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        '**/*.config.js',
        '**/*.test.js',
        '**/*.spec.js'
      ]
    }
  }
});



--- File: /vitest.config.js ---

import { defineConfig } from 'vitest/config';

export default defineConfig({
  test: {
    globals: true,
    environment: 'node',
    testMatch: ['**/*.{test,spec}.js'],
    coverage: {
      reporter: ['text', 'html'],
      exclude: [
        'node_modules/',
        'test/',
        '**/*.config.js',
        '**/*.test.js',
        '**/*.spec.js'
      ]
    }
  }
});

